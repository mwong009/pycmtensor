{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tensor-based choice modelling estimation package Welcome \u00b6 PyCMTensor is a tensor-optimized discrete choice model estimation Python library package, written with optimization compilers to speed up estimation of large datasets, simulating very large mixed logit models or implementing neural network functions into utility equations in choice models. Getting started \u00b6 Introduction - A brief introduction of the project and its features Installing PyCMTensor - Instructions to install PyCMTensor Overview - A short 5-minute quick start to estimating your first model Troubleshooting and tips - Some tips for common problems and fixes Examples \u00b6 Some basic working code examples Multinomial logit Mixed logit User guide \u00b6 User guide - Detailed guide on using PyCMTensor PyCMTensor configuration - How to modify PyCMTensor attributes Developer guide \u00b6 Developer guide - Guide for developers API reference About \u00b6 Contributing Release notes Licence Citation","title":"Home"},{"location":"#welcome","text":"PyCMTensor is a tensor-optimized discrete choice model estimation Python library package, written with optimization compilers to speed up estimation of large datasets, simulating very large mixed logit models or implementing neural network functions into utility equations in choice models.","title":"Welcome"},{"location":"#getting-started","text":"Introduction - A brief introduction of the project and its features Installing PyCMTensor - Instructions to install PyCMTensor Overview - A short 5-minute quick start to estimating your first model Troubleshooting and tips - Some tips for common problems and fixes","title":"Getting started"},{"location":"#examples","text":"Some basic working code examples Multinomial logit Mixed logit","title":"Examples"},{"location":"#user-guide","text":"User guide - Detailed guide on using PyCMTensor PyCMTensor configuration - How to modify PyCMTensor attributes","title":"User guide"},{"location":"#developer-guide","text":"Developer guide - Guide for developers API reference","title":"Developer guide"},{"location":"#about","text":"Contributing Release notes Licence Citation","title":"About"},{"location":"examples/","text":"Examples \u00b6 Table of Contents \u00b6 Multinomial logit Mixed logit Multinomial Logit \u00b6 In this example Mixed Logit \u00b6","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#table-of-contents","text":"Multinomial logit Mixed logit","title":"Table of Contents"},{"location":"examples/#multinomial-logit","text":"In this example","title":"Multinomial Logit"},{"location":"examples/#mixed-logit","text":"","title":"Mixed Logit"},{"location":"about/citation/","text":"Citation \u00b6 To cite this software package: Bibtex @misc{wongpycmtensor, author = {Melvin Wong}, title = {PyCMTensor: Tensor-based choice modelling estimation package}, year = 2023, publisher = {Zenodo}, version = {v1.3.2}, doi = {10.5281/zenodo.8074154}, url = {https://doi.org/10.5281/zenodo.8074154} }","title":"Citation"},{"location":"about/citation/#citation","text":"To cite this software package: Bibtex @misc{wongpycmtensor, author = {Melvin Wong}, title = {PyCMTensor: Tensor-based choice modelling estimation package}, year = 2023, publisher = {Zenodo}, version = {v1.3.2}, doi = {10.5281/zenodo.8074154}, url = {https://doi.org/10.5281/zenodo.8074154} }","title":"Citation"},{"location":"about/contributing/","text":"Guidelines for contributing \u00b6 Installation \u00b6 Fork a local copy Set up local development environment Contibuting to documentation \u00b6","title":"Contributing"},{"location":"about/contributing/#guidelines-for-contributing","text":"","title":"Guidelines for contributing"},{"location":"about/contributing/#installation","text":"Fork a local copy Set up local development environment","title":"Installation"},{"location":"about/contributing/#contibuting-to-documentation","text":"","title":"Contibuting to documentation"},{"location":"about/licence/","text":"MIT License Copyright \u00a9 2023, Melvin Wong Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Licence"},{"location":"about/release_notes/","text":"Release notes \u00b6 Unreleased \u00b6 Feat \u00b6 pycmtensor.py : Implemented early stopping on coefficient convergence in training loop functions.py : logit method now takes uneven dimensioned-utilities expression.py : Added RandomDraws expression for sampling in mixed logit get_train_data optional argument numpy_out to return numpy arrays rather than pandas arrays BHHH algorithm for calculating var-covar matrix applies to each data row Fix \u00b6 tests.yml : Update tests workflow file conda packages optimizers.py : Fixed name typo in __all__ results.py : Corrected calculation of hessian and bhhh matrices scheduler.py : Moved class function calls to parent class statistics.py : Fixed rob varcovar calculation error MNL.py : Moved aesara function to parent class data.py : Streamlined class function calls and removed unnecessary code removed package import clashes with config.py removed gnorm calculation update hessian matrix and bhhh algorithm functions Refactor \u00b6 utils.py : Removed unused code v1.3.2 (2023-06-23) \u00b6 Fix \u00b6 make arguments in MNL as optional keyword arguments moved learning rate variable to PyCMTensorModel class Refactor \u00b6 make model variables as property update __all__ package variables added train_data and valid_data property to Data class v1.3.1 (2022-11-17) \u00b6 Fix \u00b6 fix utility dimensions for asc only cases v1.3.0 (2022-11-10) \u00b6 Feat \u00b6 optimizers : added Nadam optimizer layers.py : added DenseLayer BatchNormLayer ResidualLayer added pycmtensor.about() to output package metadata added EMA function functions.exp_mov_average() Fix \u00b6 renamed depreceated instances of aesara modules data.py : defaults batch_size argument to 0 if batch_size is None updated syntax for expressions.py class objects added init_type property to Weights class moved model aesara compile functions from models.MNL to pycmtensor.PyCMTensorModel added argument type hints in function.py Refactor \u00b6 data : added import dataset cleaning step as arguments in Data() moved ResidualLayer to pycmtensor.models.layers updated timing to perf_counter pycmtensor : refactoring model_loglikelihood v1.2.1 (2022-10-25) \u00b6 Feat \u00b6 added pycmtensor.about() to output package metadata added EMA function functions.exp_mov_average() Fix \u00b6 updated syntax for expressions.py class objects added init_type property to Weights class moved model aesara compile functions from models.MNL to pycmtensor.PyCMTensorModel v1.2.0 (2022-10-14) \u00b6 Feat \u00b6 expressions : added Weights class object (#59) functions : added rmse and mae objective functions (#58) batch shuffle for training function : added KL divergence loss function (#50) Fix \u00b6 added expand_dims into logit function replace class function Beta.Beta with Beta.beta removed flatten() from logit function v1.1.0 (2022-09-23) \u00b6 Feat \u00b6 scheduler : added learning rate scheduling to train() code : overhaul and cleanup Fix \u00b6 environment : update project deps and pre-commit routine config : remove unnecessary cxx flags from macos builds Perf \u00b6 config : misc optimization changes v1.0.7 (2022-08-12) \u00b6 v1.0.6 (2022-08-12) \u00b6 Fix \u00b6 config : added optimizing speedups to config config : set default cyclic_lr_mode and cyclic_lr_step_size to None pre-commit-config : update black to 22.6.0 in pre-commit check Refactor \u00b6 models : refactored build_functions() into models.py database : refactor set_choice(choiceVar) v1.0.5 (2022-07-27) \u00b6 Fix \u00b6 tests : removed depreciated tests routine : remove depreciated tqdm module v1.0.4 (2022-07-27) \u00b6 Fix \u00b6 pycmtensor.py : update training method config.py : new config option verbosity: \"high\", \"low\" pycmtensor.py : remove warnings for max_iter<patience v1.0.3 (2022-05-12) \u00b6 v1.0.2 (2022-05-12) \u00b6 v1.0.1 (2022-05-12) \u00b6 Fix \u00b6 scheduler : fix missing args in input parameters scheduler : fix constantLR missing input paramerer v1.0.0 (2022-05-10) \u00b6 Feat \u00b6 python : update to python 3.10 Fix \u00b6 tests : update tests files to reflect changes in biogeme removal v0.8.0 (2022-05-10) \u00b6 Feat \u00b6 deps : remove Biogeme dependencies v0.7.1 (2022-05-10) \u00b6 Fix \u00b6 expressions : remove Biogeme dependencies database : remove dependencies of Biogeme debug : remove debug handler after each run to prevent duplication models : add function to return layer output -> get_layer_outputs() debug : disables tqdm if debug mode is on and activates debug_log Refactor \u00b6 move elasticites from models to statistics for consistency v0.7.0 (2022-03-17) \u00b6 Feat \u00b6 models : add functionality to compute elasticities of choice vs attribute in models.py Fix \u00b6 results : remove unnessary show_weights option in Results set default max_epoch on training run to adaptive rule print valid config options when invalid options are given as args to train() scheduler : modified cyclic_lr config loading sequence to fix unboundError train : turn saving model off for now config : generate os dependent ld_flags Refactor \u00b6 utils : refactored save_to_pickle and disables it Perf \u00b6 IterationTracker : use numpy array to store iteration data v0.6.5 (2022-03-14) \u00b6 Feat \u00b6 models : Implement the ResLogit layer Fix \u00b6 config : set default learning schedule to ConstantLR config : set default seed to a random number on init v0.6.4 (2022-03-13) \u00b6 Feat \u00b6 scheduler.py : add new scheduler (CyclicLR) for adaptive LR Fix \u00b6 project : fix project metadata and ci config : loadout config from train() to configparser utils : fix TypeError check v0.5.0 (2022-03-02) \u00b6 Feat \u00b6 config : add PyCMTensorConfig class to store config settings expressions : add magic methods lt le gt le ne eq config.py : enable pre-writing of .aesararc config file on module load models : add method prob() to MNLogit to output prob slices time_format : enable logging of build and estimation time results : add Predict class to output probs or discrete choices optimizers : add AdaGram algorithm Database : add getattr build-in type to Database pycmtensor.py : add model.output_choices to generate choices Fix \u00b6 statistics : add small value to stderror calculation to address sqrt(0) dependencies : move ipywidgets and pydot to dependencies renamed .rst to .md fix FileNotFoundError result : print more verbose results and options Database : add name to shared_data train : model instance now load initiated model class (not input Class as argument) Database : set choiceVar to mandatory argument PyCMTensor : rename append_to_params to add_params for consistency PyCMTensor : new method to add regularizers to cost function Expressions : invokes different operator for Beta Beta maths show excluded data in model est. output results : standardized naming conventions in modules db->database tqdm : add arg in train() to enable notebook progressbar swissmetro_test.ipynb : update swissmetro example Refactor \u00b6 PyCMTensor : refactoring models from pycmtensor.py Database : refactor(Database): refactoring database.py from pycmtensor.py optimizers : refactor base Optimizer class moved Beta Weights to expressions.py Perf \u00b6 shared_data : improve iteration speed by implementing shared() on input data","title":"Release notes"},{"location":"about/release_notes/#release-notes","text":"","title":"Release notes"},{"location":"about/release_notes/#unreleased","text":"","title":"Unreleased"},{"location":"about/release_notes/#feat","text":"pycmtensor.py : Implemented early stopping on coefficient convergence in training loop functions.py : logit method now takes uneven dimensioned-utilities expression.py : Added RandomDraws expression for sampling in mixed logit get_train_data optional argument numpy_out to return numpy arrays rather than pandas arrays BHHH algorithm for calculating var-covar matrix applies to each data row","title":"Feat"},{"location":"about/release_notes/#fix","text":"tests.yml : Update tests workflow file conda packages optimizers.py : Fixed name typo in __all__ results.py : Corrected calculation of hessian and bhhh matrices scheduler.py : Moved class function calls to parent class statistics.py : Fixed rob varcovar calculation error MNL.py : Moved aesara function to parent class data.py : Streamlined class function calls and removed unnecessary code removed package import clashes with config.py removed gnorm calculation update hessian matrix and bhhh algorithm functions","title":"Fix"},{"location":"about/release_notes/#refactor","text":"utils.py : Removed unused code","title":"Refactor"},{"location":"about/release_notes/#v132-2023-06-23","text":"","title":"v1.3.2 (2023-06-23)"},{"location":"about/release_notes/#fix_1","text":"make arguments in MNL as optional keyword arguments moved learning rate variable to PyCMTensorModel class","title":"Fix"},{"location":"about/release_notes/#refactor_1","text":"make model variables as property update __all__ package variables added train_data and valid_data property to Data class","title":"Refactor"},{"location":"about/release_notes/#v131-2022-11-17","text":"","title":"v1.3.1 (2022-11-17)"},{"location":"about/release_notes/#fix_2","text":"fix utility dimensions for asc only cases","title":"Fix"},{"location":"about/release_notes/#v130-2022-11-10","text":"","title":"v1.3.0 (2022-11-10)"},{"location":"about/release_notes/#feat_1","text":"optimizers : added Nadam optimizer layers.py : added DenseLayer BatchNormLayer ResidualLayer added pycmtensor.about() to output package metadata added EMA function functions.exp_mov_average()","title":"Feat"},{"location":"about/release_notes/#fix_3","text":"renamed depreceated instances of aesara modules data.py : defaults batch_size argument to 0 if batch_size is None updated syntax for expressions.py class objects added init_type property to Weights class moved model aesara compile functions from models.MNL to pycmtensor.PyCMTensorModel added argument type hints in function.py","title":"Fix"},{"location":"about/release_notes/#refactor_2","text":"data : added import dataset cleaning step as arguments in Data() moved ResidualLayer to pycmtensor.models.layers updated timing to perf_counter pycmtensor : refactoring model_loglikelihood","title":"Refactor"},{"location":"about/release_notes/#v121-2022-10-25","text":"","title":"v1.2.1 (2022-10-25)"},{"location":"about/release_notes/#feat_2","text":"added pycmtensor.about() to output package metadata added EMA function functions.exp_mov_average()","title":"Feat"},{"location":"about/release_notes/#fix_4","text":"updated syntax for expressions.py class objects added init_type property to Weights class moved model aesara compile functions from models.MNL to pycmtensor.PyCMTensorModel","title":"Fix"},{"location":"about/release_notes/#v120-2022-10-14","text":"","title":"v1.2.0 (2022-10-14)"},{"location":"about/release_notes/#feat_3","text":"expressions : added Weights class object (#59) functions : added rmse and mae objective functions (#58) batch shuffle for training function : added KL divergence loss function (#50)","title":"Feat"},{"location":"about/release_notes/#fix_5","text":"added expand_dims into logit function replace class function Beta.Beta with Beta.beta removed flatten() from logit function","title":"Fix"},{"location":"about/release_notes/#v110-2022-09-23","text":"","title":"v1.1.0 (2022-09-23)"},{"location":"about/release_notes/#feat_4","text":"scheduler : added learning rate scheduling to train() code : overhaul and cleanup","title":"Feat"},{"location":"about/release_notes/#fix_6","text":"environment : update project deps and pre-commit routine config : remove unnecessary cxx flags from macos builds","title":"Fix"},{"location":"about/release_notes/#perf","text":"config : misc optimization changes","title":"Perf"},{"location":"about/release_notes/#v107-2022-08-12","text":"","title":"v1.0.7 (2022-08-12)"},{"location":"about/release_notes/#v106-2022-08-12","text":"","title":"v1.0.6 (2022-08-12)"},{"location":"about/release_notes/#fix_7","text":"config : added optimizing speedups to config config : set default cyclic_lr_mode and cyclic_lr_step_size to None pre-commit-config : update black to 22.6.0 in pre-commit check","title":"Fix"},{"location":"about/release_notes/#refactor_3","text":"models : refactored build_functions() into models.py database : refactor set_choice(choiceVar)","title":"Refactor"},{"location":"about/release_notes/#v105-2022-07-27","text":"","title":"v1.0.5 (2022-07-27)"},{"location":"about/release_notes/#fix_8","text":"tests : removed depreciated tests routine : remove depreciated tqdm module","title":"Fix"},{"location":"about/release_notes/#v104-2022-07-27","text":"","title":"v1.0.4 (2022-07-27)"},{"location":"about/release_notes/#fix_9","text":"pycmtensor.py : update training method config.py : new config option verbosity: \"high\", \"low\" pycmtensor.py : remove warnings for max_iter<patience","title":"Fix"},{"location":"about/release_notes/#v103-2022-05-12","text":"","title":"v1.0.3 (2022-05-12)"},{"location":"about/release_notes/#v102-2022-05-12","text":"","title":"v1.0.2 (2022-05-12)"},{"location":"about/release_notes/#v101-2022-05-12","text":"","title":"v1.0.1 (2022-05-12)"},{"location":"about/release_notes/#fix_10","text":"scheduler : fix missing args in input parameters scheduler : fix constantLR missing input paramerer","title":"Fix"},{"location":"about/release_notes/#v100-2022-05-10","text":"","title":"v1.0.0 (2022-05-10)"},{"location":"about/release_notes/#feat_5","text":"python : update to python 3.10","title":"Feat"},{"location":"about/release_notes/#fix_11","text":"tests : update tests files to reflect changes in biogeme removal","title":"Fix"},{"location":"about/release_notes/#v080-2022-05-10","text":"","title":"v0.8.0 (2022-05-10)"},{"location":"about/release_notes/#feat_6","text":"deps : remove Biogeme dependencies","title":"Feat"},{"location":"about/release_notes/#v071-2022-05-10","text":"","title":"v0.7.1 (2022-05-10)"},{"location":"about/release_notes/#fix_12","text":"expressions : remove Biogeme dependencies database : remove dependencies of Biogeme debug : remove debug handler after each run to prevent duplication models : add function to return layer output -> get_layer_outputs() debug : disables tqdm if debug mode is on and activates debug_log","title":"Fix"},{"location":"about/release_notes/#refactor_4","text":"move elasticites from models to statistics for consistency","title":"Refactor"},{"location":"about/release_notes/#v070-2022-03-17","text":"","title":"v0.7.0 (2022-03-17)"},{"location":"about/release_notes/#feat_7","text":"models : add functionality to compute elasticities of choice vs attribute in models.py","title":"Feat"},{"location":"about/release_notes/#fix_13","text":"results : remove unnessary show_weights option in Results set default max_epoch on training run to adaptive rule print valid config options when invalid options are given as args to train() scheduler : modified cyclic_lr config loading sequence to fix unboundError train : turn saving model off for now config : generate os dependent ld_flags","title":"Fix"},{"location":"about/release_notes/#refactor_5","text":"utils : refactored save_to_pickle and disables it","title":"Refactor"},{"location":"about/release_notes/#perf_1","text":"IterationTracker : use numpy array to store iteration data","title":"Perf"},{"location":"about/release_notes/#v065-2022-03-14","text":"","title":"v0.6.5 (2022-03-14)"},{"location":"about/release_notes/#feat_8","text":"models : Implement the ResLogit layer","title":"Feat"},{"location":"about/release_notes/#fix_14","text":"config : set default learning schedule to ConstantLR config : set default seed to a random number on init","title":"Fix"},{"location":"about/release_notes/#v064-2022-03-13","text":"","title":"v0.6.4 (2022-03-13)"},{"location":"about/release_notes/#feat_9","text":"scheduler.py : add new scheduler (CyclicLR) for adaptive LR","title":"Feat"},{"location":"about/release_notes/#fix_15","text":"project : fix project metadata and ci config : loadout config from train() to configparser utils : fix TypeError check","title":"Fix"},{"location":"about/release_notes/#v050-2022-03-02","text":"","title":"v0.5.0 (2022-03-02)"},{"location":"about/release_notes/#feat_10","text":"config : add PyCMTensorConfig class to store config settings expressions : add magic methods lt le gt le ne eq config.py : enable pre-writing of .aesararc config file on module load models : add method prob() to MNLogit to output prob slices time_format : enable logging of build and estimation time results : add Predict class to output probs or discrete choices optimizers : add AdaGram algorithm Database : add getattr build-in type to Database pycmtensor.py : add model.output_choices to generate choices","title":"Feat"},{"location":"about/release_notes/#fix_16","text":"statistics : add small value to stderror calculation to address sqrt(0) dependencies : move ipywidgets and pydot to dependencies renamed .rst to .md fix FileNotFoundError result : print more verbose results and options Database : add name to shared_data train : model instance now load initiated model class (not input Class as argument) Database : set choiceVar to mandatory argument PyCMTensor : rename append_to_params to add_params for consistency PyCMTensor : new method to add regularizers to cost function Expressions : invokes different operator for Beta Beta maths show excluded data in model est. output results : standardized naming conventions in modules db->database tqdm : add arg in train() to enable notebook progressbar swissmetro_test.ipynb : update swissmetro example","title":"Fix"},{"location":"about/release_notes/#refactor_6","text":"PyCMTensor : refactoring models from pycmtensor.py Database : refactor(Database): refactoring database.py from pycmtensor.py optimizers : refactor base Optimizer class moved Beta Weights to expressions.py","title":"Refactor"},{"location":"about/release_notes/#perf_2","text":"shared_data : improve iteration speed by implementing shared() on input data","title":"Perf"},{"location":"developer_guide/","text":"Developer guide \u00b6 Virtual environment \u00b6 Installing dependencies \u00b6 Testing \u00b6","title":"Developer guide"},{"location":"developer_guide/#developer-guide","text":"","title":"Developer guide"},{"location":"developer_guide/#virtual-environment","text":"","title":"Virtual environment"},{"location":"developer_guide/#installing-dependencies","text":"","title":"Installing dependencies"},{"location":"developer_guide/#testing","text":"","title":"Testing"},{"location":"developer_guide/api/","text":"API reference \u00b6","title":"API reference"},{"location":"developer_guide/api/#api-reference","text":"","title":"API reference"},{"location":"getting_started/","text":"Introduction \u00b6 Why PyCMTensor? \u00b6 Writing mathematical operations and evaluating models involving choice based utility expressions can be difficult and time-consuming, especially with alternative specific utilities of different dimensionalities are involved or when specifying neural networks within utility specification. Typically, Python deep learning libraries such as TensorFlow, Torch, Keras, or Scikit-learn express entire datasets as products of n-dimensional arrays, without regards to the number of variables used or specification of independent taste coefficients in each part of the output choice probability equation. These libraries also do not expose the underlying operations that define the inputs of a random utility equation, making it inconvenient for hypothesis testing or running statistical tests without cumbersome modification or ad hoc calculations. Although these advanced Python deep learning libraries can be used to evaluate choice models, they are not explicitly intended to be used for estimating choice models, especially more advanced versions such as mixed logit. These deep learning models are first and foremost for model prediction, and not choice model estimation where clear interpretation of utility coefficients are needed. What can PyCMTensor do? \u00b6 Currently, PyCMTensor can be used to fully specify Multinomial Logit and Mixed Logit models, estimate and generate statistical tests, using optimized tensor operations via Aesara tensor libraries. Project goals \u00b6 The goal of PyCMTensor is to combine the easy-to-interpet choice modelling syntaxes and expressions (e.g. Biogeme) while also implementing some of the tensor-based operations and computational speed-up of a deep learning library through Aesara. PyCMTensor focuses on specifying utility expressions, and making it easy to define a deep neural network inside a choice model such as TasteNet or ResLogit model specifications. The distinction of PyCMTensor from other deep learning libraries is that it focuses on econometric modelling and statistical testing, rather than focusing purely on models for prediction. The higher level goals of this project are: provide tools to implement \"hybrid\" deep learning based discrete choice model facilitating development of deep learning tool for domain experts and researchers who are already familiar with discrete choice modelling with Biogeme Using the library to demonstrate innovative deep learning methods applied to econometric modelling make utility specification and prototyping at the meta-level more logical and consistent with choice modelling equations increase computational efficiency of estimating large scale choice models with machine learning techniques provide a platform for researchers with similar interests to contribute to an ecosystem for estimating advanced discrete choice models. Key features and differences \u00b6 Main features: Utility specification syntax writing in Python. Perform statistical tests and generate var-covar matrices for taste parameters. Fast execution of model estimation including of simulation based methods, e.g. Mixed Logit models. Model estimation with 1 st order (Stochastic GD) or 2 nd order methods (BFGS). Specifying neural nets with weight and bias parameters inside a utility function. TODO While other choice modelling estimation software are available, e.g. ..., PyCMTensor strives to fully implement deep learning based methods written in the same syntax format as Biogeme. Different software programs may occasionally vary in their behaviour and estimation results. The following are some of the key differences between PyCMTensor and other choice modelling estimation packages: Roadmap \u00b6 PyCMTensor is a work in progress, there are several proposed feature implementations that needs to be done and there are still some code maintenance, documentation writing, and testing to be performed. The following are proposed major feature implementations: Implementation of TasteNet and ResLogit hybrid deep learning choice models Optimization algorithms: Stochastic Newton Method (SNM) Momentum-based BFGS Variational inference estimation If you are interested in contributing to the development of PyCMTensor, please contact me.","title":"Introduction"},{"location":"getting_started/#introduction","text":"","title":"Introduction"},{"location":"getting_started/#why-pycmtensor","text":"Writing mathematical operations and evaluating models involving choice based utility expressions can be difficult and time-consuming, especially with alternative specific utilities of different dimensionalities are involved or when specifying neural networks within utility specification. Typically, Python deep learning libraries such as TensorFlow, Torch, Keras, or Scikit-learn express entire datasets as products of n-dimensional arrays, without regards to the number of variables used or specification of independent taste coefficients in each part of the output choice probability equation. These libraries also do not expose the underlying operations that define the inputs of a random utility equation, making it inconvenient for hypothesis testing or running statistical tests without cumbersome modification or ad hoc calculations. Although these advanced Python deep learning libraries can be used to evaluate choice models, they are not explicitly intended to be used for estimating choice models, especially more advanced versions such as mixed logit. These deep learning models are first and foremost for model prediction, and not choice model estimation where clear interpretation of utility coefficients are needed.","title":"Why PyCMTensor?"},{"location":"getting_started/#what-can-pycmtensor-do","text":"Currently, PyCMTensor can be used to fully specify Multinomial Logit and Mixed Logit models, estimate and generate statistical tests, using optimized tensor operations via Aesara tensor libraries.","title":"What can PyCMTensor do?"},{"location":"getting_started/#project-goals","text":"The goal of PyCMTensor is to combine the easy-to-interpet choice modelling syntaxes and expressions (e.g. Biogeme) while also implementing some of the tensor-based operations and computational speed-up of a deep learning library through Aesara. PyCMTensor focuses on specifying utility expressions, and making it easy to define a deep neural network inside a choice model such as TasteNet or ResLogit model specifications. The distinction of PyCMTensor from other deep learning libraries is that it focuses on econometric modelling and statistical testing, rather than focusing purely on models for prediction. The higher level goals of this project are: provide tools to implement \"hybrid\" deep learning based discrete choice model facilitating development of deep learning tool for domain experts and researchers who are already familiar with discrete choice modelling with Biogeme Using the library to demonstrate innovative deep learning methods applied to econometric modelling make utility specification and prototyping at the meta-level more logical and consistent with choice modelling equations increase computational efficiency of estimating large scale choice models with machine learning techniques provide a platform for researchers with similar interests to contribute to an ecosystem for estimating advanced discrete choice models.","title":"Project goals"},{"location":"getting_started/#key-features-and-differences","text":"Main features: Utility specification syntax writing in Python. Perform statistical tests and generate var-covar matrices for taste parameters. Fast execution of model estimation including of simulation based methods, e.g. Mixed Logit models. Model estimation with 1 st order (Stochastic GD) or 2 nd order methods (BFGS). Specifying neural nets with weight and bias parameters inside a utility function. TODO While other choice modelling estimation software are available, e.g. ..., PyCMTensor strives to fully implement deep learning based methods written in the same syntax format as Biogeme. Different software programs may occasionally vary in their behaviour and estimation results. The following are some of the key differences between PyCMTensor and other choice modelling estimation packages:","title":"Key features and differences"},{"location":"getting_started/#roadmap","text":"PyCMTensor is a work in progress, there are several proposed feature implementations that needs to be done and there are still some code maintenance, documentation writing, and testing to be performed. The following are proposed major feature implementations: Implementation of TasteNet and ResLogit hybrid deep learning choice models Optimization algorithms: Stochastic Newton Method (SNM) Momentum-based BFGS Variational inference estimation If you are interested in contributing to the development of PyCMTensor, please contact me.","title":"Roadmap"},{"location":"getting_started/installation/","text":"Installing PyCMTensor \u00b6 To ensure complete installation including the necessary libraries, it is recommended to first install dependencies via conda package manager in a virtual environment, then install PyCMTensor via pip . System requirements \u00b6 Python (3.9+) Aesara (2.9+) - from conda-forge Numpy - from conda-forge Scipy Pandas In addition, you will need: A C compiler compatible with your OS and Python installation. Libraries can be installed from conda-forge: Linux: gcc_linux-64 and gxx_linux-64 Windows (7 or later): m2w64-toolchain and vs2019_win-64 macOS (incl. M1): Clang BLAS installation MKL libraries, installed through conda with mkl-service package Openblas, default when Numpy is installed with pip, alternatively, with conda blas package Install conda dependencies \u00b6 Install Miniconda . Select the appropriate package for your operating system. Once you have installed conda, create a virtual environment and activate it conda create -n pycmtensor python=3.11 conda activate pycmtensor Install the conda dependencies: Windows conda install -c conda-forge mkl-service m2w64-toolchain vs2019_win-64 blas aesara -y macOS (incl. M1) conda install -c conda-forge mkl-service Clang blas aesara -y Linux conda install -c conda-forge mkl-service gcc_linux-64 gxx_linux-64 blas aesara -y Download PyCMTensor using pip \u00b6 Once the conda packages have been installed, install the rest of the packages using pip , type: pip install pycmtensor Checking your installation \u00b6 If PyCMTensor was installed correctly, the following should display when you run the following code in a python console: python -c \"import pycmtensor; print(pycmtensor.__version__)\" Output: 1.3.2 Updating PyCMTensor \u00b6 Update PyCMTensor by running the pip install --upgrade pycmtensor command Source code \u00b6 Source code can be checked out from the Github repository via git : git clone git::/github.com/mwong009/pycmtensor","title":"Installation"},{"location":"getting_started/installation/#installing-pycmtensor","text":"To ensure complete installation including the necessary libraries, it is recommended to first install dependencies via conda package manager in a virtual environment, then install PyCMTensor via pip .","title":"Installing PyCMTensor"},{"location":"getting_started/installation/#system-requirements","text":"Python (3.9+) Aesara (2.9+) - from conda-forge Numpy - from conda-forge Scipy Pandas In addition, you will need: A C compiler compatible with your OS and Python installation. Libraries can be installed from conda-forge: Linux: gcc_linux-64 and gxx_linux-64 Windows (7 or later): m2w64-toolchain and vs2019_win-64 macOS (incl. M1): Clang BLAS installation MKL libraries, installed through conda with mkl-service package Openblas, default when Numpy is installed with pip, alternatively, with conda blas package","title":"System requirements"},{"location":"getting_started/installation/#install-conda-dependencies","text":"Install Miniconda . Select the appropriate package for your operating system. Once you have installed conda, create a virtual environment and activate it conda create -n pycmtensor python=3.11 conda activate pycmtensor Install the conda dependencies: Windows conda install -c conda-forge mkl-service m2w64-toolchain vs2019_win-64 blas aesara -y macOS (incl. M1) conda install -c conda-forge mkl-service Clang blas aesara -y Linux conda install -c conda-forge mkl-service gcc_linux-64 gxx_linux-64 blas aesara -y","title":"Install conda dependencies"},{"location":"getting_started/installation/#download-pycmtensor-using-pip","text":"Once the conda packages have been installed, install the rest of the packages using pip , type: pip install pycmtensor","title":"Download PyCMTensor using pip"},{"location":"getting_started/installation/#checking-your-installation","text":"If PyCMTensor was installed correctly, the following should display when you run the following code in a python console: python -c \"import pycmtensor; print(pycmtensor.__version__)\" Output: 1.3.2","title":"Checking your installation"},{"location":"getting_started/installation/#updating-pycmtensor","text":"Update PyCMTensor by running the pip install --upgrade pycmtensor command","title":"Updating PyCMTensor"},{"location":"getting_started/installation/#source-code","text":"Source code can be checked out from the Github repository via git : git clone git::/github.com/mwong009/pycmtensor","title":"Source code"},{"location":"getting_started/overview/","text":"Overview \u00b6 Follow the steps below and learn how to use PyCMTensor to estimate a discrete choice model. In this tutorial, we will use the London Passenger Mode Choice (LPMC) dataset . Download the dataset from here and place it in the working directory. Jump to Putting it all together for the final Python script. Importing data from csv \u00b6 Import the PyCMTensor package and read the data using pandas : import pycmtensor import pandas as pd lpmc = pd . read_csv ( \"lpmc.dat\" , sep = ' \\t ' ) lpmc = lpmc [ lpmc [ \"travel_year\" ] == 2015 ] # select only the 2015 data to use Create a dataset object \u00b6 From the pycmtensor package, import the Dataset object, which stores and handles the tensors and arrays of the data variables. Denote the column with the choice variable with the argument choice= : from pycmtensor.dataset import Dataset ds = Dataset ( df = lpmc , choice = \"travel_mode\" ) The Dataset object takes the following arguments: df : The pandas.DataFrame object choice : The name of the choice variable found in the heading of the dataframe Note If the range of alternatives in the choice column does not start with 0 , e.g. [1, 2, 3, 4] instead of [0, 1, 2, 3] , the Dataset will automatically convert the alternatives to start with 0 . Split the dataset \u00b6 Then, split the dataset into training and validation datasets, frac= is the percentage of the data that is assigned to the training dataset. The rest of the data is assigned to the validation dataset. If frac= is not given as an argument, both training and validation dataset uses the same total number of samples. ds . split ( frac = 0.8 ) # splits 80% of the data into the training dataset and 20% into the validation dataset You should get an output showing the number of training and validation samples in the dataset: [INFO] n_train_samples:3986 n_valid_samples:997 Defining taste parameters \u00b6 Define the taste parameters using the Beta object from the pycmtensor.expressions module: from pycmtensor.expressions import Beta # Beta parameters asc_walk = Beta(\"asc_walk\", 0.0, None, None, 1) asc_cycle = Beta(\"asc_cycle\", 0.0, None, None, 0) asc_pt = Beta(\"asc_pt\", 0.0, None, None, 0) asc_drive = Beta(\"asc_drive\", 0.0, None, None, 0) b_cost = Beta(\"b_cost\", 0.0, None, None, 0) b_time = Beta(\"b_time\", 0.0, None, None, 0) b_purpose = Beta(\"b_purpose\", 0.0, None, None, 0) b_licence = Beta(\"b_licence\", 0.0, None, None, 0) The Beta object takes the following argument: name : Name of the taste parameter (required) value : The initial starting value. Defaults to 0. lb and ub : lower and upper bound. Defaults to None status : 1 if the parameter should not be estimated. Defaults to 0 . Note If a Beta variable is not used in the model, a warning will be shown in stdout. E.g. [WARNING] b_purpose not in any utility functions Specifying utility equations \u00b6 U_walk = asc_walk + b_time * ds [ \"dur_walking\" ] U_cycle = asc_cycle + b_time * ds [ \"dur_cycling\" ] U_pt = asc_pt + b_time * ( ds [ \"dur_pt_rail\" ] + ds [ \"dur_pt_bus\" ] + ds [ \"dur_pt_int\" ]) \\ + b_cost * ds [ \"cost_transit\" ] U_drive = asc_drive + b_time * ds [ \"dur_driving\" ] + b_licence * ds [ \"driving_license\" ] \\ + b_cost * ( ds [ \"cost_driving_fuel\" ] + ds [ \"cost_driving_ccharge\" ]) # vectorize the utility function U = [ U_walk , U_cycle , U_pt , U_drive ] Specifying the model \u00b6 mymodel = pycmtensor . models . MNL ( ds , locals (), U ) Output: [WARNING] b_purpose not in any utility functions [INFO] inputs in MNL: [driving_license, dur_walking, dur_cycling, dur_pt_rail, dur_pt_bus, dur_pt_int, dur_driving, cost_transit, cost_driving_fuel, cost_driving_ccharge] [INFO] Build time = 00:00:09 The MNL object takes the following argument: ds params utility av **kwargs : Optional keyword arguments for modifying the model configuration settings. See configuration for details. We use locals() as a shortcut for collecting the Beta objects for the argument params= . Estimating the model \u00b6 from pycmtensor.models import train from pycmtensor.optimizers import Adam from pycmtensor.scheduler import ConstantLR train ( model = mymodel , ds = ds , optimizer = Adam , # optional batch_size = 0 , # optional base_learning_rate = 1. , # optional convergence_threshold = 0.0001 , # optional max_steps = 200 , # optional lr_scheduler = ConstantLR , # optional ) Output: [INFO] Start (n=3986, Step=0, LL=-5525.77, Error=80.34%) [INFO] Train (Step=0, LL=-9008.61, Error=80.34%, gnorm=2.44949e+00, 0/2000) [INFO] Train (Step=16, LL=-3798.26, Error=39.12%, gnorm=4.46640e-01, 16/2000) [INFO] Train (Step=54, LL=-3487.97, Error=35.21%, gnorm=6.80979e-02, 54/2000) [INFO] Train (Step=87, LL=-3471.01, Error=35.01%, gnorm=9.93509e-03, 87/2000) [INFO] Train (Step=130, LL=-3470.29, Error=34.70%, gnorm=1.92617e-03, 130/2000) [INFO] Train (Step=168, LL=-3470.28, Error=34.70%, gnorm=3.22536e-04, 168/2000) [INFO] Train (Step=189, LL=-3470.28, Error=34.70%, gnorm=8.74120e-05, 189/2000) [INFO] Model converged (t=0.492) [INFO] Best results obtained at Step 185: LL=-3470.28, Error=34.70%, gnorm=2.16078e-04 Printing statistical test results \u00b6 print ( mymodel . results . beta_statistics ()) print ( mymodel . results . model_statistics ()) print ( mymodel . results . benchmark ()) Ouput: value std err t-test p-value rob. std err rob. t-test rob. p-value asc_cycle -3.853007 0.117872 -32.688157 0.0 0.120295 -32.029671 0.0 asc_drive -2.060414 0.099048 -20.802183 0.0 0.102918 -20.019995 0.0 asc_pt -1.305677 0.076151 -17.145988 0.0 0.079729 -16.376401 0.0 asc_walk 0.0 - - - - - - b_cost -0.135635 0.012788 -10.606487 0.0 0.01269 -10.688684 0.0 b_licence 1.420747 0.079905 17.780484 0.0 0.084526 16.808497 0.0 b_time -4.947477 0.183329 -26.986865 0.0 0.192431 -25.710378 0.0 value Number of training samples used 3986.0 Number of validation samples used 997.0 Null. log likelihood -5525.769323 Final log likelihood -3470.282749 Accuracy 65.30% Likelihood ratio test 4110.973149 Rho square 0.371982 Rho square bar 0.370715 Akaike Information Criterion 6954.565498 Bayesian Information Criterion 6998.599302 Final gradient norm 2.16078e-04 value Seed 42069 Model build time 00:00:09 Model train time 00:00:00 iterations per sec 384.15 iter/s Prediction and validation \u00b6 Putting it all together \u00b6 import pycmtensor import pandas as pd from pycmtensor.dataset import Dataset from pycmtensor.expressions import Beta lpmc = pd . read_csv ( \"lpmc.dat\" , sep = ' \\t ' ) lpmc = lpmc [ lpmc [ \"travel_year\" ] == 2015 ] # select only the 2015 data to use ds = Dataset ( df = lpmc , choice = \"travel_mode\" ) ds . split ( frac = 0.8 ) # Beta parameters asc_walk = Beta ( \"asc_walk\" , 0.0 , None , None , 1 ) asc_cycle = Beta ( \"asc_cycle\" , 0.0 , None , None , 0 ) asc_pt = Beta ( \"asc_pt\" , 0.0 , None , None , 0 ) asc_drive = Beta ( \"asc_drive\" , 0.0 , None , None , 0 ) b_cost = Beta ( \"b_cost\" , 0.0 , None , None , 0 ) b_time = Beta ( \"b_time\" , 0.0 , None , None , 0 ) b_licence = Beta ( \"b_licence\" , 0.0 , None , None , 0 ) U_walk = asc_walk + b_time * ds [ \"dur_walking\" ] U_cycle = asc_cycle + b_time * ds [ \"dur_cycling\" ] U_pt = asc_pt + b_time * ( ds [ \"dur_pt_rail\" ] + ds [ \"dur_pt_bus\" ] + ds [ \"dur_pt_int\" ]) \\ + b_cost * ds [ \"cost_transit\" ] U_drive = asc_drive + b_time * ds [ \"dur_driving\" ] + b_licence * ds [ \"driving_license\" ] \\ + b_cost * ( ds [ \"cost_driving_fuel\" ] + ds [ \"cost_driving_ccharge\" ]) # vectorize the utility function U = [ U_walk , U_cycle , U_pt , U_drive ] mymodel = pycmtensor . models . MNL ( ds , locals (), U ) from pycmtensor.models import train from pycmtensor.optimizers import Adam from pycmtensor.scheduler import ConstantLR train ( model = mymodel , ds = ds , optimizer = Adam , # optional batch_size = 0 , # optional base_learning_rate = 1. , # optional convergence_threshold = 0.0001 , # optional max_steps = 200 , # optional lr_scheduler = ConstantLR , # optional ) print ( mymodel . results . beta_statistics ()) print ( mymodel . results . model_statistics ()) print ( mymodel . results . benchmark ())","title":"Overview"},{"location":"getting_started/overview/#overview","text":"Follow the steps below and learn how to use PyCMTensor to estimate a discrete choice model. In this tutorial, we will use the London Passenger Mode Choice (LPMC) dataset . Download the dataset from here and place it in the working directory. Jump to Putting it all together for the final Python script.","title":"Overview"},{"location":"getting_started/overview/#importing-data-from-csv","text":"Import the PyCMTensor package and read the data using pandas : import pycmtensor import pandas as pd lpmc = pd . read_csv ( \"lpmc.dat\" , sep = ' \\t ' ) lpmc = lpmc [ lpmc [ \"travel_year\" ] == 2015 ] # select only the 2015 data to use","title":"Importing data from csv"},{"location":"getting_started/overview/#create-a-dataset-object","text":"From the pycmtensor package, import the Dataset object, which stores and handles the tensors and arrays of the data variables. Denote the column with the choice variable with the argument choice= : from pycmtensor.dataset import Dataset ds = Dataset ( df = lpmc , choice = \"travel_mode\" ) The Dataset object takes the following arguments: df : The pandas.DataFrame object choice : The name of the choice variable found in the heading of the dataframe Note If the range of alternatives in the choice column does not start with 0 , e.g. [1, 2, 3, 4] instead of [0, 1, 2, 3] , the Dataset will automatically convert the alternatives to start with 0 .","title":"Create a dataset object"},{"location":"getting_started/overview/#split-the-dataset","text":"Then, split the dataset into training and validation datasets, frac= is the percentage of the data that is assigned to the training dataset. The rest of the data is assigned to the validation dataset. If frac= is not given as an argument, both training and validation dataset uses the same total number of samples. ds . split ( frac = 0.8 ) # splits 80% of the data into the training dataset and 20% into the validation dataset You should get an output showing the number of training and validation samples in the dataset: [INFO] n_train_samples:3986 n_valid_samples:997","title":"Split the dataset"},{"location":"getting_started/overview/#defining-taste-parameters","text":"Define the taste parameters using the Beta object from the pycmtensor.expressions module: from pycmtensor.expressions import Beta # Beta parameters asc_walk = Beta(\"asc_walk\", 0.0, None, None, 1) asc_cycle = Beta(\"asc_cycle\", 0.0, None, None, 0) asc_pt = Beta(\"asc_pt\", 0.0, None, None, 0) asc_drive = Beta(\"asc_drive\", 0.0, None, None, 0) b_cost = Beta(\"b_cost\", 0.0, None, None, 0) b_time = Beta(\"b_time\", 0.0, None, None, 0) b_purpose = Beta(\"b_purpose\", 0.0, None, None, 0) b_licence = Beta(\"b_licence\", 0.0, None, None, 0) The Beta object takes the following argument: name : Name of the taste parameter (required) value : The initial starting value. Defaults to 0. lb and ub : lower and upper bound. Defaults to None status : 1 if the parameter should not be estimated. Defaults to 0 . Note If a Beta variable is not used in the model, a warning will be shown in stdout. E.g. [WARNING] b_purpose not in any utility functions","title":"Defining taste parameters"},{"location":"getting_started/overview/#specifying-utility-equations","text":"U_walk = asc_walk + b_time * ds [ \"dur_walking\" ] U_cycle = asc_cycle + b_time * ds [ \"dur_cycling\" ] U_pt = asc_pt + b_time * ( ds [ \"dur_pt_rail\" ] + ds [ \"dur_pt_bus\" ] + ds [ \"dur_pt_int\" ]) \\ + b_cost * ds [ \"cost_transit\" ] U_drive = asc_drive + b_time * ds [ \"dur_driving\" ] + b_licence * ds [ \"driving_license\" ] \\ + b_cost * ( ds [ \"cost_driving_fuel\" ] + ds [ \"cost_driving_ccharge\" ]) # vectorize the utility function U = [ U_walk , U_cycle , U_pt , U_drive ]","title":"Specifying utility equations"},{"location":"getting_started/overview/#specifying-the-model","text":"mymodel = pycmtensor . models . MNL ( ds , locals (), U ) Output: [WARNING] b_purpose not in any utility functions [INFO] inputs in MNL: [driving_license, dur_walking, dur_cycling, dur_pt_rail, dur_pt_bus, dur_pt_int, dur_driving, cost_transit, cost_driving_fuel, cost_driving_ccharge] [INFO] Build time = 00:00:09 The MNL object takes the following argument: ds params utility av **kwargs : Optional keyword arguments for modifying the model configuration settings. See configuration for details. We use locals() as a shortcut for collecting the Beta objects for the argument params= .","title":"Specifying the model"},{"location":"getting_started/overview/#estimating-the-model","text":"from pycmtensor.models import train from pycmtensor.optimizers import Adam from pycmtensor.scheduler import ConstantLR train ( model = mymodel , ds = ds , optimizer = Adam , # optional batch_size = 0 , # optional base_learning_rate = 1. , # optional convergence_threshold = 0.0001 , # optional max_steps = 200 , # optional lr_scheduler = ConstantLR , # optional ) Output: [INFO] Start (n=3986, Step=0, LL=-5525.77, Error=80.34%) [INFO] Train (Step=0, LL=-9008.61, Error=80.34%, gnorm=2.44949e+00, 0/2000) [INFO] Train (Step=16, LL=-3798.26, Error=39.12%, gnorm=4.46640e-01, 16/2000) [INFO] Train (Step=54, LL=-3487.97, Error=35.21%, gnorm=6.80979e-02, 54/2000) [INFO] Train (Step=87, LL=-3471.01, Error=35.01%, gnorm=9.93509e-03, 87/2000) [INFO] Train (Step=130, LL=-3470.29, Error=34.70%, gnorm=1.92617e-03, 130/2000) [INFO] Train (Step=168, LL=-3470.28, Error=34.70%, gnorm=3.22536e-04, 168/2000) [INFO] Train (Step=189, LL=-3470.28, Error=34.70%, gnorm=8.74120e-05, 189/2000) [INFO] Model converged (t=0.492) [INFO] Best results obtained at Step 185: LL=-3470.28, Error=34.70%, gnorm=2.16078e-04","title":"Estimating the model"},{"location":"getting_started/overview/#printing-statistical-test-results","text":"print ( mymodel . results . beta_statistics ()) print ( mymodel . results . model_statistics ()) print ( mymodel . results . benchmark ()) Ouput: value std err t-test p-value rob. std err rob. t-test rob. p-value asc_cycle -3.853007 0.117872 -32.688157 0.0 0.120295 -32.029671 0.0 asc_drive -2.060414 0.099048 -20.802183 0.0 0.102918 -20.019995 0.0 asc_pt -1.305677 0.076151 -17.145988 0.0 0.079729 -16.376401 0.0 asc_walk 0.0 - - - - - - b_cost -0.135635 0.012788 -10.606487 0.0 0.01269 -10.688684 0.0 b_licence 1.420747 0.079905 17.780484 0.0 0.084526 16.808497 0.0 b_time -4.947477 0.183329 -26.986865 0.0 0.192431 -25.710378 0.0 value Number of training samples used 3986.0 Number of validation samples used 997.0 Null. log likelihood -5525.769323 Final log likelihood -3470.282749 Accuracy 65.30% Likelihood ratio test 4110.973149 Rho square 0.371982 Rho square bar 0.370715 Akaike Information Criterion 6954.565498 Bayesian Information Criterion 6998.599302 Final gradient norm 2.16078e-04 value Seed 42069 Model build time 00:00:09 Model train time 00:00:00 iterations per sec 384.15 iter/s","title":"Printing statistical test results"},{"location":"getting_started/overview/#prediction-and-validation","text":"","title":"Prediction and validation"},{"location":"getting_started/overview/#putting-it-all-together","text":"import pycmtensor import pandas as pd from pycmtensor.dataset import Dataset from pycmtensor.expressions import Beta lpmc = pd . read_csv ( \"lpmc.dat\" , sep = ' \\t ' ) lpmc = lpmc [ lpmc [ \"travel_year\" ] == 2015 ] # select only the 2015 data to use ds = Dataset ( df = lpmc , choice = \"travel_mode\" ) ds . split ( frac = 0.8 ) # Beta parameters asc_walk = Beta ( \"asc_walk\" , 0.0 , None , None , 1 ) asc_cycle = Beta ( \"asc_cycle\" , 0.0 , None , None , 0 ) asc_pt = Beta ( \"asc_pt\" , 0.0 , None , None , 0 ) asc_drive = Beta ( \"asc_drive\" , 0.0 , None , None , 0 ) b_cost = Beta ( \"b_cost\" , 0.0 , None , None , 0 ) b_time = Beta ( \"b_time\" , 0.0 , None , None , 0 ) b_licence = Beta ( \"b_licence\" , 0.0 , None , None , 0 ) U_walk = asc_walk + b_time * ds [ \"dur_walking\" ] U_cycle = asc_cycle + b_time * ds [ \"dur_cycling\" ] U_pt = asc_pt + b_time * ( ds [ \"dur_pt_rail\" ] + ds [ \"dur_pt_bus\" ] + ds [ \"dur_pt_int\" ]) \\ + b_cost * ds [ \"cost_transit\" ] U_drive = asc_drive + b_time * ds [ \"dur_driving\" ] + b_licence * ds [ \"driving_license\" ] \\ + b_cost * ( ds [ \"cost_driving_fuel\" ] + ds [ \"cost_driving_ccharge\" ]) # vectorize the utility function U = [ U_walk , U_cycle , U_pt , U_drive ] mymodel = pycmtensor . models . MNL ( ds , locals (), U ) from pycmtensor.models import train from pycmtensor.optimizers import Adam from pycmtensor.scheduler import ConstantLR train ( model = mymodel , ds = ds , optimizer = Adam , # optional batch_size = 0 , # optional base_learning_rate = 1. , # optional convergence_threshold = 0.0001 , # optional max_steps = 200 , # optional lr_scheduler = ConstantLR , # optional ) print ( mymodel . results . beta_statistics ()) print ( mymodel . results . model_statistics ()) print ( mymodel . results . benchmark ())","title":"Putting it all together"},{"location":"getting_started/troubleshooting/","text":"Troubleshooting \u00b6","title":"Troubleshooting & tips"},{"location":"getting_started/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user_guide/","text":"User guide \u00b6 Data structures \u00b6 Calling variables and selecting data \u00b6 Mathematical operations \u00b6 Working with generic and alternative specific variables \u00b6 Model estimation \u00b6 Configuration \u00b6 Optimizers \u00b6 Fine tuning model \u00b6 Generate results \u00b6","title":"User guide"},{"location":"user_guide/#user-guide","text":"","title":"User guide"},{"location":"user_guide/#data-structures","text":"","title":"Data structures"},{"location":"user_guide/#calling-variables-and-selecting-data","text":"","title":"Calling variables and selecting data"},{"location":"user_guide/#mathematical-operations","text":"","title":"Mathematical operations"},{"location":"user_guide/#working-with-generic-and-alternative-specific-variables","text":"","title":"Working with generic and alternative specific variables"},{"location":"user_guide/#model-estimation","text":"","title":"Model estimation"},{"location":"user_guide/#configuration","text":"","title":"Configuration"},{"location":"user_guide/#optimizers","text":"","title":"Optimizers"},{"location":"user_guide/#fine-tuning-model","text":"","title":"Fine tuning model"},{"location":"user_guide/#generate-results","text":"","title":"Generate results"},{"location":"user_guide/configuration/","text":"Configuration \u00b6 The config module Configuration attributes \u00b6","title":"Configuration"},{"location":"user_guide/configuration/#configuration","text":"The config module","title":"Configuration"},{"location":"user_guide/configuration/#configuration-attributes","text":"","title":"Configuration attributes"}]}