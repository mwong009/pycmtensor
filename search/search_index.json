{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A Tensor-based choice modelling estimation package written in Python</p>"},{"location":"#welcome","title":"Welcome","text":"<p>PyCMTensor is a tensor-based discrete choice modelling estimation Python library package. It has a particular focus on estimation of hybrid neural networks and Logit models, as well as on Mixed Logit models. PyCMTensor models are based on computational graphs and models estimated using generalized backpropagation algorithms. PyCMTensor can be used to fully specify Multinomial Logit and Mixed Logit models, perform model estimation using computational graphs and generate statistical test results for econometric analysis.</p>"},{"location":"#key-features","title":"Key features","text":"<p>Model specification</p> <p>PyCMTensor can be used to specify and customize alternative specific linear and non-linear utility functions, random variables and deep neural networks. PyCMTensor can keep track of model taste parameters, gradients and hessian matrices for econometric interpretability, specification testing, prediction, and elasticity analysis.</p> <p>Cost functions</p> <p>To evaluate model performance, various cost functions are available, which include (negative) log likelihood, mean squared error, and KL divergence. Model accuracy can also be evaluated using out-of-sample prediction probabilities or discrete choices (Argmax). </p> <p>Data processing</p> <p>Datasets can be split into different segments for training and validation easily, to prioritize different aspects of the model. Arithmetic operations and boolean expressions can be used on variables and parameters within the utility function without pre-processing the datafile. Minibatch training is possible for estimation speedup on large datasets.</p> <p>Model tuning</p> <p>PyCMTensor includes a set of 1<sup>st</sup> order and 1.5<sup>th</sup> order optimization routines and learning rate schedulers for estimating choice models. </p>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>Introduction - A brief introduction of the project and its features</li> <li>Installing PyCMTensor - Instructions to install PyCMTensor</li> <li>Overview - A short 5-minute quick start to estimating your first model</li> <li>Troubleshooting and tips - Some tips for common problems and fixes</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>Some basic working code examples</p>"},{"location":"#user-guide","title":"User guide","text":"<ul> <li>User guide - Detailed guide on using PyCMTensor</li> <li>PyCMTensor configuration - How to modify PyCMTensor attributes</li> </ul>"},{"location":"#developer-guide","title":"Developer guide","text":"<ul> <li>Developer guide - Guide for developers</li> <li>API reference</li> </ul>"},{"location":"#about","title":"About","text":"<ul> <li>Contributing</li> <li>Release notes</li> <li>Licence</li> <li>Citation</li> </ul>"},{"location":"about/citation/","title":"Citation","text":"<p>To cite this software package:</p> <p>Bibtex</p> <pre><code>@misc{wongpycmtensor,\n    author       = {Melvin Wong},\n    title        = {PyCMTensor: Tensor-based choice modelling estimation Python package},\n    year         = 2023,\n    publisher    = {Zenodo},\n    version      = {v1.3.2},\n    doi          = {10.5281/zenodo.8074154},\n    url          = {https://doi.org/10.5281/zenodo.8074154}\n}\n</code></pre>"},{"location":"about/contributing/","title":"Guidelines for contributing","text":""},{"location":"about/contributing/#installation","title":"Installation","text":"<p>Fork a local copy</p> <p>Set up local development environment</p>"},{"location":"about/contributing/#contibuting-to-documentation","title":"Contibuting to documentation","text":""},{"location":"about/licence/","title":"Licence","text":"<p>MIT License</p> <p>Copyright \u00a9 2023, Melvin Wong</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"about/release_notes/","title":"Release notes","text":""},{"location":"about/release_notes/#unreleased","title":"Unreleased","text":""},{"location":"about/release_notes/#feat","title":"Feat","text":"<ul> <li>pycmtensor.py: Implemented early stopping on coefficient convergence in training loop</li> <li>functions.py: logit method now takes uneven dimensioned-utilities</li> <li>expression.py: Added RandomDraws expression for sampling in mixed logit</li> <li>get_train_data optional argument numpy_out to return numpy arrays rather than pandas arrays</li> <li>BHHH algorithm for calculating var-covar matrix applies to each data row</li> </ul>"},{"location":"about/release_notes/#fix","title":"Fix","text":"<ul> <li>tests.yml: Update tests workflow file conda packages</li> <li>optimizers.py: Fixed name typo in <code>__all__</code></li> <li>results.py: Corrected calculation of hessian and bhhh matrices</li> <li>scheduler.py: Moved class function calls to parent class</li> <li>statistics.py: Fixed rob varcovar calculation error</li> <li>MNL.py: Moved aesara function to parent class</li> <li>data.py: Streamlined class function calls and removed unnecessary code</li> <li>removed package import clashes with config.py</li> <li>removed gnorm calculation</li> <li>update hessian matrix and bhhh algorithm functions</li> </ul>"},{"location":"about/release_notes/#refactor","title":"Refactor","text":"<ul> <li>utils.py: Removed unused code</li> </ul>"},{"location":"about/release_notes/#v132-2023-06-23","title":"v1.3.2 (2023-06-23)","text":""},{"location":"about/release_notes/#fix_1","title":"Fix","text":"<ul> <li>make arguments in <code>MNL</code> as optional keyword arguments</li> <li>moved learning rate variable to <code>PyCMTensorModel</code> class</li> </ul>"},{"location":"about/release_notes/#refactor_1","title":"Refactor","text":"<ul> <li>make model variables as property</li> <li>update <code>__all__</code> package variables</li> <li>added <code>train_data</code> and <code>valid_data</code> property to <code>Data</code> class</li> </ul>"},{"location":"about/release_notes/#v131-2022-11-17","title":"v1.3.1 (2022-11-17)","text":""},{"location":"about/release_notes/#fix_2","title":"Fix","text":"<ul> <li>fix utility dimensions for asc only cases</li> </ul>"},{"location":"about/release_notes/#v130-2022-11-10","title":"v1.3.0 (2022-11-10)","text":""},{"location":"about/release_notes/#feat_1","title":"Feat","text":"<ul> <li>optimizers: added <code>Nadam</code> optimizer</li> <li>layers.py: added <code>DenseLayer</code> <code>BatchNormLayer</code> <code>ResidualLayer</code></li> <li>added <code>pycmtensor.about()</code> to output package metadata</li> <li>added EMA function <code>functions.exp_mov_average()</code></li> </ul>"},{"location":"about/release_notes/#fix_3","title":"Fix","text":"<ul> <li>renamed depreceated instances of <code>aesara</code> modules</li> <li>data.py: defaults <code>batch_size</code> argument to 0 if batch_size is <code>None</code></li> <li>updated syntax for <code>expressions.py</code> class objects</li> <li>added <code>init_type</code> property to <code>Weights</code> class</li> <li>moved model aesara compile functions from <code>models.MNL</code> to <code>pycmtensor.PyCMTensorModel</code></li> <li>added argument type hints in function.py</li> </ul>"},{"location":"about/release_notes/#refactor_2","title":"Refactor","text":"<ul> <li>data: added import dataset cleaning step as arguments in <code>Data()</code></li> <li>moved ResidualLayer to <code>pycmtensor.models.layers</code></li> <li>updated timing to perf_counter</li> <li>pycmtensor: refactoring model_loglikelihood</li> </ul>"},{"location":"about/release_notes/#v121-2022-10-25","title":"v1.2.1 (2022-10-25)","text":""},{"location":"about/release_notes/#feat_2","title":"Feat","text":"<ul> <li>added <code>pycmtensor.about()</code> to output package metadata</li> <li>added EMA function <code>functions.exp_mov_average()</code></li> </ul>"},{"location":"about/release_notes/#fix_4","title":"Fix","text":"<ul> <li>updated syntax for <code>expressions.py</code> class objects</li> <li>added <code>init_type</code> property to <code>Weights</code> class</li> <li>moved model aesara compile functions from <code>models.MNL</code> to <code>pycmtensor.PyCMTensorModel</code></li> </ul>"},{"location":"about/release_notes/#v120-2022-10-14","title":"v1.2.0 (2022-10-14)","text":""},{"location":"about/release_notes/#feat_3","title":"Feat","text":"<ul> <li>expressions: added Weights class object (#59)</li> <li>functions: added rmse and mae objective functions (#58)</li> <li>batch shuffle for training</li> <li>function: added KL divergence loss function (#50)</li> </ul>"},{"location":"about/release_notes/#fix_5","title":"Fix","text":"<ul> <li>added expand_dims into logit function</li> <li>replace class function Beta.Beta with Beta.beta</li> <li>removed flatten() from logit function</li> </ul>"},{"location":"about/release_notes/#v110-2022-09-23","title":"v1.1.0 (2022-09-23)","text":""},{"location":"about/release_notes/#feat_4","title":"Feat","text":"<ul> <li>scheduler: added learning rate scheduling to train()</li> <li>code: overhaul and cleanup</li> </ul>"},{"location":"about/release_notes/#fix_6","title":"Fix","text":"<ul> <li>environment: update project deps and pre-commit routine</li> <li>config: remove unnecessary cxx flags from macos builds</li> </ul>"},{"location":"about/release_notes/#perf","title":"Perf","text":"<ul> <li>config: misc optimization changes</li> </ul>"},{"location":"about/release_notes/#v107-2022-08-12","title":"v1.0.7 (2022-08-12)","text":""},{"location":"about/release_notes/#v106-2022-08-12","title":"v1.0.6 (2022-08-12)","text":""},{"location":"about/release_notes/#fix_7","title":"Fix","text":"<ul> <li>config: added optimizing speedups to config</li> <li>config: set default <code>cyclic_lr_mode</code> and <code>cyclic_lr_step_size</code> to <code>None</code></li> <li>pre-commit-config: update black to <code>22.6.0</code> in pre-commit check</li> </ul>"},{"location":"about/release_notes/#refactor_3","title":"Refactor","text":"<ul> <li>models: refactored build_functions() into models.py</li> <li>database: refactor set_choice(choiceVar)</li> </ul>"},{"location":"about/release_notes/#v105-2022-07-27","title":"v1.0.5 (2022-07-27)","text":""},{"location":"about/release_notes/#fix_8","title":"Fix","text":"<ul> <li>tests: removed depreciated tests</li> <li>routine: remove depreciated tqdm module</li> </ul>"},{"location":"about/release_notes/#v104-2022-07-27","title":"v1.0.4 (2022-07-27)","text":""},{"location":"about/release_notes/#fix_9","title":"Fix","text":"<ul> <li>pycmtensor.py: update training method</li> <li>config.py: new config option verbosity: \"high\", \"low\"</li> <li>pycmtensor.py: remove warnings for max_iter&lt;patience</li> </ul>"},{"location":"about/release_notes/#v103-2022-05-12","title":"v1.0.3 (2022-05-12)","text":""},{"location":"about/release_notes/#v102-2022-05-12","title":"v1.0.2 (2022-05-12)","text":""},{"location":"about/release_notes/#v101-2022-05-12","title":"v1.0.1 (2022-05-12)","text":""},{"location":"about/release_notes/#fix_10","title":"Fix","text":"<ul> <li>scheduler: fix missing args in input parameters</li> <li>scheduler: fix constantLR missing input paramerer</li> </ul>"},{"location":"about/release_notes/#v100-2022-05-10","title":"v1.0.0 (2022-05-10)","text":""},{"location":"about/release_notes/#feat_5","title":"Feat","text":"<ul> <li>python: update to python 3.10</li> </ul>"},{"location":"about/release_notes/#fix_11","title":"Fix","text":"<ul> <li>tests: update tests files to reflect changes in biogeme removal</li> </ul>"},{"location":"about/release_notes/#v080-2022-05-10","title":"v0.8.0 (2022-05-10)","text":""},{"location":"about/release_notes/#feat_6","title":"Feat","text":"<ul> <li>deps: remove Biogeme dependencies</li> </ul>"},{"location":"about/release_notes/#v071-2022-05-10","title":"v0.7.1 (2022-05-10)","text":""},{"location":"about/release_notes/#fix_12","title":"Fix","text":"<ul> <li>expressions: remove Biogeme dependencies</li> <li>database: remove dependencies of Biogeme</li> <li>debug: remove debug handler after each run to prevent duplication</li> <li>models: add function to return layer output -&gt; get_layer_outputs()</li> <li>debug: disables tqdm if debug mode is on and activates debug_log</li> </ul>"},{"location":"about/release_notes/#refactor_4","title":"Refactor","text":"<ul> <li>move elasticites from models to statistics for consistency</li> </ul>"},{"location":"about/release_notes/#v070-2022-03-17","title":"v0.7.0 (2022-03-17)","text":""},{"location":"about/release_notes/#feat_7","title":"Feat","text":"<ul> <li>models: add functionality to compute elasticities of choice vs attribute in models.py</li> </ul>"},{"location":"about/release_notes/#fix_13","title":"Fix","text":"<ul> <li>results: remove unnessary <code>show_weights</code> option in Results</li> <li>set default max_epoch on training run to adaptive rule</li> <li>print valid config options when invalid options are given as args to train()</li> <li>scheduler: modified cyclic_lr config loading sequence to fix unboundError</li> <li>train: turn saving model off for now</li> <li>config: generate os dependent ld_flags</li> </ul>"},{"location":"about/release_notes/#refactor_5","title":"Refactor","text":"<ul> <li>utils: refactored save_to_pickle and disables it</li> </ul>"},{"location":"about/release_notes/#perf_1","title":"Perf","text":"<ul> <li>IterationTracker: use numpy array to store iteration data</li> </ul>"},{"location":"about/release_notes/#v065-2022-03-14","title":"v0.6.5 (2022-03-14)","text":""},{"location":"about/release_notes/#feat_8","title":"Feat","text":"<ul> <li>models: Implement the ResLogit layer</li> </ul>"},{"location":"about/release_notes/#fix_14","title":"Fix","text":"<ul> <li>config: set default learning schedule to ConstantLR</li> <li>config: set default seed to a random number on init</li> </ul>"},{"location":"about/release_notes/#v064-2022-03-13","title":"v0.6.4 (2022-03-13)","text":""},{"location":"about/release_notes/#feat_9","title":"Feat","text":"<ul> <li>scheduler.py: add new scheduler (CyclicLR) for adaptive LR</li> </ul>"},{"location":"about/release_notes/#fix_15","title":"Fix","text":"<ul> <li>project: fix project metadata and ci</li> <li>config: loadout config from train() to configparser</li> <li>utils: fix TypeError check</li> </ul>"},{"location":"about/release_notes/#v050-2022-03-02","title":"v0.5.0 (2022-03-02)","text":""},{"location":"about/release_notes/#feat_10","title":"Feat","text":"<ul> <li>config: add PyCMTensorConfig class to store config settings</li> <li>expressions: add magic methods lt le gt le ne eq</li> <li>config.py: enable pre-writing of .aesararc config file on module load</li> <li>models: add method prob() to MNLogit to output prob slices</li> <li>time_format: enable logging of build and estimation time</li> <li>results: add Predict class to output probs or discrete choices</li> <li>optimizers: add AdaGram algorithm</li> <li>Database: add getattr build-in type to Database</li> <li>pycmtensor.py: add model.output_choices to generate choices</li> </ul>"},{"location":"about/release_notes/#fix_16","title":"Fix","text":"<ul> <li>statistics: add small value to stderror calculation to address sqrt(0)</li> <li>dependencies: move ipywidgets and pydot to dependencies</li> <li>renamed .rst to .md fix FileNotFoundError</li> <li>result: print more verbose results and options</li> <li>Database: add name to shared_data</li> <li>train: model instance now load initiated model class (not input Class as argument)</li> <li>Database: set choiceVar to mandatory argument</li> <li>PyCMTensor: rename append_to_params to add_params for consistency</li> <li>PyCMTensor: new method to add regularizers to cost function</li> <li>Expressions: invokes different operator for Beta Beta maths</li> <li>show excluded data in model est. output</li> <li>results: standardized naming conventions in modules db-&gt;database</li> <li>tqdm: add arg in train() to enable notebook progressbar</li> <li>swissmetro_test.ipynb: update swissmetro example</li> </ul>"},{"location":"about/release_notes/#refactor_6","title":"Refactor","text":"<ul> <li>PyCMTensor: refactoring models from pycmtensor.py</li> <li>Database: refactor(Database): refactoring database.py from pycmtensor.py</li> <li>optimizers: refactor base Optimizer class</li> <li>moved Beta Weights to expressions.py</li> </ul>"},{"location":"about/release_notes/#perf_2","title":"Perf","text":"<ul> <li>shared_data: improve iteration speed by implementing shared() on input data</li> </ul>"},{"location":"developer_guide/","title":"Developer guide","text":""},{"location":"developer_guide/#virtual-environment","title":"Virtual environment","text":""},{"location":"developer_guide/#installing-dependencies","title":"Installing dependencies","text":""},{"location":"developer_guide/#testing","title":"Testing","text":""},{"location":"developer_guide/api/","title":"API reference","text":""},{"location":"developer_guide/api/__init__/","title":"__init__.py","text":""},{"location":"developer_guide/api/__init__/#pycmtensor","title":"<code>pycmtensor</code>","text":"<p>Top-level package for PyCMTensor.</p>"},{"location":"developer_guide/api/__init__/#pycmtensor.about","title":"<code>about()</code>","text":"<p>Returns a <code>watermark.watermark</code> of various system information for debugging</p>"},{"location":"developer_guide/api/__init__/#config","title":"<code>config</code>","text":"<p>Instance of the <code>Config</code> class</p>"},{"location":"developer_guide/api/config/","title":"defaultconfig.py","text":"<p>See configuration for a list of available configuration settings.</p> <p>PyCMTensor config module</p>"},{"location":"developer_guide/api/config/#pycmtensor.defaultconfig.Config","title":"<code>Config()</code>","text":"<p>Config class object that holds configuration settings</p> <p>Attributes:</p> Name Type Description <code>descriptions</code> <code>dict</code> <p>descriptive documentation of each configuration setting</p> <p>Tip</p> <p>To display a current list of configuration settings, invoke <code>print(pycmtensor.config)</code>.</p> <pre><code>import pycmtensor\nprint(pycmtensor.config)\n</code></pre> <p>Output: <pre><code>PyCMTensor configuration\n...\n</code></pre></p>"},{"location":"developer_guide/api/config/#pycmtensor.defaultconfig.Config.add","title":"<code>add(name, value, description=None)</code>","text":"<p>Method to add a new or update a setting in the configuration</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the parameter</p> required <code>value</code> <code>any</code> <p>the value of the parameter</p> required <code>description</code> <code>str</code> <p>description of the parameter</p> <code>None</code> <p>Example</p> <p>To set the value of the random seed to 100 <pre><code>pycmtensor.config.add('seed', 100, description='seed value')\n</code></pre></p>"},{"location":"developer_guide/api/config/#pycmtensor.defaultconfig.Config.update","title":"<code>update(name, value, *args, **kwargs)</code>","text":"<p>update the config parameter</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the parameter</p> required <code>value</code> <code>any</code> <p>the value of the parameter</p> required <code>*args</code> <code>None</code> <p>overloaded arguments</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>overloaded keyword arguments</p> <code>{}</code>"},{"location":"developer_guide/api/dataset/","title":"dataset.py","text":""},{"location":"developer_guide/api/dataset/#pycmtensor.dataset","title":"<code>pycmtensor.dataset</code>","text":""},{"location":"developer_guide/api/dataset/#pycmtensor.dataset.Dataset","title":"<code>Dataset(df, choice, **kwargs)</code>","text":"<p>Base PyCMTensor Dataset class object</p> <p>This class stores the data in an array format, and a symbolic tensor reference variable object. To call the tensor variable, we invoke the label of the variable as an item in the Dataset class, like so: <pre><code>ds = Dataset(df=df, choice=\"choice\")\nreturn ds[\"label_of_variable\"]  -&gt; TensorVariable\n</code></pre></p> <p>To call the data array, we use the <code>train_dataset()</code> or <code>valid_dataset()</code> method. See method reference for info about the arguments. For example: <pre><code># to get the data array for variable \"time\"\narr = ds.train_dataset(ds[\"time\"])\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>the pandas dataframe object to load</p> required <code>choice</code> <code>str</code> <p>the name of the choice variable</p> required <p>Attributes:</p> Name Type Description <code>n</code> <code>int</code> <p>total number of rows in the dataset</p> <code>x</code> <code>list[TensorVariable]</code> <p>the full list of (input) <code>TensorVariable</code> objects to build the tensor expression from</p> <code>y</code> <code>TensorVariable</code> <p>the output (choice) <code>TensorVariable</code> object</p> <code>scale</code> <code>dict</code> <p>a dictionary of <code>float</code> values to store the scaling factor used for each variable</p> <code>choice</code> <code>str</code> <p>the name of the choice variable</p> <code>ds</code> <code>dict</code> <p>a dictionary of <code>numpy.ndarray</code> to store the values of each variable</p> <code>split_frac</code> <code>float</code> <p>the factor used to split the dataset into training and validation datasets</p> <code>train_index</code> <code>list</code> <p>the list of values of the indices of the training dataset</p> <code>valid_index</code> <code>list</code> <p>the list of values of the indices of the validation dataset</p> <code>n_train</code> <code>int</code> <p>the size of the training dataset</p> <code>n_valid</code> <code>int</code> <p>the size of the validation dataset</p> Example <p>Example initalization of a pandas dataset:</p> <pre><code>ds = Dataset(df=pd.read_csv(\"datafile.csv\", sep=\",\"), choice=\"mode\")\nds.split(frac=0.8)\n</code></pre> <p>Attributes can be access by invoking: <pre><code>print(ds.choice)\n</code></pre></p> <p>Output: <pre><code>'car'\n</code></pre></p>"},{"location":"developer_guide/api/dataset/#pycmtensor.dataset.Dataset.drop","title":"<code>drop(variables)</code>","text":"<p>Method for dropping <code>variables</code> from the dataset</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>list[str]</code> <p>list of <code>str</code> variables from the dataset to drop</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>raises an error if any item in <code>variables</code> is not found in the dataset or item is the choice variable</p> <p>Warning</p> <p>Choice variable cannot be explicity dropped.</p>"},{"location":"developer_guide/api/dataset/#pycmtensor.dataset.Dataset.scale_variable","title":"<code>scale_variable(variable, factor)</code>","text":"<p>Multiply values of the <code>variable</code> by \\(1/\\textrm{factor}\\).</p> <p>Parameters:</p> Name Type Description Default <code>variable</code> <code>str</code> <p>the name of the variable or a list of variable names</p> required <code>factor</code> <code>float</code> <p>the scaling factor</p> required"},{"location":"developer_guide/api/dataset/#pycmtensor.dataset.Dataset.split","title":"<code>split(frac)</code>","text":"<p>Method to split dataset into training and validation subsets</p> <p>Parameters:</p> Name Type Description Default <code>frac</code> <code>float</code> <p>the fraction to split the dataset into the training set. The training set will be indexed from <code>0</code> to <code>frac</code> \\(\\times\\) <code>Dataset.n</code>. The validation dataset will be from the last index of the training set to the last row of the dataset.</p> required Note <p>The actual splitting of the dataset is done during the training procedure, or when invoking the <code>train_dataset()</code> or <code>valid_dataset()</code> methods</p>"},{"location":"developer_guide/api/dataset/#pycmtensor.dataset.Dataset.train_dataset","title":"<code>train_dataset(variables, index=None, batch_size=None, shift=None)</code>","text":"<p>Returns a slice of the (or the full) training data array with the sequence matching the list of variables.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>Union[list, str, TensorVariable]</code> <p>a tensor, label, or list of tensors or list of labels</p> required <code>index</code> <code>int</code> <p>the start of the slice of the data array. If <code>None</code> is given, returns the full data array.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>length of the slice. If <code>None</code> is given, returns the index from <code>index</code> to <code>N</code> where <code>N</code> is the length of the array.</p> <code>None</code> <code>shift</code> <code>int</code> <p>the offset of the slice between <code>0</code> and <code>batch_size</code>. If <code>None</code> is given, <code>shift=0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>a list of array object(s) corresponding to the input variables</p> <p>Example</p> <p>How to retrieve data array from Dataset: <pre><code>ds = Dataset(df, choice=\"choice\")\n\n# index \"age\" and \"location\" data arrays\nreturn ds.train_dataset([ds[\"age\"], ds[\"location\"]])\n\n# similar result\nreturn ds.train_dataset([\"age\", \"location\"])\n</code></pre></p>"},{"location":"developer_guide/api/dataset/#pycmtensor.dataset.Dataset.valid_dataset","title":"<code>valid_dataset(variables, index=None, batch_size=None, shift=None)</code>","text":"<p>Returns a slice of the (or the full) validation data array with the sequence matching the list of variables.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>Union[list, str, TensorVariable]</code> <p>a tensor, label, or list of tensors or list of labels</p> required <code>index</code> <code>int</code> <p>the start of the slice of the data array. If <code>None</code> is given, returns the full data array.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>length of the slice. If <code>None</code> is given, returns the index from <code>index</code> to <code>N</code> where <code>N</code> is the length of the array.</p> <code>None</code> <code>shift</code> <code>int</code> <p>the offset of the slice between <code>0</code> and <code>batch_size</code>. If <code>None</code> is given, <code>shift=0</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>a list of array object(s) corresponding to the input variables</p>"},{"location":"developer_guide/api/expressions/","title":"expressions.py","text":""},{"location":"developer_guide/api/expressions/#pycmtensor.expressions","title":"<code>pycmtensor.expressions</code>","text":"<p>PyCMTensor expressions module</p>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.ExpressionParser","title":"<code>ExpressionParser(expression=None)</code>","text":"<p>             Bases: <code>object</code></p> <p>Base class for the ExpressionParser object</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>TensorVariable</code> <p>the TensorVariable object to evaluate</p> <code>None</code>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.ExpressionParser.parse","title":"<code>parse(expression)</code>  <code>staticmethod</code>","text":"<p>Parses Aesara Tensor string expression from <code>aesara.pprint()</code>. This function removes parentheses and Tensor operators and returns a 'clean' list of expressions</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>TensorVariable</code> <p>the symbolic Tensor object to parse</p> required <p>Returns:</p> Type Description <code>list</code> <p>found keywords in expressions</p>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.TensorExpressions","title":"<code>TensorExpressions()</code>","text":"<p>Base class for expression objects</p>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.Param","title":"<code>Param(name, value=0.0, lb=None, ub=None, status=0)</code>","text":"<p>             Bases: <code>TensorExpressions</code></p> <p>Constructor for model param object</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of parameter</p> required <code>value</code> <code>float</code> <p>the default value of the parameter</p> <code>0.0</code> <code>lb</code> <code>float</code> <p>value lower bound</p> <code>None</code> <code>ub</code> <code>float</code> <p>value upper bound</p> <code>None</code> <code>status</code> <code>int</code> <p>if 1, do not estimate this parameter</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>init_value</code> <code>float</code> <p>the inital value set at object creation</p> <code>shape</code> <code>list</code> <p>the shape of the Param</p> <p>Note</p> <p><code>init_value</code> is an immutable property</p>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.Param.name","title":"<code>name</code>  <code>property</code>","text":"<p>Returns the name of the object</p>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.Param.__call__","title":"<code>__call__()</code>","text":"<p>Returns the shared value</p>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.Param.get_value","title":"<code>get_value()</code>","text":"<p>Returns the numpy representation of the parameter value</p>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.Param.set_value","title":"<code>set_value(value)</code>","text":"<p>Set the value of the shared variable</p>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.Param.reset_value","title":"<code>reset_value()</code>","text":"<p>Resets the value of the shared variable to the initial value</p>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.Beta","title":"<code>Beta(name, value=0.0, lb=None, ub=None, status=0)</code>","text":"<p>             Bases: <code>Param</code></p> <p>Constructor for Beta parameter</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of Beta parameter</p> required <code>value</code> <code>float</code> <p>the default value of the Beta parameter</p> <code>0.0</code> <code>lb</code> <code>float</code> <p>value lower bound</p> <code>None</code> <code>ub</code> <code>float</code> <p>value upper bound</p> <code>None</code> <code>status</code> <code>int</code> <p>if 1, do not estimate this Beta parameter</p> <code>0</code> <p>Example</p> <p>Specifying a Beta parameter:</p> <pre><code>b_time = Beta(\"b_time\", value=0., lb=None, ub=0., status=0)\n</code></pre> <p>To obtain the raw array value from the Beta parameter: <pre><code>b_time = Beta(\"b_time\", value=-1., lb=None, ub=0., status=0)\nb_time.get_value()\n</code></pre></p> <p>output: <pre><code>array(-1.)\n</code></pre></p>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.RandomDraws","title":"<code>RandomDraws(name, draw_type, n_draws)</code>","text":"<p>             Bases: <code>TensorExpressions</code></p> <p>Constructor for model random draws</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the RandomDraw object</p> required <code>draw_type</code> <code>str</code> <p>the distribution of the draw</p> required <code>n_draws</code> <code>int</code> <p>number of draws, determines the size of this shared tensor</p> required <p>Note</p> <p><code>draw_type</code> can be the following:</p> <ul> <li><code>\"normal\"</code></li> <li><code>\"lognormal\"</code></li> <li><code>\"gumbel\"</code></li> <li><code>\"exponential\"</code></li> <li><code>\"gamma\"</code></li> <li><code>\"poisson\"</code></li> </ul>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.RandomDraws.name","title":"<code>name</code>  <code>property</code>","text":"<p>returns the name of the random draw tensor variable</p>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.Bias","title":"<code>Bias(name, size, value=None)</code>","text":"<p>             Bases: <code>Param</code></p> <p>Class object for neural net bias vector</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the parameter</p> required <code>size</code> <code>Union[tuple, list]</code> <p>size of the array in 1 dimension</p> required <code>value</code> <code>ndarray</code> <p>initial values of the parameter, if <code>None</code> given, defaults to <code>0</code></p> <code>None</code>"},{"location":"developer_guide/api/expressions/#pycmtensor.expressions.Weight","title":"<code>Weight(name, size, value=None, init_type=None)</code>","text":"<p>             Bases: <code>Param</code></p> <p>Class object for neural net weight matrix</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the parameter</p> required <code>size</code> <code>Union[tuple, list]</code> <p>size of the array</p> required <code>value</code> <code>ndarray</code> <p>initial values of the parameter. Defaults to <code>random.uniform(-0.1, 0.1, size)</code></p> <code>None</code> <code>init_type</code> <code>str</code> <p>initialization type, see notes</p> <code>None</code> Note <p>Initialization types are one of the following:</p> <ul> <li> <p><code>\"zeros\"</code>: a 2-D array of zeros</p> </li> <li> <p><code>\"he\"</code>: initialization method for neural networks that takes into account   the non-linearity of activation functions, e.g. ReLU or Softplus [^1]</p> </li> <li> <p><code>\"glorot\"</code>: initialization method that maintains the variance for   symmetric activation functions, e.g. sigm, tanh [^2]</p> </li> </ul> <p>[^1] He, K., Zhang, X., Ren, S. and Sun, J., 2015. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision (pp. 1026-1034). [^2] Glorot, X. and Bengio, Y., 2010, March. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256). JMLR Workshop and Conference Proceedings.</p> <p>Example</p> <p>Specifying a weight array:</p> <pre><code>code\n</code></pre>"},{"location":"developer_guide/api/functions/","title":"functions.py","text":""},{"location":"developer_guide/api/functions/#pycmtensor.functions","title":"<code>pycmtensor.functions</code>","text":"<p>PyCMTensor functions module</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.relu","title":"<code>relu(x, alpha=0.0)</code>","text":"<p>Compute the element-wise rectified linear activation function.</p> <p>Source taken from Theano 0.7.1</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>TensorVariable</code> <p>symbolic tensor</p> required <code>alpha</code> <code>Union[float, TensorSharedVariable]</code> <p>Slope for negative input, usually between 0 and 1. The default value of 0 will lead to the standard rectifier, 1 will lead to a linear activation function, and any value in between will give a leaky rectifier. A shared variable (broadcastable against <code>x</code>) will result in a parameterized rectifier with learnable slope (s).</p> <code>0.0</code> <p>Returns:</p> Type Description <code>TensorVariable</code> <p>Elementwise rectifier applied to <code>x</code>.</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.neg_relu","title":"<code>neg_relu(x, alpha=0.0)</code>","text":"<p>negative variant of relu</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.exp_mov_average","title":"<code>exp_mov_average(batch_avg, moving_avg, alpha=0.1)</code>","text":"<p>Calculates the exponential moving average (EMA) of a new minibatch</p> <p>Parameters:</p> Name Type Description Default <code>batch_avg</code> <code>TensorVariable</code> <p>mean batch value</p> required <code>moving_avg</code> <code>TensorVariable</code> <p>accumulated mean</p> required <code>alpha</code> <code>float</code> <p>ratio of moving average to batch average</p> <code>0.1</code> <p>Returns:</p> Type Description <code>TensorVariable</code> <p>the new moving average</p> <p>Note</p> <p>The moving average will decay by the difference between the existing value and the new value multiplied by the moving average factor. A higher <code>alpha</code> value results in faster changing moving average.</p> <p>Formula:</p> \\[ x_{EMA} = \\alpha * x_t + x_{EMA} * (1-\\alpha) \\]"},{"location":"developer_guide/api/functions/#pycmtensor.functions.logit","title":"<code>logit(utility, avail=None)</code>","text":"<p>Computes the Logit function, with availability conditions.</p> <p>Parameters:</p> Name Type Description Default <code>utility</code> <code>Union[list, tuple, TensorVariable]</code> <p>utility equations</p> required <code>avail</code> <code>Union[list, tuple, TensorVariable]</code> <p>availability conditions, if no availability conditions are provided, defaults to <code>1</code> for all availabilities.</p> <code>None</code> <p>Returns:</p> Type Description <code>TensorVariable</code> <p>A NxM matrix of probabilities.</p> Note <p>The 0-th dimension is the numbering of alternatives, the N-th dimension is the size of the input (# rows).</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.log_likelihood","title":"<code>log_likelihood(prob, y, index=None)</code>","text":"<p>Symbolic representation of the log likelihood cost function.</p> <p>Parameters:</p> Name Type Description Default <code>prob</code> <code>TensorVariable</code> <p>choice probabilites tensor</p> required <code>y</code> <code>TensorVariable</code> <p>choice variable tensor</p> required <code>index</code> <code>TensorVariable</code> <p>index tensor, if <code>None</code>, dynamically get the same of the <code>y</code> tensor</p> <code>None</code> <p>Returns:</p> Type Description <code>TensorVariable</code> <p>a symbolic tensor of the log likelihood</p> Note <p>The 0-th dimension is the numbering of alternatives, the N-th dimension is the size of the input (# rows).</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.rmse","title":"<code>rmse(y_hat, y)</code>","text":"<p>Computes the root mean squared error (RMSE) between pairs of observations</p> <p>Parameters:</p> Name Type Description Default <code>y_hat</code> <code>TensorVariable</code> <p>model estimated values</p> required <code>y</code> <code>TensorVariable</code> <p>ground truth values</p> required <p>Returns:</p> Type Description <code>TensorVariable</code> <p>symbolic scalar representation of the rmse</p> Note <p>Tensor is flattened to a <code>dim=1</code> vector if the input tensor is <code>dim=2</code>.</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.mae","title":"<code>mae(y_hat, y)</code>","text":"<p>Computes the mean absolute error (MAE) between pairs of observations</p> <p>Parameters:</p> Name Type Description Default <code>y_hat</code> <code>TensorVariable</code> <p>model estimated values</p> required <code>y</code> <code>TensorVariable</code> <p>ground truth values</p> required <p>Returns:</p> Type Description <code>TensorVariable</code> <p>symbolic scalar representation of the mean absolute error</p> Note <p>Tensor is flattened to a <code>dim=1</code> vector if the input tensor is <code>dim=2</code>.</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.kl_divergence","title":"<code>kl_divergence(p, q)</code>","text":"<p>Computes the KL divergence loss between discrete distributions <code>p</code> and <code>q</code>.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>TensorVariable</code> <p>model output probabilities</p> required <code>q</code> <code>TensorVariable</code> <p>ground truth probabilities</p> required <p>Returns:</p> Type Description <code>TensorVariable</code> <p>a symbolic representation of the KL loss </p> Note <p>Formula:</p> \\[ L = \\begin{cases}     \\sum_{i=1}^N (p_i * log(p_i/q_i)) &amp; p&gt;0\\\\     0 &amp; p&lt;=0 \\end{cases} \\]"},{"location":"developer_guide/api/functions/#pycmtensor.functions.kl_multivar_norm","title":"<code>kl_multivar_norm(m0, v0, m1, v1, epsilon=1e-06)</code>","text":"<p>Computes the KL divergence loss between two multivariate normal distributions.</p> <p>Parameters:</p> Name Type Description Default <code>m0</code> <code>TensorVariable</code> <p>mean vector of the first Normal m.v. distribution \\(N_0\\)</p> required <code>v0</code> <code>TensorVariable</code> <p>(co-)variance matrix of the first Normal m.v. distribution \\(N_0\\)</p> required <code>m1</code> <code>TensorVariable</code> <p>mean vector of the second Normal m.v. distribution \\(N_1\\)</p> required <code>v1</code> <code>TensorVariable</code> <p>(co-)variance of the second Normal m.v. distribution \\(N_1\\)</p> required <code>epsilon</code> <code>float</code> <p>small value to prevent divide-by-zero error</p> <code>1e-06</code> Note <p>k = dimension of the distribution.</p> <p>Formula:</p> \\[     D_{KL}(N_0||N_1) = 0.5 * \\Big(\\ln\\big(\\frac{|v_1|}{|v_0|}\\big) + trace(v_1^{-1} v_0) + (m_1-m_0)^T v_1^{-1} (m_1-m_0) - k\\Big) \\] <p>In variational inference, the kl divergence is the relative entropy between a diagonal multivariate Normal and a standard Normal distribution, \\(N(0, 1)\\), therefore, for VI, <code>m1=1</code>, <code>v1=1</code></p> <p>For two univariate distributions, dimensions of <code>m0,m1,v0,v1 = 0</code></p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.errors","title":"<code>errors(prob, y)</code>","text":"<p>Symbolic representation of the discrete prediction as a percentage error.</p> <p>Parameters:</p> Name Type Description Default <code>prob</code> <code>TensorVariable</code> <p>choice probabilites tensor</p> required <code>y</code> <code>TensorVariable</code> <p>choice variable tensor</p> required <p>Returns:</p> Type Description <code>TensorVariable</code> <p>the mean prediction error over <code>y</code></p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.second_order_derivative","title":"<code>second_order_derivative(cost, params)</code>","text":"<p>Symbolic representation of the 2<sup>nd</sup> order Hessian matrix given cost.</p> <p>Parameters:</p> Name Type Description Default <code>cost</code> <code>TensorVariable</code> <p>function to compute the gradients over</p> required <code>params</code> <code>List[Beta]</code> <p>params to compute the gradients over</p> required <p>Returns:</p> Type Description <code>TensorVariable</code> <p>the Hessian matrix of the cost function wrt to the params</p> Note <p>Parameters with <code>status=1</code> are ignored.</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.first_order_derivative","title":"<code>first_order_derivative(cost, params)</code>","text":"<p>Symbolic representation of the 1<sup>st</sup> order gradient vector given the cost.</p> <p>Parameters:</p> Name Type Description Default <code>cost</code> <code>TensorVariable</code> <p>function to compute the gradients over</p> required <code>params</code> <code>List[Beta]</code> <p>params to compute the gradients over</p> required <p>Returns:</p> Type Description <code>TensorVariable</code> <p>the gradient vector of the cost function wrt to the params</p> Note <p>Parameters with <code>status=1</code> are ignored.</p>"},{"location":"developer_guide/api/logger/","title":"logger.py","text":""},{"location":"developer_guide/api/logger/#pycmtensor.logger","title":"<code>pycmtensor.logger</code>","text":"<p>PyCMTensor logger module</p> <p>This module sets the logging state of the program. Verbosity is defined by <code>set_level()</code></p>"},{"location":"developer_guide/api/logger/#pycmtensor.logger.set_level","title":"<code>set_level(level)</code>","text":"<p>Set the level of the logger. The higher the number, the higher the verbosity.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>the level of the logger - <code>DEBUG</code> = 10 - <code>INFO</code> = 20 (DEFAULT) - <code>WARNING</code> = 30 - <code>ERROR</code> = 40 - <code>CRITICAL</code> = 50</p> required"},{"location":"developer_guide/api/logger/#pycmtensor.logger.get_effective_level","title":"<code>get_effective_level()</code>","text":"<p>Gets the level of the logger</p>"},{"location":"developer_guide/api/optimizers/","title":"optimizers.py","text":""},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers","title":"<code>pycmtensor.optimizers</code>","text":"<p>PyCMTensor optimizers module</p>"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.Optimizer","title":"<code>Optimizer(name, epsilon=1e-08, **kwargs)</code>","text":"<p>Base optimizer class</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the optimizer</p> required"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.Optimizer.update","title":"<code>update(cost, params, lr)</code>","text":"<p>Update parameters for aesara function calls</p> <p>Parameters:</p> Name Type Description Default <code>cost</code> <code>TensorVariable</code> <p>a scalar element for the expression of the cost function where the derivatives are calculated</p> required <code>params</code> <code>list[TensorSharedVariable]</code> <p>parameters of the model</p> required <code>lr</code> <code>Union[float, TensorSharedVariable]</code> <p>the learning rate</p> required <p>Returns:</p> Type Description <code>list</code> <p>a list of <code>(param, param_new)</code> tuple pairs</p>"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.Adam","title":"<code>Adam(params, b1=0.9, b2=0.999, **kwargs)</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>An optimizer that implments the Adam algorithm[^1]</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[TensorSharedVariable]</code> <p>parameters of the model</p> required <code>b1</code> <code>float</code> <p>exponential decay rate for the 1<sup>st</sup> moment estimates. Defaults to <code>0.9</code></p> <code>0.9</code> <code>b2</code> <code>float</code> <p>exponential decay rate for the 2<sup>nd</sup> moment estimates. Defaults to <code>0.999</code></p> <code>0.999</code> <p>Attributes:</p> Name Type Description <code>t</code> <code>TensorSharedVariable</code> <p>time step</p> <code>m_prev</code> <code>list[TensorSharedVariable]</code> <p>previous time step momentum</p> <code>v_prev</code> <code>list[TensorSharedVariable]</code> <p>previous time step velocity</p> <ol> <li> <p>Kingma et al., 2014. Adam: A Method for Stochastic Optimization. http://arxiv.org/abs/1412.6980 \u21a9</p> </li> </ol>"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.AdamW","title":"<code>AdamW(params, b1=0.9, b2=0.999, **kwargs)</code>","text":"<p>             Bases: <code>Adam</code></p> <p>Adam with weight decay</p>"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.Nadam","title":"<code>Nadam(params, b1=0.99, b2=0.999, **kwargs)</code>","text":"<p>             Bases: <code>Adam</code></p> <p>An optimizer that implements the Nesterov Adam algorithm[^1]</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[TensorSharedVariable]</code> <p>parameters of the model</p> required <code>b1</code> <code>float</code> <p>exponential decay rate for the 1<sup>st</sup> moment estimates. Defaults to <code>0.9</code></p> <code>0.99</code> <code>b2</code> <code>float</code> <p>exponential decay rate for the 2<sup>nd</sup> moment estimates. Defaults to <code>0.999</code></p> <code>0.999</code> <p>Attributes:</p> Name Type Description <code>t</code> <code>TensorSharedVariable</code> <p>time step</p> <code>m_prev</code> <code>list[TensorSharedVariable]</code> <p>previous time step momentum</p> <code>v_prev</code> <code>list[TensorSharedVariable]</code> <p>previous time step velocity</p> <ol> <li> <p>Dozat, T., 2016. Incorporating nesterov momentum into adam.(2016). Dostupn\u00e9 z: http://cs229.stanford.edu/proj2015/054_report.pdf.\u00a0\u21a9</p> </li> </ol>"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.Adamax","title":"<code>Adamax(params, b1=0.9, b2=0.999, **kwargs)</code>","text":"<p>             Bases: <code>Adam</code></p> <p>An optimizer that implements the Adamax algorithm[^1]. It is a variant of the Adam algorithm</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[TensorSharedVariable]</code> <p>parameters of the model</p> required <code>b1</code> <code>float</code> <p>exponential decay rate for the 1<sup>st</sup> moment estimates. Defaults to <code>0.9</code></p> <code>0.9</code> <code>b2</code> <code>float</code> <p>exponential decay rate for the 2<sup>nd</sup> moment estimates. Defaults to <code>0.999</code></p> <code>0.999</code> <p>Attributes:</p> Name Type Description <code>t</code> <code>TensorSharedVariable</code> <p>time step</p> <code>m_prev</code> <code>list[TensorSharedVariable]</code> <p>previous time step momentum</p> <code>v_prev</code> <code>list[TensorSharedVariable]</code> <p>previous time step velocity</p> <ol> <li> <p>Kingma et al., 2014. Adam: A Method for Stochastic Optimization. http://arxiv.org/abs/1412.6980 \u21a9</p> </li> </ol>"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.Adadelta","title":"<code>Adadelta(params, rho=0.95, **kwargs)</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>An optimizer that implements the Adadelta algorithm[^1]</p> <p>Adadelta is a stochastic gradient descent method that is based on adaptive learning rate per dimension to address two drawbacks:</p> <ul> <li>The continual decay of learning rates throughout training</li> <li>The need for a manually selected global learning rate</li> </ul> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[TensorSharedVariable]</code> <p>parameters of the model</p> required <code>rho</code> <code>float</code> <p>the decay rate for learning rate. Defaults to <code>0.95</code></p> <code>0.95</code> <p>Attributes:</p> Name Type Description <code>accumulator</code> <code>list[TensorSharedVariable]</code> <p>gradient accumulator</p> <code>delta</code> <code>list[TensorSharedVariable]</code> <p>adaptive difference between gradients</p> <ol> <li> <p>Zeiler, 2012. ADADELTA: An Adaptive Learning Rate Method. http://arxiv.org/abs/1212.5701 \u21a9</p> </li> </ol>"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.RProp","title":"<code>RProp(params, inc=1.05, dec=0.5, bounds=[1e-06, 50.0], **kwargs)</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>An optimizer that implements the Rprop algorithm[^1]</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[TensorSharedVariable]</code> <p>parameters of the model</p> required <code>inc</code> <code>float</code> <p>increment step if same gradient direction</p> <code>1.05</code> <code>dec</code> <code>float</code> <p>decrement step if different gradient direction</p> <code>0.5</code> <code>bounds</code> <code>List[float]</code> <p>min and maximum bounds for increment step</p> <code>[1e-06, 50.0]</code> <p>Attributes:</p> Name Type Description <code>factor</code> <code>List[TensorVariable]</code> <p>learning rate factor multiplier (init=1.)</p> <code>ghat</code> <code>List[TensorVariable]</code> <p>previous step gradients</p> <ol> <li> <p>Igel, C., &amp; H\u00fcsken, M. (2003). Empirical evaluation of the improved Rprop learning algorithms. Neurocomputing, 50, 105-123.\u00a0\u21a9</p> </li> </ol>"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.RMSProp","title":"<code>RMSProp(params, rho=0.9, **kwargs)</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>An optimizer that implements the RMSprop algorithm[^1]</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[TensorSharedVariable]</code> <p>parameters of the model</p> required <code>rho</code> <code>float</code> <p>discounting factor for the history/coming gradient. Defaults to <code>0.9</code></p> <code>0.9</code> <p>Attributes:</p> Name Type Description <code>accumulator</code> <code>TensorVariable</code> <p>gradient accumulator</p> <ol> <li> <p>Hinton, 2012. rmsprop: Divide the gradient by a running average of its recent magnitude. http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf \u21a9</p> </li> </ol>"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.Momentum","title":"<code>Momentum(params, mu=0.9, **kwargs)</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>An optimizer that implements the Momentum algorithm[^1]</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[TensorSharedVariable]</code> <p>parameters of the model</p> required <code>mu</code> <code>float</code> <p>acceleration factor in the relevant direction and dampens oscillations. Defaults to <code>0.9</code></p> <code>0.9</code> <p>Attributes:</p> Name Type Description <code>velocity</code> <code>list[TensorSharedVariable]</code> <p>momentum velocity</p> <ol> <li> <p>Sutskever et al., 2013. On the importance of initialization and momentum in deep learning. http://jmlr.org/proceedings/papers/v28/sutskever13.pdf \u21a9</p> </li> </ol>"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.NAG","title":"<code>NAG(params, mu=0.99, **kwargs)</code>","text":"<p>             Bases: <code>Momentum</code></p> <p>An optimizer that implements the Nestrov Accelerated Gradient algorithm[^1]</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[TensorSharedVariable]</code> <p>parameters of the model</p> required <code>mu</code> <code>float</code> <p>acceleration factor in the relevant direction and dampens oscillations. Defaults to <code>0.9</code></p> <code>0.99</code> <p>Attributes:</p> Name Type Description <code>t</code> <code>TensorSharedVariable</code> <p>momentum time step</p> <code>velocity</code> <code>list[TensorSharedVariable]</code> <p>momentum velocity</p> <ol> <li> <p>Sutskever et al., 2013. On the importance of initialization and momentum in deep learning. http://jmlr.org/proceedings/papers/v28/sutskever13.pdf \u21a9</p> </li> </ol>"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.AdaGrad","title":"<code>AdaGrad(params, **kwargs)</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>An optimizer that implements the Adagrad algorithm[^1]</p> <p>Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the updates.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[TensorSharedVariable]</code> <p>parameters of the model</p> required <p>Attributes:</p> Name Type Description <code>accumulator</code> <code>list[TensorSharedVariable]</code> <p>gradient accumulators</p> <ol> <li> <p>Duchi et al., 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf \u21a9</p> </li> </ol>"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.SGD","title":"<code>SGD(params, **kwargs)</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>An optimizer that implements the stochastic gradient algorithm</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[TensorSharedVariable]</code> <p>parameters of the model</p> required"},{"location":"developer_guide/api/optimizers/#pycmtensor.optimizers.SQNBFGS","title":"<code>SQNBFGS(params, config=None, **kwargs)</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>A L-BFGS optimizer implementing the adaptive stochastic Quasi-Newton (SQN) based approach [^1]</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[TensorSharedVariable]</code> <p>parameters of the model</p> required <code>config</code> <code>config</code> <p>pycmtensor config object</p> <code>None</code> <ol> <li> <p>Byrd, R. H., Hansen, S. L., Nocedal, J., &amp; Singer, Y. (2016). A stochastic quasi-Newton method for large-scale optimization. SIAM Journal on Optimization, 26(2), 1008-1031.\u00a0\u21a9</p> </li> </ol>"},{"location":"developer_guide/api/results/","title":"results.py","text":""},{"location":"developer_guide/api/results/#pycmtensor.results","title":"<code>pycmtensor.results</code>","text":"<p>PyCMTensor results module</p> <p>This module provides the results of the estimated model and output formatting</p>"},{"location":"developer_guide/api/results/#pycmtensor.results.Results","title":"<code>Results()</code>","text":"<p>             Bases: <code>object</code></p> <p>Base class object to hold model results and scores</p> <p>Attributes:</p> Name Type Description <code>build_time</code> <code>str</code> <p>string formatted time stamp of the duration of the build stage</p> <code>train_time</code> <code>str</code> <p>string formatted time stamp of the duration of the training stage</p> <code>epochs_per_sec</code> <code>float</code> <p>number of epochs of the training dataset per second, benchmark for calculation speed</p> <code>n_params</code> <code>int</code> <p>total number of parameters, used for calculating statistics</p> <code>n_train</code> <code>int</code> <p>number of training samples used</p> <code>n_valid</code> <code>int</code> <p>number of validation samples used</p> <code>seed</code> <code>int</code> <p>random seed value used</p> <code>null_loglikelihood</code> <code>float</code> <p>null log likelihood of the model</p> <code>best_loglikelihood</code> <code>float</code> <p>the final estimated model log likelihood</p> <code>best_valid_error</code> <code>float</code> <p>final estimated model validation error</p> <code>best_epoch</code> <code>int</code> <p>the epoch number at the final estimated model</p> <code>gnorm</code> <code>float</code> <p>the gradient norm at the final estimated model</p> <code>hessian_matrix</code> <code>ndarray</code> <p>the 2-D hessian matrix</p> <code>bhhh_matrix</code> <code>ndarray</code> <p>the 3-D bhhh matrix where the 1<sup>st</sup> dimension is the length of the dataset and the last 2 dimensions are the matrix for each data observation</p> <code>statistics_graph</code> <code>dict</code> <p>a dictionary containing the learning_rate, training, and validation statistics.</p>"},{"location":"developer_guide/api/results/#pycmtensor.results.Results.rho_square","title":"<code>rho_square()</code>","text":"<p>rho square value of the model</p> <p>Returns:</p> Type Description <code>float</code> <p>analogue for the model fit</p>"},{"location":"developer_guide/api/results/#pycmtensor.results.Results.rho_square_bar","title":"<code>rho_square_bar()</code>","text":"<p>McFadden's adjusted rho square</p> <p>Returns:</p> Type Description <code>float</code> <p>the adjusted McFadden's rho square value</p>"},{"location":"developer_guide/api/results/#pycmtensor.results.Results.loglikelihood_ratio_test","title":"<code>loglikelihood_ratio_test()</code>","text":"<p>Log likelihood ratio test</p> <p>Returns:</p> Type Description <code>float</code> <p>the log likelihood ratio test: $-2 times (NLL-LL)</p>"},{"location":"developer_guide/api/results/#pycmtensor.results.Results.AIC","title":"<code>AIC()</code>","text":"<p>Akaike information criterion</p> <p>Returns:</p> Type Description <code>float</code> <p>the AIC of the model</p>"},{"location":"developer_guide/api/results/#pycmtensor.results.Results.BIC","title":"<code>BIC()</code>","text":"<p>Bayesian information criterion, adjusted for the number of parameters and number of training samples</p> <p>Returns:</p> Type Description <code>float</code> <p>the BIC of the model</p>"},{"location":"developer_guide/api/results/#pycmtensor.results.Results.benchmark","title":"<code>benchmark()</code>","text":"<p>benchmark statistics</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Summary of the model performance benchmark</p>"},{"location":"developer_guide/api/results/#pycmtensor.results.Results.model_statistics","title":"<code>model_statistics()</code>","text":"<p>model statistics</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Summary of the model statistics</p>"},{"location":"developer_guide/api/results/#pycmtensor.results.Results.beta_statistics","title":"<code>beta_statistics()</code>","text":"<p>Beta statistics</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Summary of the estimated Beta statistics</p>"},{"location":"developer_guide/api/results/#pycmtensor.results.Results.model_correlation_matrix","title":"<code>model_correlation_matrix()</code>","text":"<p>Correlation matrix calculated from the hessian</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The correlation matrix</p>"},{"location":"developer_guide/api/results/#pycmtensor.results.Results.model_robust_correlation_matrix","title":"<code>model_robust_correlation_matrix()</code>","text":"<p>Robust correlation matrix calculated from the hessian and bhhh</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The robust correlation matrix</p>"},{"location":"developer_guide/api/results/#pycmtensor.results.Results.show_training_plot","title":"<code>show_training_plot(sample_intervals=1)</code>","text":"<p>Displays the statistics graph as a line plot</p> <p>Parameters:</p> Name Type Description Default <code>sample_intervals</code> <code>int</code> <p>plot only the n-th point from the statistics</p> <code>1</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The statistics as a pandas dataframe</p>"},{"location":"developer_guide/api/scheduler/","title":"scheduler.py","text":""},{"location":"developer_guide/api/scheduler/#pycmtensor.scheduler","title":"<code>pycmtensor.scheduler</code>","text":"<p>PyCMTensor scheduler module</p> <p>This module contains the implementation of the learning rate scheduler. By default, a constant learning rate is used.</p>"},{"location":"developer_guide/api/scheduler/#pycmtensor.scheduler.Scheduler","title":"<code>Scheduler(lr)</code>","text":"<p>Base class for learning rate scheduler</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>the base learning rate</p> required <p>Attributes:     history (list): (iteration #, lr) tuples</p>"},{"location":"developer_guide/api/scheduler/#pycmtensor.scheduler.Scheduler.record","title":"<code>record(lr)</code>","text":"<p>Saves the history of the learning rate and returns the current learning rate</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>the learning rate</p> required <p>Returns:</p> Type Description <code>float</code> <p>the current learning rate</p>"},{"location":"developer_guide/api/scheduler/#pycmtensor.scheduler.ConstantLR","title":"<code>ConstantLR(lr=0.01, **kwargs)</code>","text":"<p>             Bases: <code>Scheduler</code></p> <p>Base class for constant learning rate scheduler</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>initial learning rate</p> <code>0.01</code> <code>**kwargs</code> <code>dict</code> <p>overloaded keyword arguments</p> <code>{}</code>"},{"location":"developer_guide/api/scheduler/#pycmtensor.scheduler.StepLR","title":"<code>StepLR(lr=0.01, factor=0.95, drop_every=10, **kwargs)</code>","text":"<p>             Bases: <code>Scheduler</code></p> <p>Base class for step learning rate scheduler</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>initial learning rate</p> <code>0.01</code> <code>factor</code> <code>float</code> <p>percentage reduction to the learning rate</p> <code>0.95</code> <code>drop_every</code> <code>int</code> <p>step down the learning rate after every n steps</p> <code>10</code> <code>**kwargs</code> <code>dict</code> <p>overloaded keyword arguments</p> <code>{}</code>"},{"location":"developer_guide/api/scheduler/#pycmtensor.scheduler.PolynomialLR","title":"<code>PolynomialLR(max_epochs, lr=0.01, power=1.0, **kwargs)</code>","text":"<p>             Bases: <code>Scheduler</code></p> <p>Base class for polynomial decay learning rate scheduler</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>initial learning rate value</p> <code>0.01</code> <code>max_epochs</code> <code>int</code> <p>the max number of training epochs</p> required <code>power</code> <code>float</code> <p>the exponential factor to decay</p> <code>1.0</code> <code>**kwargs</code> <code>dict</code> <p>overloaded keyword arguments</p> <code>{}</code>"},{"location":"developer_guide/api/scheduler/#pycmtensor.scheduler.Triangular2CLR","title":"<code>Triangular2CLR(lr=0.01, max_lr=0.1, cycle_steps=16, **kwargs)</code>","text":"<p>             Bases: <code>CyclicLR</code></p> <p>Base class for Triangular Cyclic LR scheduler. The scaling of the Triangular Cyclic function is:</p> \\[ scale = \\frac{1}{2^{step-1}} \\] <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>initial learning rate value</p> <code>0.01</code> <code>max_lr</code> <code>float</code> <p>peak learning rate value</p> <code>0.1</code> <code>cycle_steps</code> <code>int</code> <p>the number of steps to complete a cycle</p> <code>16</code> <code>**kwargs</code> <code>dict</code> <p>overloaded keyword arguments</p> <code>{}</code>"},{"location":"developer_guide/api/scheduler/#pycmtensor.scheduler.ExpRangeCLR","title":"<code>ExpRangeCLR(lr=0.01, max_lr=0.1, cycle_steps=16, gamma=0.5, **kwargs)</code>","text":"<p>             Bases: <code>CyclicLR</code></p> <p>Base class for exponential range Cyclic LR scheduler. The scaling is:</p> \\[ scale = \\gamma^{step} \\] <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>initial learning rate value</p> <code>0.01</code> <code>max_lr</code> <code>float</code> <p>peak learning rate value</p> <code>0.1</code> <code>cycle_steps</code> <code>int</code> <p>the number of steps to complete a cycle</p> <code>16</code> <code>gamma</code> <code>float</code> <p>exponential parameter. Default=0.5</p> <code>0.5</code> <code>**kwargs</code> <code>dict</code> <p>overloaded keyword arguments</p> <code>{}</code>"},{"location":"developer_guide/api/statistics/","title":"statistics.py","text":""},{"location":"developer_guide/api/statistics/#pycmtensor.statistics","title":"<code>pycmtensor.statistics</code>","text":"<p>PyCMTensor statistics module</p> <p>This module contains methods for calculating the statistics of the estimated parameters.</p>"},{"location":"developer_guide/api/statistics/#pycmtensor.statistics.variance_covariance","title":"<code>variance_covariance(hessian)</code>","text":"<p>computes the variance covariance matrix given the Hessian</p> <p>Parameters:</p> Name Type Description Default <code>hessian</code> <code>ndarray</code> <p>a 2-D hessian matrix</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>the variance covariance matrix</p> <p>Notes</p> <p>The variance covariance matrix is calculated by taking the inverse of the (negative) hessian matrix. If the inverse is undefined, returns a zero or a large finite number.</p> \\[ varcovar = -H^{-1} \\]"},{"location":"developer_guide/api/statistics/#pycmtensor.statistics.rob_variance_covariance","title":"<code>rob_variance_covariance(hessian, bhhh)</code>","text":"<p>computes the robust variance covariance matrix given the Hessian and the BHHH matrices</p> <p>Parameters:</p> Name Type Description Default <code>hessian</code> <code>ndarray</code> <p>the hessian matrix</p> required <code>bhhh</code> <code>ndarray</code> <p>the BHHH matrix</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>the robust variance covariance matrix</p> <p>Notes</p> <p>The robust variance covariance matrix is computed as follows:</p> \\[ rob. varcovar = (-H)^{-1}\\cdot BHHH\\cdot (-H)^{-1} \\]"},{"location":"developer_guide/api/statistics/#pycmtensor.statistics.t_test","title":"<code>t_test(stderr, params)</code>","text":"<p>computes the statistical t-test of the estimated parameter and the standard error</p> <p>Parameters:</p> Name Type Description Default <code>stderr</code> <code>list</code> <p>standard errors</p> required <code>params</code> <code>list</code> <p>estimated parameters</p> required <p>Returns:</p> Type Description <code>list</code> <p>t-test of the estimated parameters</p>"},{"location":"developer_guide/api/statistics/#pycmtensor.statistics.p_value","title":"<code>p_value(stderr, params)</code>","text":"<p>computes the p-value (statistical significance) of the estimated parameter using the two-tailed normal distribution, where p-value=\\(2(1-\\phi(|t|)\\), \\(\\phi\\) is the cdf of the normal distribution</p> <p>Parameters:</p> Name Type Description Default <code>stderr</code> <code>list</code> <p>standard errors</p> required <code>params</code> <code>list</code> <p>estimated parameters</p> required <p>Returns:</p> Type Description <code>list</code> <p>p-value of the estimated parameters</p>"},{"location":"developer_guide/api/statistics/#pycmtensor.statistics.stderror","title":"<code>stderror(hessian, params)</code>","text":"<p>calculates the standard error of the estimated parameter given the hessian matrix</p> <p>Parameters:</p> Name Type Description Default <code>hessian</code> <code>ndarray</code> <p>the hessian matrix</p> required <code>params</code> <code>list</code> <p>estimated parameters</p> required <p>Returns:</p> Type Description <code>list</code> <p>the standard error of the estimates</p> <p>Note</p> <p>The standard errors is calculated using the formula:</p> \\[ stderr = \\sqrt{diag(-H^{-1})} \\]"},{"location":"developer_guide/api/statistics/#pycmtensor.statistics.rob_stderror","title":"<code>rob_stderror(hessian, bhhh, params)</code>","text":"<p>calculates the robust standard error of the estimated parameter given the hessian and the bhhh matrices</p> <p>Parameters:</p> Name Type Description Default <code>hessian</code> <code>ndarray</code> <p>the hessian matrix</p> required <code>bhhh</code> <code>ndarray</code> <p>the bhhh matrix</p> required <code>params</code> <code>list</code> <p>estimated parameters</p> required <p>Returns:</p> Type Description <code>list</code> <p>the robust standard error of the estimates</p> <p>Note</p> <p>The robust standard errors is calculated using the formula:</p> \\[ rob. stderr = \\sqrt{diag(rob. varcovar)} \\]"},{"location":"developer_guide/api/statistics/#pycmtensor.statistics.correlation_matrix","title":"<code>correlation_matrix(hessian)</code>","text":"<p>computes the correlation matrix from the hessian matrix</p> <p>Parameters:</p> Name Type Description Default <code>hessian</code> <code>ndarray</code> <p>the hessian matrix</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>the correlation matrix</p>"},{"location":"developer_guide/api/statistics/#pycmtensor.statistics.rob_correlation_matrix","title":"<code>rob_correlation_matrix(hessian, bhhh)</code>","text":"<p>computes the robust correlation matrix from the hessian and bhhh matrix</p> <p>Parameters:</p> Name Type Description Default <code>hessian</code> <code>ndarray</code> <p>the hessian matrix</p> required <code>bhhh</code> <code>ndarray</code> <p>the bhhh matrix</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>the tobust correlation matrix</p>"},{"location":"developer_guide/api/models/MNL/","title":"models/MNL.py","text":""},{"location":"developer_guide/api/models/MNL/#pycmtensor.models.MNL.MNL","title":"<code>MNL(ds, variables, utility, av=None, **kwargs)</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Defines a Multinomial Logit model</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Data</code> <p>the database object</p> required <code>variables</code> <code>dict</code> <p>dictionary containing Param objects or a dictionary of python variables</p> required <code>utility</code> <code>Union[list[TensorVariable], TensorVariable]</code> <p>the vector of utility functions</p> required <code>av</code> <code>List[TensorVariable]</code> <p>list of availability conditions. If <code>None</code>, all availability is set to 1</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Optional keyword arguments for modifying the model configuration settings. See configuration in the user guide for details on possible options</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>x</code> <code>List[TensorVariable]</code> <p>symbolic variable objects for independent variables</p> <code>y</code> <code>TensorVariable</code> <p>symbolic variable object for the choice variable</p> <code>xy</code> <code>List[TensorVariable]</code> <p>concatenated list of x and y</p> <code>betas</code> <code>List[Beta]</code> <p>model beta variables</p> <code>cost</code> <code>TensorVariable</code> <p>symbolic cost tensor variable function</p> <code>p_y_given_x</code> <code>TensorVariable</code> <p>probability tensor variable function</p> <code>ll</code> <code>TensorVariable</code> <p>loglikelihood tensor variable function</p> <code>pred</code> <code>TensorVariable</code> <p>prediction tensor variable function</p>"},{"location":"developer_guide/api/models/MNL/#pycmtensor.models.MNL.MNL.n_betas","title":"<code>n_betas</code>  <code>property</code>","text":"<p>Return the number of estimated betas</p>"},{"location":"developer_guide/api/models/MNL/#pycmtensor.models.MNL.MNL.n_params","title":"<code>n_params</code>  <code>property</code>","text":"<p>Return the total number of estimated parameters</p>"},{"location":"developer_guide/api/models/MNL/#pycmtensor.models.MNL.MNL.build_cost_fn","title":"<code>build_cost_fn()</code>","text":"<p>constructs aesara functions for cost and prediction errors</p>"},{"location":"developer_guide/api/models/MNL/#pycmtensor.models.MNL.MNL.build_cost_updates_fn","title":"<code>build_cost_updates_fn(updates)</code>","text":"<p>build/rebuilt cost function with updates to the model. Creates a class function <code>MNL.cost_updates_fn(*inputs, output, lr)</code> that receives a list of input variable arrays, the output array, and a learning rate.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>List[Tuple[TensorSharedVariable, TensorVariable]]</code> <p>The list of tuples containing the target shared variable and the new value of the variable.</p> required"},{"location":"developer_guide/api/models/MNL/#pycmtensor.models.MNL.MNL.build_gh_fn","title":"<code>build_gh_fn()</code>","text":"<p>constructs aesara functions for hessians and gradient vectors</p> <p>Note</p> <p>The hessians and gradient vector are evaluation at the maximum log likelihood estimates instead of the negative loglikelihood, therefore the cost is multiplied by negative one.</p>"},{"location":"developer_guide/api/models/MNL/#pycmtensor.models.MNL.MNL.elasticities","title":"<code>elasticities(ds, wrt_choice)</code>","text":"<p>disaggregated point/cross elasticities of choice y wrt x</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>dataset containing the training data</p> required <code>wrt_choice</code> <code>int</code> <p>alternative to evaluate the variables on</p> required <p>Returns:</p> Type Description <code>dict</code> <p>the disaggregate point elasticities of x</p> <p>Example</p> <p>To calculate the elasticity of the choosing alternative 1 w.r.t. (represented by <code>wrt_choice</code>) w.r.t. to variable x.</p> <pre><code>disag_elas = self.elasticities(ds, wrt_choice=1)\n</code></pre> <p>output: <pre><code>{\n    'variable_1': array([...]),\n    'variable_2': array([...]),\n    ...\n}\n</code></pre></p> <p>The return values in the dictionary are the disaggregated elasticities. To calculate the aggregated elasticities, use <code>np.mean()</code>.</p> <p>Note</p> <p>This function returns the elasticities for all variables. To obtain the point or cross elasticities, simply select the appropriate dictionary key from the output (<code>wrt_choice</code> w.r.t. x).</p>"},{"location":"developer_guide/api/models/MNL/#pycmtensor.models.MNL.MNL.get_betas","title":"<code>get_betas()</code>","text":"<p>returns the values of the betas</p> <p>Returns:</p> Type Description <code>dict</code> <p>beta values</p>"},{"location":"developer_guide/api/models/MNL/#pycmtensor.models.MNL.MNL.predict","title":"<code>predict(ds, return_probabilities=False)</code>","text":"<p>predicts the output of the most likely alternative given the validation dataset in <code>ds</code>. The formula is:</p> \\[     argmax(p_n(y|x)) \\] <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>pycmtensor dataset</p> required <code>return_probabilities</code> <code>bool</code> <p>if true, returns the probability vector instead</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>the predicted choices or the vector of probabilities</p> <p>Example</p> <p>To return predicted choices: <pre><code>predictions = self.predict(ds)\nprint(predictions)\n</code></pre></p> <p>output: <pre><code>{'pred_choice': array([...])}\n</code></pre></p> <p>To return probabilities: <pre><code>prob = self.predict(ds, return_probabilities=True)\nprint(prob)\n</code></pre></p> <p>output: <pre><code>{   0: array([...]),\n    1: array([...]),\n    ...\n}\n</code></pre></p>"},{"location":"developer_guide/api/models/MNL/#pycmtensor.models.MNL.MNL.reset_values","title":"<code>reset_values()</code>","text":"<p>resets the values of all parameters</p>"},{"location":"developer_guide/api/models/basic/","title":"models/basic.py","text":""},{"location":"developer_guide/api/models/basic/#pycmtensor.models.basic","title":"<code>pycmtensor.models.basic</code>","text":""},{"location":"developer_guide/api/models/basic/#pycmtensor.models.basic.BaseModel","title":"<code>BaseModel(**kwargs)</code>","text":"<p>             Bases: <code>object</code></p> <p>Basic model class object</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>name of the model</p> <code>config</code> <code>Config</code> <p>pycmtensor config object</p> <code>rng</code> <code>Generator</code> <p>random number generator</p> <code>params</code> <code>list</code> <p>list of model parameters (<code>betas</code> &amp; <code>weights</code>)</p> <code>betas</code> <code>list</code> <p>list of model scalar betas</p> <code>sigmas</code> <code>list</code> <p>list of model scalar sigmas</p> <code>weights</code> <code>list</code> <p>list of model weight matrices</p> <code>biases</code> <code>list</code> <p>list of model vector biases</p> <code>updates</code> <code>list</code> <p>list of (param, update) tuples</p> <code>learning_rate</code> <code>TensorVariable</code> <p>symbolic reference to the learning rate</p> <code>results</code> <code>Results</code> <p>stores the results of the model estimation</p>"},{"location":"developer_guide/api/models/basic/#pycmtensor.models.basic.BaseModel.get_weights","title":"<code>get_weights()</code>","text":"<p>returns the values of the weights</p> <p>Returns:</p> Type Description <code>dict</code> <p>weight values</p>"},{"location":"developer_guide/api/models/basic/#pycmtensor.models.basic.BaseModel.get_biases","title":"<code>get_biases()</code>","text":"<p>returns the values of the biases</p> <p>Returns:</p> Type Description <code>dict</code> <p>biases values</p>"},{"location":"developer_guide/api/models/basic/#pycmtensor.models.basic.BaseModel.get_betas","title":"<code>get_betas()</code>","text":"<p>returns the values of the betas</p> <p>Returns:</p> Type Description <code>dict</code> <p>beta values</p>"},{"location":"developer_guide/api/models/basic/#pycmtensor.models.basic.BaseModel.reset_values","title":"<code>reset_values()</code>","text":"<p>resets the values of all parameters</p>"},{"location":"developer_guide/api/models/basic/#pycmtensor.models.basic.BaseModel.include_params_for_convergence","title":"<code>include_params_for_convergence(*args, **kwargs)</code>","text":"<p>dummy method for additional parameter objects for calculating convergence</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>None</code> <p>overloaded arguments</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>overloaded keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>OrderedDict</code> <p>a dictonary of addition parameters to include</p>"},{"location":"developer_guide/api/models/basic/#pycmtensor.models.basic.BaseModel.include_regularization_terms","title":"<code>include_regularization_terms(*regularizers)</code>","text":"<p>dummy method for additional regularizers into the cost function</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>None</code> <p>overloaded arguments</p> required <p>Returns:</p> Type Description <code>list[TensorVariable]</code> <p>a list of symbolic variables that specify additional regualrizers to minimize against</p>"},{"location":"developer_guide/api/models/basic/#pycmtensor.models.basic.BaseModel.extract_params","title":"<code>extract_params(cost, variables)</code>  <code>staticmethod</code>","text":"<p>Extracts Param objects from variables</p> <p>Parameters:</p> Name Type Description Default <code>cost</code> <code>TensorVariable</code> <p>function to evaluate</p> required <code>variables</code> <code>Union[dict, list]</code> <p>list of variables from the current program</p> required"},{"location":"developer_guide/api/models/basic/#pycmtensor.models.basic.BaseModel.drop_unused_variables","title":"<code>drop_unused_variables(cost, params, variables)</code>  <code>staticmethod</code>","text":"<p>Internal method to remove ununsed tensors</p> <p>Parameters:</p> Name Type Description Default <code>cost</code> <code>TensorVariable</code> <p>function to evaluate</p> required <code>params</code> <code>Param</code> <p>param objects</p> required <code>variables</code> <code>dict</code> <p>list of array variables from the dataset</p> required <p>Returns:</p> Type Description <code>list</code> <p>a list of param names which are not used in the model</p>"},{"location":"developer_guide/api/models/basic/#pycmtensor.models.basic.compute","title":"<code>compute(model, ds, update=False, **params)</code>","text":"<p>Function for manual computation of model by specifying parameters as arguments</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>model to train</p> required <code>ds</code> <code>Dataset</code> <p>dataset to use for training</p> required <code>update</code> <code>bool</code> <p>if True, run <code>cost_updates_fn</code> once before computing the model</p> <code>False</code> <code>**params</code> <code>dict</code> <p>keyword arguments for model coefficients (<code>Params</code>)</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>model likelihood and error for training and validation datasets</p> <p>Example: <pre><code>compute(model, ds, b_time=-5.009, b_purpose=0.307, asc_pt=-1.398, asc_drive=4.178,\n        asc_cycle=-3.996, b_car_own=1.4034)\n</code></pre></p>"},{"location":"developer_guide/api/models/basic/#pycmtensor.models.basic.train","title":"<code>train(model, ds, **kwargs)</code>","text":"<p>main training loop</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseModel</code> <p>model to train</p> required <code>ds</code> <code>Dataset</code> <p>dataset to use for training</p> required <code>**kwargs</code> <code>dict</code> <p>overloaded keyword arguments. See configuration in the user guide for details on possible options</p> <code>{}</code>"},{"location":"examples/mixed_mnl/","title":"Mixed logit","text":"<p>This example shows a multinomial logit model with one random Beta variable for time. We use a \"random coefficients\" method of defining the model and a simulation based estimation: <pre><code>Beta(\"b_time\") + Beta(\"s_time\") * RandomDraws(\"rnd_time\", \"lognormal\", 100)\n</code></pre></p> <p>The arguments for <code>RandomDraws(name, draw_type, draws)</code> are:</p> <ul> <li>name (<code>str</code>) the name of the draw</li> <li>draw type (<code>str</code>) the distribution of the draws</li> <li>draws (<code>int</code>) the number of random draws to sample from</li> </ul> <pre><code>#import packages\nimport pandas as pd\nfrom pycmtensor.dataset import Dataset\nfrom pycmtensor.expressions import Beta, RandomDraws\nfrom pycmtensor.models import MNL\n\n\n# load data\nlpmc = pd.read_csv(\"../../data/lpmc.dat\", sep=\"\\t\")\nds = Dataset(df=lpmc, choice=\"travel_mode\")\nds.split(0.8)\n\n\n# Beta parameters\nasc_walk = Beta(\"asc_walk\", 0.0, None, None, 1)\nasc_cycle = Beta(\"asc_cycle\", 0.0, None, None, 0)\nasc_pt = Beta(\"asc_pt\", 0.0, None, None, 0)\nasc_drive = Beta(\"asc_drive\", 0.0, None, None, 0)\nb_cost = Beta(\"b_cost\", 0.0, None, None, 0)\nb_time = Beta(\"b_time\", 0.0, None, None, 0)\ns_time = Beta(\"s_time\", 0.5, None, None, 0)\nb_purpose = Beta(\"b_purpose\", 0.0, None, None, 0)\nb_licence = Beta(\"b_licence\", 0.0, None, None, 0)\nb_car_own = Beta(\"b_car_own\", 0.0, None, None, 0)\n\n\n# additional parameter for variance of b_time\ns_time = Beta(\"s_time\", 0.5, None, None, 0)\n\n\n# define the sampling variable\nrnd_time = RandomDraws('rnd_time', 'lognormal', 100)\n\n\n# composite variables\ndur_pt = ds[\"dur_pt_rail\"] + ds[\"dur_pt_bus\"] + ds[\"dur_pt_int\"]\ncost_drive = ds[\"cost_driving_fuel\"] + ds[\"cost_driving_ccharge\"]\n\n\n# define the random beta time coefficient\nb_time_rnd = b_time + s_time * rnd_time\n\n\n# utility functions\nU_walk = asc_walk + b_time_rnd * ds[\"dur_walking\"]\nU_cycle = asc_cycle + b_time_rnd * ds[\"dur_cycling\"] \nU_pt = asc_pt + b_time_rnd * dur_pt + b_cost * ds[\"cost_transit\"]\nU_drive = (asc_drive + b_time_rnd * ds[\"dur_driving\"] + b_cost * cost_drive \n        + b_licence * ds[\"driving_license\"] + b_purpose * ds[\"purpose\"] \n        + b_car_own * ds[\"car_ownership\"])\n\nU = [U_walk, U_cycle, U_pt, U_drive]\n\n\n# the choice model is MNL\nmymodel = MNL(ds, locals(), U)\n\n\n# estimate the model\nfrom pycmtensor.models import train\ntrain(mymodel, ds)\n</code></pre> <p>The following code displays the results of the model estimation, model correlation matrices, predicitons, and elasticities</p> <pre><code>print(mymodel.results.beta_statistics())\nprint(mymodel.results.model_statistics())\nprint(mymodel.results.benchmark())\n\n# correlation matrix\nprint(mymodel.results.model_correlation_matrix())\nprint(mymodel.results.model_robust_correlation_matrix())\n\n# probability prediction\nprob = mymodel.predict(ds, return_probabilities=True)\npd.DataFrame(prob)\n\n# choice prediction\nchoice = mymodel.predict(ds, return_probabilities=False)\npd.DataFrame(choice)\n\n# elasticity w.r.t. choice 1\nelas = mymodel.elasticities(ds, wrt_choice=1)\npd.DataFrame(elas)\n</code></pre>"},{"location":"examples/mixed_mnl/#mixed-logit","title":"Mixed logit","text":""},{"location":"examples/mnl/","title":"Multnomial logit","text":"<pre><code>#import packages\nimport pandas as pd\nfrom pycmtensor.dataset import Dataset\nfrom pycmtensor.expressions import Beta\nfrom pycmtensor.models import MNL\n\n\n# load data\nlpmc = pd.read_csv(\"../../data/lpmc.dat\", sep=\"\\t\")\nds = Dataset(df=lpmc, choice=\"travel_mode\")\nds.split(0.8)\n\n\n# Beta parameters\nasc_walk = Beta(\"asc_walk\", 0.0, None, None, 1)\nasc_cycle = Beta(\"asc_cycle\", 0.0, None, None, 0)\nasc_pt = Beta(\"asc_pt\", 0.0, None, None, 0)\nasc_drive = Beta(\"asc_drive\", 0.0, None, None, 0)\nb_cost = Beta(\"b_cost\", 0.0, None, None, 0)\nb_time = Beta(\"b_time\", 0.0, None, None, 0)\ns_time = Beta(\"s_time\", 0.5, None, None, 0)\nb_purpose = Beta(\"b_purpose\", 0.0, None, None, 0)\nb_licence = Beta(\"b_licence\", 0.0, None, None, 0)\nb_car_own = Beta(\"b_car_own\", 0.0, None, None, 0)\n\n\n# composite variables\ndur_pt = ds[\"dur_pt_rail\"] + ds[\"dur_pt_bus\"] + ds[\"dur_pt_int\"]\ncost_drive = ds[\"cost_driving_fuel\"] + ds[\"cost_driving_ccharge\"]\n\n\n# utility functions\nU_walk = asc_walk + b_time * ds[\"dur_walking\"]\nU_cycle = asc_cycle + b_time * ds[\"dur_cycling\"] \nU_pt = asc_pt + b_time * dur_pt + b_cost * ds[\"cost_transit\"]\nU_drive = (asc_drive + b_time * ds[\"dur_driving\"] + b_cost * cost_drive \n        + b_licence * ds[\"driving_license\"] + b_purpose * ds[\"purpose\"] \n        + b_car_own * ds[\"car_ownership\"])\n\nU = [U_walk, U_cycle, U_pt, U_drive]\n\n\n# the choice model is MNL\nmymodel = MNL(ds, locals(), U)\n\n\n# estimate the model\nfrom pycmtensor.models import train\ntrain(mymodel, ds)\n</code></pre> <p>The following code displays the results of the model estimation</p> <pre><code>print(mymodel.results.beta_statistics())\nprint(mymodel.results.model_statistics())\nprint(mymodel.results.benchmark())\n</code></pre> <p>The following code displays the model correlation and robust correlation matrices</p> <pre><code>mymodel.results.model_correlation_matrix()\n</code></pre> <pre><code>mymodel.results.model_robust_correlation_matrix()\n</code></pre> <p>The following code displays the model prediction on the validation dataset</p> <pre><code>prob = mymodel.predict(ds, return_probabilities=True)\npd.DataFrame(prob)\n</code></pre> <pre><code>choice = mymodel.predict(ds, return_probabilities=False)\npd.DataFrame(choice)\n</code></pre> <p>The following code displays the model elasticities (w.r.t to choice 1)</p> <pre><code>elas = mymodel.elasticities(ds, wrt_choice=1)\npd.DataFrame(elas)\n</code></pre>"},{"location":"examples/mnl/#multinomial-logit","title":"Multinomial logit","text":"<p>This example shows a simple multinomial logit model using the LPMC dataset.</p>"},{"location":"getting_started/","title":"Introduction","text":""},{"location":"getting_started/#why-pycmtensor","title":"Why PyCMTensor?","text":"<p>Writing mathematical operations and evaluating models involving choice based utility  expressions can be difficult and time-consuming, especially with alternative specific  utilities of different dimensionalities are involved or when specifying neural networks  within utility specification.  Typically, Python deep learning libraries such as TensorFlow, Torch, Keras, or Scikit-learn express entire datasets as products of n-dimensional arrays, without regards to the number of variables used or specification of independent taste coefficients in each part of the output choice probability equation.  These libraries also do not expose the underlying operations that define the  inputs of a random utility equation, making it inconvenient for hypothesis testing or  running statistical tests without cumbersome modification or ad hoc calculations. </p> <p>Although these advanced Python deep learning libraries can be used to evaluate choice  models, they are not explicitly intended to be used for estimating choice models,  especially more advanced versions such as mixed logit. These deep learning models are first and foremost for model prediction, and not choice model estimation where clear interpretation of utility coefficients are needed.</p>"},{"location":"getting_started/#what-can-pycmtensor-do","title":"What can PyCMTensor do?","text":"<p>PyCMTensor can be used to fully specify hybrid discrete choice models, estimate and generate statistical tests, using optimized tensor operations via Aesara. It has a particular focus on estimation of hybrid neural networks and Logit models, as well as on Mixed Logit models. PyCMTensor models are based on computational graphs and models estimated using generalized backpropagation algorithms.</p>"},{"location":"getting_started/#project-goals","title":"Project goals","text":"<p>The goal of PyCMTensor is to combine the easy-to-interpet choice modelling syntaxes and expressions while also implementing some of the mathematical operations and computational speed-up using Aesara tensor libraries.  PyCMTensor focuses on specifying 'hybrid' utility expressions, and making it easy to define a deep neural network inside a choice model such as TasteNet or ResLogit model specifications.  The distinction of PyCMTensor from other deep learning libraries is that it focuses on  econometric modelling and statistical testing, rather than focusing purely on models  for prediction or classification.</p> <p>The higher level goals of this project are:</p> <ul> <li>Provide a flexible and customizable platform for implementing 'hybrid' neural network discrete choice model</li> <li>Facilitating development and introduction of deep learning algorithms and methods for choice modelling domain experts and researchers who are already somewhat familiar with neural networks</li> <li>Develop hybrid neural network based utility specification expressions and which are logical and consistent with other conventional choice modelling tools</li> <li>Increase the computational efficiency of estimating large scale choice models with machine learning algorithms and optimization methods</li> <li>Provide a tool for researchers with similar interests to contribute to an ecosystem for estimating hybrid discrete choice models.</li> </ul>"},{"location":"getting_started/#key-features","title":"Key features","text":"<p>Main features:</p> <ul> <li>Interpretable and customizable utility specification syntaxes</li> <li>Ability to specifying neural nets with weight and bias parameters inside a utility functions (e.g. TasteNet)</li> <li>Perform specification testing, analyze covariances, standard errors for taste parameters.</li> <li>Fast execution of model estimation including of simulation based methods, e.g. Mixed Logit models, using computational graph approach</li> <li>Model estimating tuning with 1<sup>st</sup> order (e.g. Adam, Stochastic Gradient Descent) or 1.5<sup>th</sup> order methods (e.g. Stochastic BFGS)</li> </ul> <p>While other choice modelling estimation software in Python are available, e.g. Biogeme, xlogit, PyLogit, etc., PyCMTensor strives to fully implement deep learning based methods written in a simplified syntax for utility equation specification.</p>"},{"location":"getting_started/#roadmap","title":"Roadmap","text":"<p>PyCMTensor is a work in progress, there are several proposed feature implementations that needs to be done and there are still some code maintenance, documentation writing, and testing to be performed. </p> <p>The following are proposed major feature implementations:</p> <ul> <li>Implementation of TasteNet and ResLogit hybrid deep learning choice model</li> <li>Optimization algorithms:<ul> <li>1<sup>st</sup> order estimation (Adam, RMSProp, Rprop. Adagrad)</li> <li>Stochastic Quasi Newton NFGS (SQN)</li> <li>Momentum-based BFGS</li> </ul> </li> <li>Variational inference estimation</li> </ul> <p>If you are interested in contributing to the development of PyCMTensor, please contact me.</p>"},{"location":"getting_started/installation/","title":"Installing PyCMTensor","text":""},{"location":"getting_started/installation/#overview","title":"Overview","text":"<p>To ensure complete installation including the necessary libraries, it is recommended to first install dependencies via <code>conda</code> package manager in a virtual environment, then install PyCMTensor via <code>pip</code>.</p>"},{"location":"getting_started/installation/#system-requirements","title":"System requirements","text":"<ul> <li>Python (3.9+)</li> <li>Aesara (2.9+) - from conda-forge</li> <li>Numpy - from conda-forge</li> <li>Scipy</li> <li>Pandas</li> </ul> <p>In addition, you will need:</p> <ul> <li>A C compiler compatible with your OS and Python installation. Libraries can be installed from conda-forge:<ul> <li>Linux: <code>gcc_linux-64</code> and <code>gxx_linux-64</code></li> <li>Windows (7 or later): <code>m2w64-toolchain</code> and <code>vs2019_win-64</code></li> <li>macOS (incl. M1): <code>Clang</code></li> </ul> </li> <li>BLAS installation<ul> <li>MKL libraries, installed through conda with <code>mkl-service</code> package</li> <li>Openblas, default when Numpy is installed with pip, alternatively, with conda <code>blas</code> package</li> </ul> </li> </ul>"},{"location":"getting_started/installation/#installation","title":"Installation","text":"<ol> <li>Install conda dependencies</li> <li>Install PyCMTensor</li> <li>Validate installation</li> </ol>"},{"location":"getting_started/installation/#step-1-install-conda-dependencies","title":"Step 1: Install conda dependencies","text":"<p>Install Miniconda. Select the appropriate package for your operating system.</p> <p>Once you have installed conda, create a virtual environment and activate it. For example:</p> <pre><code>conda create -n pycmtensor python=3.11 \nconda activate pycmtensor\n</code></pre> <p>Next, install the conda dependencies inside the virtual environment:</p> <p>Windows</p> <pre><code>conda install -c conda-forge mkl-service m2w64-toolchain vs2019_win-64 blas aesara -y\n</code></pre> <p>macOS (incl. M1)</p> <pre><code>conda install -c conda-forge mkl-service Clang blas aesara -y\n</code></pre> <p>Linux</p> <pre><code>conda install -c conda-forge mkl-service gcc_linux-64 gxx_linux-64 blas aesara -y\n</code></pre>"},{"location":"getting_started/installation/#step-2-install-pycmtensor-using-pip","title":"Step 2: Install PyCMTensor using pip","text":"<p>Once the conda packages have been installed, install the rest of the packages using <code>pip</code>, type:</p> <pre><code>pip install pycmtensor\n</code></pre>"},{"location":"getting_started/installation/#step-3-checking-your-installation","title":"Step 3: Checking your installation","text":"<p>If PyCMTensor was installed correctly, the following should display when you run the following code in a python console:</p> <pre><code>python -c \"import pycmtensor; print(pycmtensor.__version__)\"\n</code></pre> <p>Output:</p> <pre><code>1.6.3\n</code></pre>"},{"location":"getting_started/installation/#updating-pycmtensor","title":"Updating PyCMTensor","text":"<p>Update PyCMTensor by running the <code>pip install --upgrade pycmtensor</code> command</p>"},{"location":"getting_started/installation/#source-code","title":"Source code","text":"<p>Source code can be checked out from the Github repository via <code>git</code>:</p> <pre><code>git clone git::/github.com/mwong009/pycmtensor\n</code></pre>"},{"location":"getting_started/overview/","title":"Overview","text":"<p>Follow the steps below and learn how to use PyCMTensor to estimate a discrete choice model. In this tutorial, we will use the London Passenger Mode Choice (LPMC) dataset (pdf). Download the dataset here and place it in the working directory.</p> <p>Jump to Putting it all together for the final Python script.</p>"},{"location":"getting_started/overview/#importing-data-from-csv","title":"Importing data from csv","text":"<p>Import the PyCMTensor package and read the data using <code>pandas</code>:</p> <pre><code>import pycmtensor\nimport pandas as pd\n\nlpmc = pd.read_csv(\"lpmc.dat\", sep='\\t')  # read the .dat file and use &lt;TAB&gt; separator\nlpmc = lpmc[lpmc[\"travel_year\"]==2015]  # select only the 2015 data to use\n</code></pre>"},{"location":"getting_started/overview/#create-a-dataset-object","title":"Create a dataset object","text":"<p>From the <code>pycmtensor</code> package, import the <code>Dataset</code> object, which stores and manages the tensors and arrays of the data variables. Denote the column name with the choice variable in the argument <code>choice=</code>:</p> <pre><code>from pycmtensor.dataset import Dataset\nds = Dataset(df=lpmc, choice=\"travel_mode\")\n</code></pre> <p>The <code>Dataset</code> object takes the following arguments:</p> <ul> <li><code>df</code>: The <code>pandas.DataFrame</code> object</li> <li><code>choice</code>: The name of the choice variable found in the heading of the dataframe</li> </ul> <p>Note</p> <p>If the range of alternatives in the choice column does not start with <code>0</code>, e.g. <code>[1, 2, 3, 4]</code> instead of <code>[0, 1, 2, 3]</code>, the Dataset will automatically convert the alternatives to start with <code>0</code>.</p>"},{"location":"getting_started/overview/#split-the-dataset","title":"Split the dataset","text":"<p>Next, split the dataset into training and validation datasets, <code>frac=</code> argument is the percentage of the data that is assigned to the training dataset. The rest of the data is assigned to the validation dataset. </p> <pre><code>ds.split(frac=0.8)  # splits 80% of the data into the training dataset\n                    # and the other 20% into the validation dataset\n</code></pre> <p>You should get an output showing the number of training and validation samples in the dataset.</p> <p>Output:</p> <pre><code>[INFO] n_train_samples:3986 n_valid_samples:997\n</code></pre> <p>Note</p> <p>Splitting the dataset is optional. If <code>frac=</code> is not given as an argument, both training and validation dataset will use the same samples.</p>"},{"location":"getting_started/overview/#defining-taste-parameters","title":"Defining taste parameters","text":"<p>Define the taste parameters using the <code>Beta</code> object from the <code>pycmtensor.expressions</code> module:</p> <pre><code>from pycmtensor.expressions import Beta\n\n# Beta parameters\nasc_walk = Beta(\"asc_walk\", 0.0, None, None, 1)\nasc_cycle = Beta(\"asc_cycle\", 0.0, None, None, 0)\nasc_pt = Beta(\"asc_pt\", 0.0, None, None, 0)\nasc_drive = Beta(\"asc_drive\", 0.0, None, None, 0)\nb_cost = Beta(\"b_cost\", 0.0, None, None, 0)\nb_time = Beta(\"b_time\", 0.0, None, None, 0)\nb_purpose = Beta(\"b_purpose\", 0.0, None, None, 0)\nb_licence = Beta(\"b_licence\", 0.0, None, None, 0)\n</code></pre> <p>The <code>Beta</code> object takes the following argument:</p> <ul> <li><code>name</code>: Name of the taste parameter (required)</li> <li><code>value</code>: The initial starting value. Defaults to <code>0.</code></li> <li><code>lb</code> and <code>ub</code>: lower and upper bound of the parameter. Defaults to <code>None</code></li> <li><code>status</code>: <code>1</code> if the parameter should not be estimated. Defaults to <code>0</code>.</li> </ul> <p>Note</p> <p>If a <code>Beta</code> variable is not used in the model, a warning will be shown in stdout. E.g.</p> <pre><code>[WARNING] b_purpose not in any utility functions\n</code></pre> <p>Info</p> <p><code>pycmtensor.expressions.Beta</code> follows the same syntax as in Biogeme <code>biogeme.expressions.Beta</code> for familiarity sake. However, <code>pycmtensor.expressions.Beta</code> uses <code>aesara.tensor</code> variables to define the mathematical ops. Currently they are not interchangable.</p>"},{"location":"getting_started/overview/#specifying-utility-equations","title":"Specifying utility equations","text":"<pre><code>U_walk  = asc_walk + b_time * ds[\"dur_walking\"]\nU_cycle = asc_cycle + b_time  * ds[\"dur_cycling\"]\nU_pt    = asc_pt + b_time * (ds[\"dur_pt_rail\"] + ds[\"dur_pt_bus\"] + \\\n          ds[\"dur_pt_int\"]) + b_cost * ds[\"cost_transit\"]\nU_drive = asc_drive + b_time * ds[\"dur_driving\"] + b_licence * ds[\"driving_license\"] + \\\n          b_cost * (ds[\"cost_driving_fuel\"] + ds[\"cost_driving_ccharge\"])\n\n# vectorize the utility function\nU = [U_walk, U_cycle, U_pt, U_drive]\n</code></pre> <p>We define data variables as an item from the <code>Dataset</code> object. For instance, the variable <code>\"dur_walking\"</code> from the LPMC dataset can be expressed as such: <code>ds[\"dur_walking\"]</code>. Furthermore, composite variables or interactions can also be specified using standard mathematical operators, for example, adding <code>\"dur_pt_rail\"</code> and <code>\"dur_pt_bus\"</code> can be expressed as <code>ds[\"dur_pt_rail\"] + ds[\"dur_pt_bus\"]</code>.</p> <p>Finally, we vectorize the utility functions by putting them into a <code>list()</code>. The index of the utility in the list corresponds to the (zero-adjusted) indexing of the choice variable.</p> <p>(Advanced, optional) We can also define the utlity functions as a 2-D <code>tensorVariable</code> object instead of a list. </p>"},{"location":"getting_started/overview/#specifying-the-model","title":"Specifying the model","text":"<p>To specify the model, we create a model object from a model in the <code>pycmtensor.models</code> module.</p> <pre><code>mymodel = pycmtensor.models.MNL(ds=ds, variables=locals(), utility=U, av=None)\n</code></pre> <p>The <code>MNL</code> object takes the following argument:</p> <ul> <li><code>ds</code>: The dataset object</li> <li><code>variables</code>: the list (or dict) of declared parameter objects*</li> <li><code>utility</code>: The list of utilities to be estimated</li> <li><code>av</code>: The availability conditions as a list with the same index as <code>utility</code>. See here for an example on specifying availability conditions. Defaults to <code>None</code></li> <li><code>**kwargs</code>: Optional keyword arguments for modifying the model configuration settings. See configuration in the user guide for details on possible options</li> </ul> <p>Tip</p> <p>*: We use <code>locals()</code> as a shortcut for collecting and fitering the <code>Beta</code> objects from the Python local environment for the argument <code>variables=</code>.</p> <p>Output:</p> <pre><code>[INFO] inputs in MNL: [driving_license, dur_walking, dur_cycling, dur_pt_rail, \ndur_pt_bus, dur_pt_int, dur_driving, cost_transit, cost_driving_fuel, \ncost_driving_ccharge]\n[INFO] Build time = 00:00:01\n</code></pre>"},{"location":"getting_started/overview/#estimating-the-model","title":"Estimating the model","text":"<pre><code>from pycmtensor.models import train\nfrom pycmtensor.optimizers import Adam\nfrom pycmtensor.scheduler import ConstantLR\n\ntrain(\n    model=mymodel, \n    ds=ds, \n    optimizer=Adam,  # optional\n    batch_size=0,  # optional \n    base_learning_rate=1.,  # optional \n    convergence_threshold=0.0001,  # optional \n    max_steps=200,  # optional\n    lr_scheduler=ConstantLR,  # optional\n)\n</code></pre> <p>The function <code>train()</code> estimates the model until convergence specified by the gradient norm between two complete passes of the entire training dataset. In order to limit repeated calculation, we store the \\(\\beta\\) of the previous epoch and approximate the gradient step using: \\(\\nabla_\\beta = \\beta_t - \\beta_{t-1}\\). The estimation is terminated either when the <code>max_steps</code> is reached or when the gradient norm \\(||\\nabla_\\beta||_{_2}\\) is less than the <code>convergence_threshold</code> value (set as <code>0.0001</code> in this example).</p> <p>The <code>train()</code> function takes the following required arguments:</p> <ul> <li><code>model</code>: The model object. <code>MNL</code> in the example above</li> <li><code>ds</code>: The dataset object</li> <li><code>**kwargs</code>: Optional keyword arguments for modifying the model configuration settings. See configuration in the user guide for details on possible options</li> </ul> <p>The other arguments <code>**kwargs</code> are optional, and they can be set when calling the <code>train()</code> function or during model specification. These optional arguments are the so-called hyperparameters of the model that modifies the training procedure.</p> <p>Note</p> <p>A <code>step</code> is one full pass of the training dataset. An <code>iteration</code> is one model update operation, usually it is every mini-batch (when <code>batch_size != 0</code>).</p> <p>Tip</p> <p>The hyperparameters can also be set with the <code>pycmtensor.config</code> module before the training function is called.</p> <p>For example, to set the training <code>batch_size</code> to <code>50</code> and <code>base_learning_rate</code> to <code>0.1</code>:</p> <pre><code>pycmtensor.config.batch_size = 50\npycmtensor.config.base_learning_rate = 0.1\n\ntrain (\n    model=...\n)\n</code></pre> <p>Output:</p> <pre><code>[INFO] Start (n=3986, Step=0, LL=-5525.77, Error=80.34%)\n[INFO] Train (Step=0, LL=-9008.61, Error=80.34%, gnorm=2.44949e+00, 0/2000)\n[INFO] Train (Step=16, LL=-3798.26, Error=39.12%, gnorm=4.46640e-01, 16/2000)\n[INFO] Train (Step=54, LL=-3487.97, Error=35.21%, gnorm=6.80979e-02, 54/2000)\n[INFO] Train (Step=87, LL=-3471.01, Error=35.01%, gnorm=9.93509e-03, 87/2000)\n[INFO] Train (Step=130, LL=-3470.29, Error=34.70%, gnorm=1.92617e-03, 130/2000)\n[INFO] Train (Step=168, LL=-3470.28, Error=34.70%, gnorm=3.22536e-04, 168/2000)\n[INFO] Train (Step=189, LL=-3470.28, Error=34.70%, gnorm=8.74120e-05, 189/2000)\n[INFO] Model converged (t=0.492)\n[INFO] Best results obtained at Step 185: LL=-3470.28, Error=34.70%, gnorm=2.16078e-04\n</code></pre>"},{"location":"getting_started/overview/#printing-statistical-test-results","title":"Printing statistical test results","text":"<p>The results are stored in the <code>Results</code> class object of th <code>MNL</code> model. The following are function calls to display the statistical results of the model estimation:</p> <pre><code>print(mymodel.results.beta_statistics())\nprint(mymodel.results.model_statistics())\nprint(mymodel.results.benchmark())\n</code></pre> <p><code>beta_statistics()</code> show the estimated values of the model coefficients, standard errors, t-test, and p-values, including the robust measures.</p> <p>The standard errors are calculated using the diagonals of the square root of the variance-covariance matrix (the inverse of the negative Hessian matrix):</p> \\[  std. error = diag.\\Big(\\sqrt{-H^{-1}}\\Big) \\] <p>The robust standard errors are calculated using the 'sandwich method', where the variance-covariance matrix is as follows:</p> \\[ covar = (-H^{-1})(\\nabla\\cdot\\nabla^\\top)(-H^{-1}) \\] <p>The rest of the results are self-explanatory.</p> <p>Ouput:</p> <pre><code>              value   std err     t-test p-value rob. std err rob. t-test rob. p-value\nasc_cycle -3.853007  0.117872 -32.688157     0.0     0.120295  -32.029671          0.0\nasc_drive -2.060414  0.099048 -20.802183     0.0     0.102918  -20.019995          0.0\nasc_pt    -1.305677  0.076151 -17.145988     0.0     0.079729  -16.376401          0.0\nasc_walk        0.0         -          -       -            -           -            -\nb_cost    -0.135635  0.012788 -10.606487     0.0      0.01269  -10.688684          0.0\nb_licence  1.420747  0.079905  17.780484     0.0     0.084526   16.808497          0.0\nb_time    -4.947477  0.183329 -26.986865     0.0     0.192431  -25.710378          0.0\n\n                                         value\nNumber of training samples used         3986.0\nNumber of validation samples used        997.0\nNull. log likelihood              -5525.769323\nFinal log likelihood              -3470.282749\nAccuracy                                65.30%\nLikelihood ratio test              4110.973149\nRho square                            0.371982\nRho square bar                        0.370715\nAkaike Information Criterion       6954.565498\nBayesian Information Criterion     6998.599302\nFinal gradient norm                2.16078e-04\n\n                            value\nSeed                        42069\nModel build time         00:00:09\nModel train time         00:00:00\niterations per sec  384.15 iter/s\n</code></pre>"},{"location":"getting_started/overview/#prediction-and-validation","title":"Prediction and validation","text":"<p>For choice prediction, PyCMTensor generates a vector of probabilities for each observation in the validation dataset. It is also possible to output discrete prediction (e.g. classification) using the Argmax function. To output the predicted probabilites after estimation, use the function:</p> <pre><code>prob = mymodel.predict(ds, return_probabilities=True)\nprint(pd.DataFrame(prob))\n</code></pre> <p>Output: <pre><code>    0               1           2           3\n0   1.805676e-02    0.026041    0.114326    0.841576\n1   3.052870e-02    0.015358    0.128087    0.826026\n2   6.262815e-01    0.018934    0.264090    0.090694\n3   3.649617e-08    0.002540    0.072809    0.924651\n4   4.880317e-05    0.024973    0.828194    0.146784\n... ...             ...         ...         ...\n</code></pre></p> <p>to generate probabilities for each observation in the validation dataset, or for discrete predictions, set <code>return_probabilties=False</code>.</p>"},{"location":"getting_started/overview/#elasticities","title":"Elasticities","text":"<p>Disaggregated elasticities are generated from the model by specifiying the dataset and the reference choice as <code>wrt_choice</code>. For instance, to compute the elasticities of the mode for driving (<code>wrt_choice=3</code>) over all the input variables:</p> <pre><code>elas = mymodel.elasticities(ds, wrt_choice=3)\npd.DataFrame(elas)\n</code></pre> <p>Output: <pre><code>cost_driving_ccharge    cost_driving_fuel   cost_transit    driving_license dur_cycling dur_driving dur_pt_bus  dur_pt_int  dur_pt_rail dur_walking purpose\n0   0.0 0.177908    0.063031    -1.237733   -3.634088   1.047106    0.287043    0.099481    0.000000    0.003005    -0.246805\n1   0.0 0.076653    0.158756    -0.747595   -2.335536   1.079767    0.000000    0.000000    0.313203    0.062477    -0.745355\n2   0.0 0.025086    0.189326    -0.000000   -2.397259   0.266012    0.000000    0.000000    0.461397    0.061311    -0.163128\n3   0.0 0.027003    0.000000    -0.670951   -0.914464   0.408430    0.000000    0.000000    0.107671    0.297314    -0.401365\n4   0.0 0.374654    0.112961    -0.000000   -6.296690   2.359637    0.062192    0.248769    0.559729    0.000010    -0.944990\n... ... ... ... ... ... ... ... ...\n</code></pre></p> <p>The aggregated elasticities can then be obtained by taking the mean over the rows</p>"},{"location":"getting_started/overview/#putting-it-all-together","title":"Putting it all together","text":"<pre><code>import pycmtensor\nimport pandas as pd\n\nfrom pycmtensor.dataset import Dataset\nfrom pycmtensor.expressions import Beta\n\n# read data\nlpmc = pd.read_csv(\"lpmc.dat\", sep='\\t')\nlpmc = lpmc[lpmc[\"travel_year\"]==2015] \n\n# load data into dataset\nds = Dataset(df=lpmc, choice=\"travel_mode\")\nds.split(frac=0.8) \n\n# Beta parameters\nasc_walk = Beta(\"asc_walk\", 0.0, None, None, 1)\nasc_cycle = Beta(\"asc_cycle\", 0.0, None, None, 0)\nasc_pt = Beta(\"asc_pt\", 0.0, None, None, 0)\nasc_drive = Beta(\"asc_drive\", 0.0, None, None, 0)\nb_cost = Beta(\"b_cost\", 0.0, None, None, 0)\nb_time = Beta(\"b_time\", 0.0, None, None, 0)\nb_licence = Beta(\"b_licence\", 0.0, None, None, 0)\n\n# utility equations\nU_walk  = asc_walk + b_time * ds[\"dur_walking\"]\nU_cycle = asc_cycle + b_time  * ds[\"dur_cycling\"]\nU_pt    = asc_pt + b_time * (ds[\"dur_pt_rail\"] + ds[\"dur_pt_bus\"] + \\\n          ds[\"dur_pt_int\"]) + b_cost * ds[\"cost_transit\"]\nU_drive = asc_drive + b_time * ds[\"dur_driving\"] + b_licence * ds[\"driving_license\"] + \\\n          b_cost * (ds[\"cost_driving_fuel\"] + ds[\"cost_driving_ccharge\"])\n\n# vectorize the utility function\nU = [U_walk, U_cycle, U_pt, U_drive]\n\nmymodel = pycmtensor.models.MNL(ds=ds, params=locals(), utility=U, av=None)\n\n\nfrom pycmtensor.models import train\nfrom pycmtensor.optimizers import Adam\nfrom pycmtensor.scheduler import ConstantLR\n\n# main training loop\ntrain(\n    model=mymodel, \n    ds=ds, \n    optimizer=Adam,  # optional\n    batch_size=0,  # optional \n    base_learning_rate=1.,  # optional \n    convergence_threshold=0.0001,  # optional \n    max_steps=200,  # optional\n    lr_scheduler=ConstantLR,  # optional\n)\n\n# print results\nprint(mymodel.results.beta_statistics())\nprint(mymodel.results.model_statistics())\nprint(mymodel.results.benchmark())\n\n# predictions\nprob = mymodel.predict(ds, return_probabilities=True)\nprint(pd.DataFrame(prob))\n\n# elasticities\nelas = mymodel.elasticities(ds, wrt_choice=1)\nprint(pd.DataFrame(elas))\n</code></pre>"},{"location":"getting_started/troubleshooting/","title":"Troubleshooting","text":""},{"location":"user_guide/","title":"User guide","text":""},{"location":"user_guide/#data-structures","title":"Data structures","text":"<p>PyCMTensor works by specifying utility equations, variables, indices, and outputs using symbolic tensors, similar to programs like TensorFlow.  These do not hold any value until an input is given.  The entire model is build as a computational graph with nodes representing intermediate values and edges representing mathematical operations.  The graph is then manipulated internally and optimized during compilation. When an input is provided to certain tensors, the graph computes the values for other symbols in the graph. </p> <p>The core API library, Aesara, gives us a set of tools that takes the defined symbolic tensors, constructs a computational graph and optimizes the mathematical operations around it.</p> <p>As a developer, you define or extend a new model by creating new tensor variable objects from primitive tensors. For instance, take a basic linear equation, \\(y = bx +c\\). The variables \\(y\\), \\(x\\), and \\(c\\) are symbolic tensors making up a computational graph, and \\(b\\) is a shared variable consisting of a mutable defined value. When we call <code>train(y)</code>, an Aesara function is executed on the equation and an update value is passed to \\(b\\).</p> <p>A PyCMTensor model is composed of </p> <ol> <li>A Database object holding the symbolic tensors of input and choice variables</li> <li>Beta variables, which are the taste parameters in the choice model to be estimated</li> <li>Optionally, Random (symbolic) variables, to define random components in the model</li> <li>Neural networks, defined as layers with inputs and outputs, and</li> <li>A function which translates a set of utility equations into a symbolic tensor output</li> </ol> <p>Additional complexity</p> <p>This symbolic approach does add complexity, however, the tradeoffs are that a. more complex structures can be defined more easily and with internal code optimizations, this can result in very efficient estimation of very large datasets. b. The most compelling use of computational graph approach is that for econometric analysis, we can inspect any part of the graph and obtain statistics on any part of the graph. This is especially practical for neural network based discrete choice models.</p>"},{"location":"user_guide/#beta","title":"<code>Beta</code>","text":"<p>A <code>Beta</code> variable is the main data object when estimate a PyCMTensor model. <code>Beta</code> inherits the <code>TensorExpression</code> class which adds Aesara tensor operations to the data object. A <code>Beta</code> is defined as such:</p> <pre><code>Beta(name, value, lb, ub, status)\n</code></pre> <p>The <code>Beta</code> object can be maniputated in various ways:</p> <p>In mathematical operations, for instance:</p> <pre><code>from pycmtensor.expressions import Beta\nimport aesara.tensor as aet\nimport numpy as np \nbeta_cost = Beta(\"beta_cost\", 2.)\ntrain_cost = aet.vector(\"train_cost\") \ny = beta_cost * train_cost\ny.eval({train_cost: np.array([1, 2, 3])})\n</code></pre> <p>Output: <pre><code>array([2., 4., 6.])\n</code></pre></p> <p><code>train_cost</code> and <code>y</code> are symbolic variables, while <code>beta_cost</code> is a <code>TensorExpression.Beta</code> object</p> <p>Updating values</p> <p>During model estimation, a global optimization function is called (e.g. log likelihood) and updates are received through the gradients with respect to the cost function. The <code>Beta</code> is then updated with this new value, and the estimation run iteratively, until convergence is reached.</p> <p>Convergence</p> <p>Model convergence is defined as reaching the minimum gradient norm of the model parameters. For non-convex optimizations, for example: large, complex neural network models, convergence may not be reached, therefore many implementations are added such as learning rate adaptation algorithms and learning rate decay are avaiable so that sufficient global convergence can be achieved, albeit non-optimally.  </p>"},{"location":"user_guide/#calling-variables-and-selecting-data","title":"Calling variables and selecting data","text":""},{"location":"user_guide/#mathematical-operations","title":"Mathematical operations","text":""},{"location":"user_guide/#working-with-generic-and-alternative-specific-variables","title":"Working with generic and alternative specific variables","text":""},{"location":"user_guide/#model-estimation","title":"Model estimation","text":""},{"location":"user_guide/#configuration","title":"Configuration","text":""},{"location":"user_guide/#optimizers","title":"Optimizers","text":""},{"location":"user_guide/#fine-tuning-model","title":"Fine tuning model","text":""},{"location":"user_guide/#generate-results","title":"Generate results","text":""},{"location":"user_guide/configuration/","title":"PyCMTensor Configuration","text":""},{"location":"user_guide/configuration/#guide","title":"Guide","text":"<p>The <code>config</code> module contains <code>attributes</code> that is used for setting the model training hyperparameters, type of optimizer to use, random seed value, and other customizable values. These attributes are loaded when importing the <code>pycmtensor</code> module, but can be modified at any time before invoking the <code>train()</code> method.</p> <p>Display the list of configuration settings with the following in Python: <pre><code>import pycmtensor\nprint(pycmtensor.config)\n</code></pre></p> <p>Set or update a given configuration with the following: <pre><code>pycmtensor.config.add('seed', 100)\n</code></pre></p>"},{"location":"user_guide/configuration/#configuration-attributes","title":"Configuration attributes","text":""},{"location":"user_guide/configuration/#configbatch_size","title":"<code>config.batch_size</code>","text":"<p>Number of samples processed on each iteration of the model update</p> <p>Default: <code>32</code></p>"},{"location":"user_guide/configuration/#configseed","title":"<code>config.seed</code>","text":"<p>Seed value for random number generators. </p> <p>Default: <code>100</code></p>"},{"location":"user_guide/configuration/#configmax_epochs","title":"<code>config.max_epochs</code>","text":"<p>Maximum number of model update epochs</p> <p>Default: <code>500</code></p>"},{"location":"user_guide/configuration/#configpatience","title":"<code>config.patience</code>","text":"<p>Process this number of iterations at minimum</p> <p>Default: <code>2000</code></p>"},{"location":"user_guide/configuration/#configpatience_increase","title":"<code>config.patience_increase</code>","text":"<p>Increase patience by this factor if model does not converge</p> <p>Default: <code>2</code></p>"},{"location":"user_guide/configuration/#configvalidation_threshold","title":"<code>config.validation_threshold</code>","text":"<p>The factor of the validation error score to meet in order to register an improvement</p> <p>Default: <code>1.005</code></p>"},{"location":"user_guide/configuration/#configbase_learning_rate","title":"<code>config.base_learning_rate</code>","text":"<p>The initial learning rate</p> <p>Default: <code>1.003</code></p>"},{"location":"user_guide/configuration/#configmax_learning_rate","title":"<code>config.max_learning_rate</code>","text":"<p>The maximum learning rate (additional option for various schedulers)</p> <p>Default: <code>0.1</code></p>"},{"location":"user_guide/configuration/#configmin_learning_rate","title":"<code>config.min_learning_rate</code>","text":"<p>The minimum learning rate (additional option for various schedulers)</p> <p>Default: <code>1e-5</code></p>"},{"location":"user_guide/configuration/#configconvergence_threshold","title":"<code>config.convergence_threshold</code>","text":"<p>The gradient norm convergence threshold before model termination</p> <p>Default: <code>1e-4</code></p>"},{"location":"user_guide/configuration/#configoptimizer","title":"<code>config.optimizer</code>","text":"<p>Optimization algorithm to use for model estimation</p> <p>Default: <code>pycmtensor.optimizers.Adam</code></p> <p>Possible options are: </p> <ul> <li>1<sup>st</sup> order optimizers: <code>Adam</code>, <code>Nadam</code>, <code>Adam</code>, <code>Adamax</code>, <code>Adadelta</code>, <code>RMSProp</code>, <code>Momentum</code>, <code>NAG</code>, <code>AdaGrad</code>, <code>SGD</code></li> <li>2<sup>nd</sup> order optimizers: <code>SQNBFGS</code></li> </ul> <p>Note</p> <p><code>config.optimizer</code> takes a <code>pycmtensor.optimizers.Optimizer</code> class object as a value. Refer to here for more information on optimizers.</p>"},{"location":"user_guide/configuration/#configlr_scheduler","title":"<code>config.lr_scheduler</code>","text":"<p>Learning rate scheduler to use for model estimation</p> <p>Default: <code>pycmtensor.scheduler.ConstantLR</code></p> <p>Possible options are: </p> <ul> <li><code>ConstantLR</code>, <code>StepLR</code>, <code>PolynomialLR</code>, <code>CyclicLR</code>, <code>TriangularCLR</code>, <code>ExpRangeCLR</code></li> </ul> <p>Note</p> <p><code>config.lr_scheduler</code> takes a <code>pycmtensor.optimizers.Scheduler</code> class object as a value. Refer to here for more information on learning rate scheduler.</p>"},{"location":"user_guide/configuration/#configlr_exprangeclr_gamma","title":"<code>config.lr_ExpRangeCLR_gamma</code>","text":"<p>Gamma parameter for <code>ExpRangeCLR</code></p> <p>Default: <code>0.5</code></p>"},{"location":"user_guide/configuration/#configlr_steplr_factor","title":"<code>config.lr_stepLR_factor</code>","text":"<p>Drop step multiplier factor for <code>stepLR</code></p> <p>Default: <code>0.5</code></p>"},{"location":"user_guide/configuration/#configlr_steplr_drop_every","title":"<code>config.lr_stepLR_drop_every</code>","text":"<p>Drop learning rate every n steps for <code>stepLR</code></p> <p>Default: <code>10</code></p>"},{"location":"user_guide/configuration/#configlr_clr_cycle_steps","title":"<code>config.lr_CLR_cycle_steps</code>","text":"<p>Steps per cycle for <code>CyclicLR</code></p> <p>Default: <code>16</code></p>"},{"location":"user_guide/configuration/#configlr_polynomiallr_power","title":"<code>config.lr_PolynomialLR_power</code>","text":"<p>Power factor for <code>PolynomialLR</code></p> <p>Default: <code>0.999</code></p>"},{"location":"user_guide/configuration/#configbfgs_warmup","title":"<code>config.BFGS_warmup</code>","text":"<p>Discards this number of hessian matrix updates when running the <code>BFGS</code> algorithm</p> <p>Default: <code>10</code></p>"},{"location":"user_guide/configuration/#configbeta_clipping","title":"<code>config.beta_clipping</code>","text":"<p>If true, Beta parameters are clipped to lower and upper bounds</p> <p>Default: <code>True</code></p>"},{"location":"user_guide/configuration/#aesara-config","title":"Aesara config","text":"<p>PyCMTensor uses the <code>aesara</code> library, which has its own set of configurations. We use the following by default:</p> <p><code>aesara.config.on_unused_input = \"ignore\"</code></p> <p><code>aesara.config.mode = \"Mode\"</code></p> <p><code>aesara.config.allow_gc = False</code></p> <p>Refer to https://aesara.readthedocs.io/en/latest/config.html for other options. </p>"}]}