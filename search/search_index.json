{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Tensor-based choice modelling estimation Python package</p>"},{"location":"#welcome","title":"Welcome","text":"<p>PyCMTensor is a tensor-optimized discrete choice model estimation Python library  package, written with optimization compilers to speed up estimation of large datasets,  simulating very large mixed logit models or implementing neural network functions into  utility equations in choice models.</p>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>Introduction - A brief introduction of the project and its features</li> <li>Installing PyCMTensor - Instructions to install PyCMTensor</li> <li>Overview - A short 5-minute quick start to estimating your first model</li> <li>Troubleshooting and tips - Some tips for common problems and fixes</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>Some basic working code examples</p> <ul> <li>Multinomial logit</li> <li>Mixed logit</li> </ul>"},{"location":"#user-guide","title":"User guide","text":"<ul> <li>User guide - Detailed guide on using PyCMTensor</li> <li>PyCMTensor configuration - How to modify PyCMTensor attributes</li> </ul>"},{"location":"#developer-guide","title":"Developer guide","text":"<ul> <li>Developer guide - Guide for developers</li> <li>API reference</li> </ul>"},{"location":"#about","title":"About","text":"<ul> <li>Contributing</li> <li>Release notes</li> <li>Licence</li> <li>Citation</li> </ul>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Multinomial logit</li> <li>Mixed logit</li> </ul>"},{"location":"examples/#multinomial-logit","title":"Multinomial Logit","text":"<p>In this example</p>"},{"location":"examples/#mixed-logit","title":"Mixed Logit","text":""},{"location":"about/citation/","title":"Citation","text":"<p>To cite this software package:</p> <p>Bibtex</p> <pre><code>@misc{wongpycmtensor,\n    author       = {Melvin Wong},\n    title        = {PyCMTensor: Tensor-based choice modelling estimation Python package},\n    year         = 2023,\n    publisher    = {Zenodo},\n    version      = {v1.3.2},\n    doi          = {10.5281/zenodo.8074154},\n    url          = {https://doi.org/10.5281/zenodo.8074154}\n}\n</code></pre>"},{"location":"about/contributing/","title":"Guidelines for contributing","text":""},{"location":"about/contributing/#installation","title":"Installation","text":"<p>Fork a local copy</p> <p>Set up local development environment</p>"},{"location":"about/contributing/#contibuting-to-documentation","title":"Contibuting to documentation","text":""},{"location":"about/licence/","title":"Licence","text":"<p>MIT License</p> <p>Copyright \u00a9 2023, Melvin Wong</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"about/release_notes/","title":"Release notes","text":""},{"location":"about/release_notes/#unreleased","title":"Unreleased","text":""},{"location":"about/release_notes/#feat","title":"Feat","text":"<ul> <li>pycmtensor.py: Implemented early stopping on coefficient convergence in training loop</li> <li>functions.py: logit method now takes uneven dimensioned-utilities</li> <li>expression.py: Added RandomDraws expression for sampling in mixed logit</li> <li>get_train_data optional argument numpy_out to return numpy arrays rather than pandas arrays</li> <li>BHHH algorithm for calculating var-covar matrix applies to each data row</li> </ul>"},{"location":"about/release_notes/#fix","title":"Fix","text":"<ul> <li>tests.yml: Update tests workflow file conda packages</li> <li>optimizers.py: Fixed name typo in <code>__all__</code></li> <li>results.py: Corrected calculation of hessian and bhhh matrices</li> <li>scheduler.py: Moved class function calls to parent class</li> <li>statistics.py: Fixed rob varcovar calculation error</li> <li>MNL.py: Moved aesara function to parent class</li> <li>data.py: Streamlined class function calls and removed unnecessary code</li> <li>removed package import clashes with config.py</li> <li>removed gnorm calculation</li> <li>update hessian matrix and bhhh algorithm functions</li> </ul>"},{"location":"about/release_notes/#refactor","title":"Refactor","text":"<ul> <li>utils.py: Removed unused code</li> </ul>"},{"location":"about/release_notes/#v132-2023-06-23","title":"v1.3.2 (2023-06-23)","text":""},{"location":"about/release_notes/#fix_1","title":"Fix","text":"<ul> <li>make arguments in <code>MNL</code> as optional keyword arguments</li> <li>moved learning rate variable to <code>PyCMTensorModel</code> class</li> </ul>"},{"location":"about/release_notes/#refactor_1","title":"Refactor","text":"<ul> <li>make model variables as property</li> <li>update <code>__all__</code> package variables</li> <li>added <code>train_data</code> and <code>valid_data</code> property to <code>Data</code> class</li> </ul>"},{"location":"about/release_notes/#v131-2022-11-17","title":"v1.3.1 (2022-11-17)","text":""},{"location":"about/release_notes/#fix_2","title":"Fix","text":"<ul> <li>fix utility dimensions for asc only cases</li> </ul>"},{"location":"about/release_notes/#v130-2022-11-10","title":"v1.3.0 (2022-11-10)","text":""},{"location":"about/release_notes/#feat_1","title":"Feat","text":"<ul> <li>optimizers: added <code>Nadam</code> optimizer</li> <li>layers.py: added <code>DenseLayer</code> <code>BatchNormLayer</code> <code>ResidualLayer</code></li> <li>added <code>pycmtensor.about()</code> to output package metadata</li> <li>added EMA function <code>functions.exp_mov_average()</code></li> </ul>"},{"location":"about/release_notes/#fix_3","title":"Fix","text":"<ul> <li>renamed depreceated instances of <code>aesara</code> modules</li> <li>data.py: defaults <code>batch_size</code> argument to 0 if batch_size is <code>None</code></li> <li>updated syntax for <code>expressions.py</code> class objects</li> <li>added <code>init_type</code> property to <code>Weights</code> class</li> <li>moved model aesara compile functions from <code>models.MNL</code> to <code>pycmtensor.PyCMTensorModel</code></li> <li>added argument type hints in function.py</li> </ul>"},{"location":"about/release_notes/#refactor_2","title":"Refactor","text":"<ul> <li>data: added import dataset cleaning step as arguments in <code>Data()</code></li> <li>moved ResidualLayer to <code>pycmtensor.models.layers</code></li> <li>updated timing to perf_counter</li> <li>pycmtensor: refactoring model_loglikelihood</li> </ul>"},{"location":"about/release_notes/#v121-2022-10-25","title":"v1.2.1 (2022-10-25)","text":""},{"location":"about/release_notes/#feat_2","title":"Feat","text":"<ul> <li>added <code>pycmtensor.about()</code> to output package metadata</li> <li>added EMA function <code>functions.exp_mov_average()</code></li> </ul>"},{"location":"about/release_notes/#fix_4","title":"Fix","text":"<ul> <li>updated syntax for <code>expressions.py</code> class objects</li> <li>added <code>init_type</code> property to <code>Weights</code> class</li> <li>moved model aesara compile functions from <code>models.MNL</code> to <code>pycmtensor.PyCMTensorModel</code></li> </ul>"},{"location":"about/release_notes/#v120-2022-10-14","title":"v1.2.0 (2022-10-14)","text":""},{"location":"about/release_notes/#feat_3","title":"Feat","text":"<ul> <li>expressions: added Weights class object (#59)</li> <li>functions: added rmse and mae objective functions (#58)</li> <li>batch shuffle for training</li> <li>function: added KL divergence loss function (#50)</li> </ul>"},{"location":"about/release_notes/#fix_5","title":"Fix","text":"<ul> <li>added expand_dims into logit function</li> <li>replace class function Beta.Beta with Beta.beta</li> <li>removed flatten() from logit function</li> </ul>"},{"location":"about/release_notes/#v110-2022-09-23","title":"v1.1.0 (2022-09-23)","text":""},{"location":"about/release_notes/#feat_4","title":"Feat","text":"<ul> <li>scheduler: added learning rate scheduling to train()</li> <li>code: overhaul and cleanup</li> </ul>"},{"location":"about/release_notes/#fix_6","title":"Fix","text":"<ul> <li>environment: update project deps and pre-commit routine</li> <li>config: remove unnecessary cxx flags from macos builds</li> </ul>"},{"location":"about/release_notes/#perf","title":"Perf","text":"<ul> <li>config: misc optimization changes</li> </ul>"},{"location":"about/release_notes/#v107-2022-08-12","title":"v1.0.7 (2022-08-12)","text":""},{"location":"about/release_notes/#v106-2022-08-12","title":"v1.0.6 (2022-08-12)","text":""},{"location":"about/release_notes/#fix_7","title":"Fix","text":"<ul> <li>config: added optimizing speedups to config</li> <li>config: set default <code>cyclic_lr_mode</code> and <code>cyclic_lr_step_size</code> to <code>None</code></li> <li>pre-commit-config: update black to <code>22.6.0</code> in pre-commit check</li> </ul>"},{"location":"about/release_notes/#refactor_3","title":"Refactor","text":"<ul> <li>models: refactored build_functions() into models.py</li> <li>database: refactor set_choice(choiceVar)</li> </ul>"},{"location":"about/release_notes/#v105-2022-07-27","title":"v1.0.5 (2022-07-27)","text":""},{"location":"about/release_notes/#fix_8","title":"Fix","text":"<ul> <li>tests: removed depreciated tests</li> <li>routine: remove depreciated tqdm module</li> </ul>"},{"location":"about/release_notes/#v104-2022-07-27","title":"v1.0.4 (2022-07-27)","text":""},{"location":"about/release_notes/#fix_9","title":"Fix","text":"<ul> <li>pycmtensor.py: update training method</li> <li>config.py: new config option verbosity: \"high\", \"low\"</li> <li>pycmtensor.py: remove warnings for max_iter&lt;patience</li> </ul>"},{"location":"about/release_notes/#v103-2022-05-12","title":"v1.0.3 (2022-05-12)","text":""},{"location":"about/release_notes/#v102-2022-05-12","title":"v1.0.2 (2022-05-12)","text":""},{"location":"about/release_notes/#v101-2022-05-12","title":"v1.0.1 (2022-05-12)","text":""},{"location":"about/release_notes/#fix_10","title":"Fix","text":"<ul> <li>scheduler: fix missing args in input parameters</li> <li>scheduler: fix constantLR missing input paramerer</li> </ul>"},{"location":"about/release_notes/#v100-2022-05-10","title":"v1.0.0 (2022-05-10)","text":""},{"location":"about/release_notes/#feat_5","title":"Feat","text":"<ul> <li>python: update to python 3.10</li> </ul>"},{"location":"about/release_notes/#fix_11","title":"Fix","text":"<ul> <li>tests: update tests files to reflect changes in biogeme removal</li> </ul>"},{"location":"about/release_notes/#v080-2022-05-10","title":"v0.8.0 (2022-05-10)","text":""},{"location":"about/release_notes/#feat_6","title":"Feat","text":"<ul> <li>deps: remove Biogeme dependencies</li> </ul>"},{"location":"about/release_notes/#v071-2022-05-10","title":"v0.7.1 (2022-05-10)","text":""},{"location":"about/release_notes/#fix_12","title":"Fix","text":"<ul> <li>expressions: remove Biogeme dependencies</li> <li>database: remove dependencies of Biogeme</li> <li>debug: remove debug handler after each run to prevent duplication</li> <li>models: add function to return layer output -&gt; get_layer_outputs()</li> <li>debug: disables tqdm if debug mode is on and activates debug_log</li> </ul>"},{"location":"about/release_notes/#refactor_4","title":"Refactor","text":"<ul> <li>move elasticites from models to statistics for consistency</li> </ul>"},{"location":"about/release_notes/#v070-2022-03-17","title":"v0.7.0 (2022-03-17)","text":""},{"location":"about/release_notes/#feat_7","title":"Feat","text":"<ul> <li>models: add functionality to compute elasticities of choice vs attribute in models.py</li> </ul>"},{"location":"about/release_notes/#fix_13","title":"Fix","text":"<ul> <li>results: remove unnessary <code>show_weights</code> option in Results</li> <li>set default max_epoch on training run to adaptive rule</li> <li>print valid config options when invalid options are given as args to train()</li> <li>scheduler: modified cyclic_lr config loading sequence to fix unboundError</li> <li>train: turn saving model off for now</li> <li>config: generate os dependent ld_flags</li> </ul>"},{"location":"about/release_notes/#refactor_5","title":"Refactor","text":"<ul> <li>utils: refactored save_to_pickle and disables it</li> </ul>"},{"location":"about/release_notes/#perf_1","title":"Perf","text":"<ul> <li>IterationTracker: use numpy array to store iteration data</li> </ul>"},{"location":"about/release_notes/#v065-2022-03-14","title":"v0.6.5 (2022-03-14)","text":""},{"location":"about/release_notes/#feat_8","title":"Feat","text":"<ul> <li>models: Implement the ResLogit layer</li> </ul>"},{"location":"about/release_notes/#fix_14","title":"Fix","text":"<ul> <li>config: set default learning schedule to ConstantLR</li> <li>config: set default seed to a random number on init</li> </ul>"},{"location":"about/release_notes/#v064-2022-03-13","title":"v0.6.4 (2022-03-13)","text":""},{"location":"about/release_notes/#feat_9","title":"Feat","text":"<ul> <li>scheduler.py: add new scheduler (CyclicLR) for adaptive LR</li> </ul>"},{"location":"about/release_notes/#fix_15","title":"Fix","text":"<ul> <li>project: fix project metadata and ci</li> <li>config: loadout config from train() to configparser</li> <li>utils: fix TypeError check</li> </ul>"},{"location":"about/release_notes/#v050-2022-03-02","title":"v0.5.0 (2022-03-02)","text":""},{"location":"about/release_notes/#feat_10","title":"Feat","text":"<ul> <li>config: add PyCMTensorConfig class to store config settings</li> <li>expressions: add magic methods lt le gt le ne eq</li> <li>config.py: enable pre-writing of .aesararc config file on module load</li> <li>models: add method prob() to MNLogit to output prob slices</li> <li>time_format: enable logging of build and estimation time</li> <li>results: add Predict class to output probs or discrete choices</li> <li>optimizers: add AdaGram algorithm</li> <li>Database: add getattr build-in type to Database</li> <li>pycmtensor.py: add model.output_choices to generate choices</li> </ul>"},{"location":"about/release_notes/#fix_16","title":"Fix","text":"<ul> <li>statistics: add small value to stderror calculation to address sqrt(0)</li> <li>dependencies: move ipywidgets and pydot to dependencies</li> <li>renamed .rst to .md fix FileNotFoundError</li> <li>result: print more verbose results and options</li> <li>Database: add name to shared_data</li> <li>train: model instance now load initiated model class (not input Class as argument)</li> <li>Database: set choiceVar to mandatory argument</li> <li>PyCMTensor: rename append_to_params to add_params for consistency</li> <li>PyCMTensor: new method to add regularizers to cost function</li> <li>Expressions: invokes different operator for Beta Beta maths</li> <li>show excluded data in model est. output</li> <li>results: standardized naming conventions in modules db-&gt;database</li> <li>tqdm: add arg in train() to enable notebook progressbar</li> <li>swissmetro_test.ipynb: update swissmetro example</li> </ul>"},{"location":"about/release_notes/#refactor_6","title":"Refactor","text":"<ul> <li>PyCMTensor: refactoring models from pycmtensor.py</li> <li>Database: refactor(Database): refactoring database.py from pycmtensor.py</li> <li>optimizers: refactor base Optimizer class</li> <li>moved Beta Weights to expressions.py</li> </ul>"},{"location":"about/release_notes/#perf_2","title":"Perf","text":"<ul> <li>shared_data: improve iteration speed by implementing shared() on input data</li> </ul>"},{"location":"developer_guide/","title":"Developer guide","text":""},{"location":"developer_guide/#virtual-environment","title":"Virtual environment","text":""},{"location":"developer_guide/#installing-dependencies","title":"Installing dependencies","text":""},{"location":"developer_guide/#testing","title":"Testing","text":""},{"location":"developer_guide/api/","title":"API reference","text":"<p>Core libraries</p> <ul> <li>__init__.py: Init module</li> <li>config.py: PyCMTensor config module</li> <li>dataset.py: Dataset related class objects and methods</li> </ul> <p>Models</p> <ul> <li>models/MNL.py</li> </ul>"},{"location":"developer_guide/api/__init__/","title":"__init__.py","text":""},{"location":"developer_guide/api/__init__/#pycmtensor","title":"<code>pycmtensor</code>","text":"<p>Top-level package for PyCMTensor.</p>"},{"location":"developer_guide/api/__init__/#pycmtensor.about","title":"<code>about()</code>","text":"<p>Returns a <code>watermark.watermark</code> of various system information for debugging</p>"},{"location":"developer_guide/api/__init__/#config","title":"<code>config</code>","text":"<p>Instance of the <code>Config</code> class</p>"},{"location":"developer_guide/api/config/","title":"config.py","text":"<p>See configuration for a list of available configuration settings.</p>"},{"location":"developer_guide/api/config/#pycmtensor.config","title":"<code>pycmtensor.config</code>","text":"<p>PyCMTensor config module</p>"},{"location":"developer_guide/api/config/#pycmtensor.config.Config","title":"<code>Config()</code>","text":"<p>Config class object that holds configuration settings</p> <p>Attributes:</p> Name Type Description <code>descriptions</code> <code>dict</code> <p>descriptive documentation of each configuration setting</p> <p>Tip</p> <p>To display a current list of configuration settings, invoke <code>print(pycmtensor.config)</code>.</p> <pre><code>import pycmtensor\nprint(pycmtensor.config)\n</code></pre> <p>Output: <pre><code>PyCMTensor configuration\n...\n</code></pre></p>"},{"location":"developer_guide/api/config/#pycmtensor.config.Config.add","title":"<code>add(name, value, description=None)</code>","text":"<p>Method to add a new or update a setting in the configuration</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the setting</p> required <code>value</code> <code>any</code> <p>value given to the setting</p> required <code>description</code> <code>str</code> <p>a string text describing the function of the setting</p> <code>None</code> <p>Example</p> <p>To set the value of the random seed to 100 <pre><code>pycmtensor.config.add('seed', 100)\n</code></pre></p>"},{"location":"developer_guide/api/dataset/","title":"dataset.py","text":""},{"location":"developer_guide/api/dataset/#pycmtensor.dataset","title":"<code>pycmtensor.dataset</code>","text":""},{"location":"developer_guide/api/dataset/#pycmtensor.dataset.Dataset","title":"<code>Dataset(df, choice)</code>","text":"<p>Base PyCMTensor Dataset class object</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas.DataFrame</code> <p>the pandas dataframe object to load</p> required <code>choice</code> <code>str</code> <p>the name of the choice variable</p> required <p>Attributes:</p> Name Type Description <code>n</code> <code>int</code> <p>total number of rows in the dataset</p> <code>x</code> <code>list[TensorVariable]</code> <p>a list of (input) <code>TensorVariable</code> objects to build the tensor expression from</p> <code>y</code> <code>TensorVariable</code> <p>the output (choice) <code>TensorVariable</code> object</p> <code>scale</code> <code>dict</code> <p>a dictionary of <code>float</code> values to store the scaling factor used for each variable</p> <code>choice</code> <code>str</code> <p>the name of the choice variable</p> <code>ds</code> <code>dict</code> <p>a dictionary of <code>np.ndarray</code> to store the values of each variable</p> <code>split_frac</code> <code>float</code> <p>the factor used to split the dataset into training and validation datasets</p> <code>train_index</code> <code>list</code> <code>valid_index</code> <code>list</code> <code>n_train</code> <code>int</code> <code>n_valid</code> <code>int</code> Example <p>Example initalization of a pandas dataset:</p> <pre><code>ds = Dataset(df=pd.read_csv(\"datafile.csv\", sep=\",\"), choice=\"mode\")\nds.split(frac=0.8)\n</code></pre> <p>Attributes can be access by invoking: <pre><code>print(ds.choice)\n</code></pre></p> <p>Output: <pre><code>'car'\n</code></pre></p>"},{"location":"developer_guide/api/dataset/#pycmtensor.dataset.Dataset.drop","title":"<code>drop(variables)</code>","text":"<p>Method for dropping <code>variables</code> from the dataset</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>list</code> <p>list of variables from the dataset to drop</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>raises an error if any item in <code>variables</code> is not found in the dataset or item is the choice variable</p> <p>Warning</p> <p>Choice variable cannot be explicity dropped.</p>"},{"location":"developer_guide/api/dataset/#pycmtensor.dataset.Dataset.scale_variable","title":"<code>scale_variable(variable, factor)</code>","text":"<p>Multiply values of the <code>variable</code> by factor 1/factor.</p> <p>Parameters:</p> Name Type Description Default <code>variable</code> <code>str</code> <p>the name of the variable or a list of variable names</p> required <code>factor</code> <code>float</code> <p>the scaling factor</p> required"},{"location":"developer_guide/api/dataset/#pycmtensor.dataset.Dataset.split","title":"<code>split(frac)</code>","text":"<p>TODO</p>"},{"location":"developer_guide/api/dataset/#pycmtensor.dataset.Dataset.train_dataset","title":"<code>train_dataset(variables, index=None, batch_size=None, shift=None)</code>","text":"<p>Return a slice of the training dataset with the sequence matching the list of variables</p>"},{"location":"developer_guide/api/dataset/#pycmtensor.dataset.Dataset.valid_dataset","title":"<code>valid_dataset(variables, index=None, batch_size=None, shift=None)</code>","text":"<p>Return a slice of the valid dataset with the sequence matching the list of variables</p>"},{"location":"developer_guide/api/functions/","title":"functions.py","text":""},{"location":"developer_guide/api/functions/#pycmtensor.functions","title":"<code>pycmtensor.functions</code>","text":"<p>PyCMTensor functions module</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.errors","title":"<code>errors(prob, y)</code>","text":"<p>Symbolic representation of the discrete prediction as a percentage error.</p> <p>Parameters:</p> Name Type Description Default <code>prob</code> <code>TensorVariable</code> <p>matrix describing the choice probabilites</p> required <code>y</code> <code>TensorVariable</code> <p>the <code>TensorVariable</code> referencing the choice column</p> required <p>Returns:</p> Type Description <code>TensorVariable</code> <p>the mean prediction error over <code>y</code></p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.exp_mov_average","title":"<code>exp_mov_average(batch_avg, moving_avg, alpha=0.1)</code>","text":"<p>Calculates the exponential moving average (EMA) of a new minibatch</p> <p>Parameters:</p> Name Type Description Default <code>batch_avg</code> <code>TensorVariable</code> <p>the new batch value of the mean</p> required <code>moving_avg</code> <code>TensorVariable</code> <p>the moving value of the accumulated mean</p> required <code>alpha</code> <code>float</code> <p>the moving average factor of the batch mean</p> <code>0.1</code> <p>Returns:</p> Type Description <code>TensorVariable</code> <p>the new moving average</p> Note <p>The moving average will decay by the difference between the existing value and the new value multiplied by the moving average factor. A higher <code>alpha</code> value results in faster changing moving average.</p> <p>Formula:</p> \\[x_{EMA} = \\alpha * x_t + x_{EMA} * (1-\\alpha)\\]"},{"location":"developer_guide/api/functions/#pycmtensor.functions.first_order_derivative","title":"<code>first_order_derivative(cost, params)</code>","text":"<p>Symbolic representation of the 1st order gradient vector given the neg. log likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>cost</code> <code>TensorVariable</code> <p>the neg loglikelihood to compute the gradients over</p> required <code>params</code> <code>list[Beta]</code> <p>list of params to compute the gradients over</p> required <p>Returns:</p> Type Description <code>TensorVariable</code> <p>the gradient vector</p> Note <p>Parameters with <code>status=1</code> are ignored.</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.kl_divergence","title":"<code>kl_divergence(p, q)</code>","text":"<p>Computes the KL divergence loss between discrete distributions <code>p</code> and <code>q</code>.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>TensorVariable</code> <p>model output probabilities</p> required <code>q</code> <code>TensorVariable</code> <p>ground truth probabilities</p> required <p>Returns:</p> Type Description <code>TensorVariable</code> <p>a symbolic representation of the KL loss </p> Note <p>Formula:</p> \\[ L = \\begin{cases}     \\sum_{i=1}^N (p_i * log(p_i/q_i)) &amp; p&gt;0\\\\     0 &amp; p&lt;=0 \\end{cases} \\]"},{"location":"developer_guide/api/functions/#pycmtensor.functions.kl_multivar_norm","title":"<code>kl_multivar_norm(m0, v0, m1, v1, epsilon=1e-06)</code>","text":"<p>Computes the KL divergence loss between two multivariate normal distributions.</p> <p>Parameters:</p> Name Type Description Default <code>m0</code> <code>TensorVariable</code> <p>mean vector of the first Normal m.v. distribution \\(N_0\\)</p> required <code>v0</code> <code>TensorVariable</code> <p>(co-)variance matrix of the first Normal m.v. distribution \\(N_0\\)</p> required <code>m1</code> <code>TensorVariable</code> <p>mean vector of the second Normal m.v. distribution \\(N_1\\)</p> required <code>v1</code> <code>TensorVariable</code> <p>(co-)variance of the second Normal m.v. distribution \\(N_1\\)</p> required <code>epsilon</code> <code>float</code> <p>small value to prevent divide-by-zero error</p> <code>1e-06</code> Note <p>k = dimension of the distribution.</p> <p>Formula:</p> \\[     D_{KL}(N_0||N_1) = 0.5 * \\Big(\\ln\\big(\\frac{|v_1|}{|v_0|}\\big) + trace(v_1^{-1} v_0) + (m_1-m_0)^T v_1^{-1} (m_1-m_0) - k\\Big) \\] <p>In variational inference, the kl divergence is the relative entropy between a diagonal multivariate Normal and a standard Normal distribution, \\(N(0, 1)\\), therefore, for VI, <code>m1=1</code>, <code>v1=1</code></p> <p>For two univariate distributions, dimensions of <code>m0,m1,v0,v1 = 0</code></p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.log_likelihood","title":"<code>log_likelihood(prob, y)</code>","text":"<p>Symbolic representation of the log likelihood cost function.</p> <p>Parameters:</p> Name Type Description Default <code>prob</code> <code>TensorVariable</code> <p>a <code>TensorVariable</code> matrix describing the choice probabilites</p> required <code>y</code> <code>TensorVariable</code> <p>a <code>TensorVariable</code> referencing the choice variable</p> required <p>Returns:</p> Type Description <code>TensorVariable</code> <p>a symbolic representation of the log likelihood with <code>ndim=0</code>.</p> Note <p>The 0-th dimension is the numbering of alternatives, the N-th dimension is the size of the input (# rows).</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.logit","title":"<code>logit(utility, avail=None)</code>","text":"<p>Computes the Logit function, with availability conditions.</p> <p>Parameters:</p> Name Type Description Default <code>utility</code> <code>Union[list, tuple, TensorVariable]</code> <p>list of M utility equations</p> required <code>avail</code> <code>Union[list, tuple, TensorVariable]</code> <p>list of M availability conditions, if no availability conditions are provided, defaults to <code>1</code> for all availabilities.</p> <code>None</code> <p>Returns:</p> Type Description <code>TensorVariable</code> <p>A NxM matrix of probabilities.</p> Note <p>The 0-th dimension is the numbering of alternatives, the N-th dimension is the size of the input (# rows).</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.mae","title":"<code>mae(y_hat, y)</code>","text":"<p>Computes the mean absolute error (MAE) between pairs of observations</p> <p>Parameters:</p> Name Type Description Default <code>y_hat</code> <code>TensorVariable</code> <p>model estimated values</p> required <code>y</code> <p>ground truth values</p> required <p>Returns:</p> Type Description <code>TensorVariable</code> <p>symbolic scalar representation of the mean absolute error</p> Note <p>Tensor is flattened to a <code>dim=1</code> vector if the input tensor is <code>dim=2</code>.</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.rmse","title":"<code>rmse(y_hat, y)</code>","text":"<p>Computes the root mean squared error (RMSE) between pairs of observations</p> <p>Parameters:</p> Name Type Description Default <code>y_hat</code> <code>TensorVariable</code> <p>model estimated values</p> required <code>y</code> <code>TensorVariable</code> <p>ground truth values</p> required <p>Returns:</p> Type Description <code>TensorVariable</code> <p>symbolic scalar representation of the rmse</p> Note <p>Tensor is flattened to a <code>dim=1</code> vector if the input tensor is <code>dim=2</code>.</p>"},{"location":"developer_guide/api/functions/#pycmtensor.functions.second_order_derivative","title":"<code>second_order_derivative(cost, params)</code>","text":"<p>Symbolic representation of the 2nd order Hessian matrix given the neg. log likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>cost</code> <code>TensorVariable</code> <p>the neg loglikelihood to compute the gradients over</p> required <code>params</code> <code>list[Beta]</code> <p>list of params to compute the gradients over</p> required <p>Returns:</p> Type Description <code>TensorVariable</code> <p>the Hessian matrix</p> Note <p>Parameters with <code>status=1</code> are ignored.</p>"},{"location":"developer_guide/api/models.mnl/","title":"models.MNL.py","text":""},{"location":"developer_guide/api/models.mnl/#pycmtensor.models.MNL","title":"<code>pycmtensor.models.MNL</code>","text":""},{"location":"developer_guide/api/models.mnl/#pycmtensor.models.MNL.MNL","title":"<code>MNL(ds, params, utility, av=None, **kwargs)</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Defines a Multinomial Logit model</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>pycmtensor.Data</code> <p>the database object</p> required <code>params</code> <code>dict</code> <p>dictionary of name:parameter pair</p> required <code>utility</code> <code>list or TensorVariable</code> <p>the vector of utility functions</p> required <code>av</code> <code>list</code> <p>list of availability conditions. If <code>None</code>, all availability is set to 1</p> <code>None</code> <code>**kwargs</code> <p>keyword arguments. Possible options are <code>optimizer</code> set the optimizer to use. see mod:<code>pycmtensor.optimizer</code> for available options.</p> <code>{}</code>"},{"location":"getting_started/","title":"Introduction","text":""},{"location":"getting_started/#why-pycmtensor","title":"Why PyCMTensor?","text":"<p>Writing mathematical operations and evaluating models involving choice based utility  expressions can be difficult and time-consuming, especially with alternative specific  utilities of different dimensionalities are involved or when specifying neural networks  within utility specification. Typically, Python deep learning libraries such as  TensorFlow, Torch, Keras, or Scikit-learn express entire datasets as products of  n-dimensional arrays, without regards to the number of variables used or specification  of independent taste coefficients in each part of the output choice probability  equation. These libraries also do not expose the underlying operations that define the  inputs of a random utility equation, making it inconvenient for hypothesis testing or  running statistical tests without cumbersome modification or ad hoc calculations. </p> <p>Although these advanced Python deep learning libraries can be used to evaluate choice  models, they are not explicitly intended to be used for estimating choice models,  especially more advanced versions such as mixed logit. These deep learning models are first and foremost for model prediction, and not choice model estimation where clear interpretation of utility coefficients are needed.</p>"},{"location":"getting_started/#what-can-pycmtensor-do","title":"What can PyCMTensor do?","text":"<p>Currently, PyCMTensor can be used to fully specify Multinomial Logit and Mixed Logit models, estimate and generate statistical tests, using optimized tensor operations via the Aesara tensor libraries.</p>"},{"location":"getting_started/#project-goals","title":"Project goals","text":"<p>The goal of PyCMTensor is to combine the easy-to-interpet choice modelling syntaxes and  expressions while also implementing some of the tensor-based operations  and computational speed-up of a deep learning library through Aesara. PyCMTensor  focuses on specifying utility expressions, and making it easy to define a deep neural  network inside a choice model such as TasteNet or ResLogit model specifications. The  distinction of PyCMTensor from other deep learning libraries is that it focuses on  econometric modelling and statistical testing, rather than focusing purely on models  for prediction.</p> <p>The higher level goals of this project are:</p> <ul> <li>Provide a flexible and customizable library for implementing \"hybrid\" deep learning based discrete choice model</li> <li>Facilitating development and introduction of deep learning algorithms and methods for domain experts and researchers who are already familiar with discrete choice modelling</li> <li>Develop tensor based utility specification and prototyping which are logical and consistent with choice modelling equations and methodologies</li> <li>increase computational efficiency of estimating large scale choice models with machine learning algorithms</li> <li>Provide a platform for researchers with similar interests to contribute to an ecosystem for estimating advanced discrete choice models.</li> </ul>"},{"location":"getting_started/#key-features","title":"Key features","text":"<p>Main features:</p> <ul> <li>Interpretable and customizable utility specification syntax</li> <li>Perform statistical tests and  generate var-covar matrices for taste parameters.</li> <li>Fast execution of model estimation including of simulation based methods, e.g. Mixed Logit models</li> <li>Model estimation with 1st order (Stochastic Gradient Descent) or 2nd order methods (BFGS)</li> <li>Specifying neural nets with weight and bias parameters inside a utility function [TODO]</li> </ul> <p>While other choice modelling estimation software in Python are available, e.g. Biogeme, xlogit, PyLogit, etc., PyCMTensor strives to fully implement deep learning based methods written in the same syntax format as Biogeme. Different software programs may occasionally vary in their behaviour and estimation results. The following are some of the key differences between PyCMTensor and other choice modelling estimation packages:</p>"},{"location":"getting_started/#roadmap","title":"Roadmap","text":"<p>PyCMTensor is a work in progress, there are several proposed feature implementations that needs to be done and there are still some code maintenance, documentation writing, and testing to be performed. </p> <p>The following are proposed major feature implementations:</p> <ul> <li> Implementation of TasteNet and ResLogit hybrid deep learning choice models</li> <li> Optimization algorithms:<ul> <li> Stochastic Newton Method (SNM)</li> <li> Momentum-based BFGS</li> </ul> </li> <li> Variational inference estimation</li> </ul> <p>If you are interested in contributing to the development of PyCMTensor, please contact me.</p>"},{"location":"getting_started/installation/","title":"Installing PyCMTensor","text":"<p>To ensure complete installation including the necessary libraries, it is recommended to first install dependencies via <code>conda</code> package manager in a virtual environment, then install PyCMTensor via <code>pip</code>.</p>"},{"location":"getting_started/installation/#system-requirements","title":"System requirements","text":"<ul> <li>Python (3.9+)</li> <li>Aesara (2.9+) - from conda-forge</li> <li>Numpy - from conda-forge</li> <li>Scipy</li> <li>Pandas</li> </ul> <p>In addition, you will need:</p> <ul> <li>A C compiler compatible with your OS and Python installation. Libraries can be installed from conda-forge:<ul> <li>Linux: <code>gcc_linux-64</code> and <code>gxx_linux-64</code></li> <li>Windows (7 or later): <code>m2w64-toolchain</code> and <code>vs2019_win-64</code></li> <li>macOS (incl. M1): <code>Clang</code></li> </ul> </li> <li>BLAS installation<ul> <li>MKL libraries, installed through conda with <code>mkl-service</code> package</li> <li>Openblas, default when Numpy is installed with pip, alternatively, with conda <code>blas</code> package</li> </ul> </li> </ul>"},{"location":"getting_started/installation/#install-conda-dependencies","title":"Install conda dependencies","text":"<p>Install Miniconda. Select the appropriate package for your operating system.</p> <p>Once you have installed conda, create a virtual environment and activate it</p> <pre><code>conda create -n pycmtensor python=3.11 \nconda activate pycmtensor\n</code></pre> <p>Install the conda dependencies:</p> <p>Windows</p> <pre><code>conda install -c conda-forge mkl-service m2w64-toolchain vs2019_win-64 blas aesara -y\n</code></pre> <p>macOS (incl. M1)</p> <pre><code>conda install -c conda-forge mkl-service Clang blas aesara -y\n</code></pre> <p>Linux</p> <pre><code>conda install -c conda-forge mkl-service gcc_linux-64 gxx_linux-64 blas aesara -y\n</code></pre>"},{"location":"getting_started/installation/#download-pycmtensor-using-pip","title":"Download PyCMTensor using pip","text":"<p>Once the conda packages have been installed, install the rest of the packages using <code>pip</code>, type:</p> <pre><code>pip install pycmtensor\n</code></pre>"},{"location":"getting_started/installation/#checking-your-installation","title":"Checking your installation","text":"<p>If PyCMTensor was installed correctly, the following should display when you run the following code in a python console:</p> <pre><code>python -c \"import pycmtensor; print(pycmtensor.__version__)\"\n</code></pre> <p>Output:</p> <pre><code>1.3.2\n</code></pre>"},{"location":"getting_started/installation/#updating-pycmtensor","title":"Updating PyCMTensor","text":"<p>Update PyCMTensor by running the <code>pip install --upgrade pycmtensor</code> command</p>"},{"location":"getting_started/installation/#source-code","title":"Source code","text":"<p>Source code can be checked out from the Github repository via <code>git</code>:</p> <pre><code>git clone git::/github.com/mwong009/pycmtensor\n</code></pre>"},{"location":"getting_started/overview/","title":"Overview","text":"<p>Follow the steps below and learn how to use PyCMTensor to estimate a discrete choice model. In this tutorial, we will use the London Passenger Mode Choice (LPMC) dataset (pdf). Download the dataset here and place it in the working directory.</p> <p>Jump to Putting it all together for the final Python script.</p>"},{"location":"getting_started/overview/#importing-data-from-csv","title":"Importing data from csv","text":"<p>Import the PyCMTensor package and read the data using <code>pandas</code>:</p> <pre><code>import pycmtensor\nimport pandas as pd\n\nlpmc = pd.read_csv(\"lpmc.dat\", sep='\\t')  # read the .dat file and use &lt;TAB&gt; separator\nlpmc = lpmc[lpmc[\"travel_year\"]==2015]  # select only the 2015 data to use\n</code></pre>"},{"location":"getting_started/overview/#create-a-dataset-object","title":"Create a dataset object","text":"<p>From the <code>pycmtensor</code> package, import the <code>Dataset</code> object, which stores and manages the tensors and arrays of the data variables. Denote the column name with the choice variable in the argument <code>choice=</code>:</p> <pre><code>from pycmtensor.dataset import Dataset\nds = Dataset(df=lpmc, choice=\"travel_mode\")\n</code></pre> <p>The <code>Dataset</code> object takes the following arguments:</p> <ul> <li><code>df</code>: The <code>pandas.DataFrame</code> object</li> <li><code>choice</code>: The name of the choice variable found in the heading of the dataframe</li> </ul> <p>Note</p> <p>If the range of alternatives in the choice column does not start with <code>0</code>, e.g. <code>[1, 2, 3, 4]</code> instead of <code>[0, 1, 2, 3]</code>, the Dataset will automatically convert the alternatives to start with <code>0</code>.</p>"},{"location":"getting_started/overview/#split-the-dataset","title":"Split the dataset","text":"<p>Next, split the dataset into training and validation datasets, <code>frac=</code> argument is the percentage of the data that is assigned to the training dataset. The rest of the data is assigned to the validation dataset. </p> <pre><code>ds.split(frac=0.8)  # splits 80% of the data into the training dataset\n                    # and the other 20% into the validation dataset\n</code></pre> <p>You should get an output showing the number of training and validation samples in the dataset.</p> <p>Output:</p> <pre><code>[INFO] n_train_samples:3986 n_valid_samples:997\n</code></pre> <p>Note</p> <p>Splitting the dataset is optional. If <code>frac=</code> is not given as an argument, both training and validation dataset will use the same samples.</p>"},{"location":"getting_started/overview/#defining-taste-parameters","title":"Defining taste parameters","text":"<p>Define the taste parameters using the <code>Beta</code> object from the <code>pycmtensor.expressions</code> module:</p> <pre><code>from pycmtensor.expressions import Beta\n\n# Beta parameters\nasc_walk = Beta(\"asc_walk\", 0.0, None, None, 1)\nasc_cycle = Beta(\"asc_cycle\", 0.0, None, None, 0)\nasc_pt = Beta(\"asc_pt\", 0.0, None, None, 0)\nasc_drive = Beta(\"asc_drive\", 0.0, None, None, 0)\nb_cost = Beta(\"b_cost\", 0.0, None, None, 0)\nb_time = Beta(\"b_time\", 0.0, None, None, 0)\nb_purpose = Beta(\"b_purpose\", 0.0, None, None, 0)\nb_licence = Beta(\"b_licence\", 0.0, None, None, 0)\n</code></pre> <p>The <code>Beta</code> object takes the following argument:</p> <ul> <li><code>name</code>: Name of the taste parameter (required)</li> <li><code>value</code>: The initial starting value. Defaults to <code>0.</code></li> <li><code>lb</code> and <code>ub</code>: lower and upper bound of the parameter. Defaults to <code>None</code></li> <li><code>status</code>: <code>1</code> if the parameter should not be estimated. Defaults to <code>0</code>.</li> </ul> <p>Note</p> <p>If a <code>Beta</code> variable is not used in the model, a warning will be shown in stdout. E.g.</p> <pre><code>[WARNING] b_purpose not in any utility functions\n</code></pre> <p>Info</p> <p><code>pycmtensor.Beta</code> follows the same syntax as in Biogeme's <code>biogeme.expressions.Beta</code> for familiarity sake. However, <code>pycmtensor.Beta</code> uses <code>aesara.tensor</code> variables to define the mathematical ops. Both are not interchangable.</p>"},{"location":"getting_started/overview/#specifying-utility-equations","title":"Specifying utility equations","text":"<pre><code>U_walk  = asc_walk + b_time * ds[\"dur_walking\"]\nU_cycle = asc_cycle + b_time  * ds[\"dur_cycling\"]\nU_pt    = asc_pt + b_time * (ds[\"dur_pt_rail\"] + ds[\"dur_pt_bus\"] + \\\n          ds[\"dur_pt_int\"]) + b_cost * ds[\"cost_transit\"]\nU_drive = asc_drive + b_time * ds[\"dur_driving\"] + b_licence * ds[\"driving_license\"] + \\\n          b_cost * (ds[\"cost_driving_fuel\"] + ds[\"cost_driving_ccharge\"])\n\n# vectorize the utility function\nU = [U_walk, U_cycle, U_pt, U_drive]\n</code></pre>"},{"location":"getting_started/overview/#specifying-the-model","title":"Specifying the model","text":"<pre><code>mymodel = pycmtensor.models.MNL(ds=ds, params=locals(), utility=U, av=None)\n</code></pre> <p>Output:</p> <pre><code>[WARNING] b_purpose not in any utility functions\n[INFO] inputs in MNL: [driving_license, dur_walking, dur_cycling, dur_pt_rail, dur_pt_bus, dur_pt_int, dur_driving, cost_transit, cost_driving_fuel, cost_driving_ccharge]\n[INFO] Build time = 00:00:09\n</code></pre> <p>The <code>MNL</code> object takes the following argument:</p> <ul> <li><code>ds</code>: The dataset object</li> <li><code>params</code>: the list (or dict) of declared parameter objects</li> <li><code>utility</code>: The (list of) utilities to be estimated*</li> <li><code>av</code>: The availability conditions as a list with the same index as <code>utility</code>. See here for details on specifying availability conditions. Defaults to <code>None</code></li> <li><code>**kwargs</code>: Optional keyword arguments for modifying the model configuration settings. See configuration in the user guide for details on possible options</li> </ul> <p>Tip</p> <p>*: We use <code>locals()</code> as a shortcut for collecting the <code>Beta</code> objects from the Python local environment for the argument <code>params=</code>.</p>"},{"location":"getting_started/overview/#estimating-the-model","title":"Estimating the model","text":"<pre><code>from pycmtensor.models import train\nfrom pycmtensor.optimizers import Adam\nfrom pycmtensor.scheduler import ConstantLR\n\ntrain(\n    model=mymodel, \n    ds=ds, \n    optimizer=Adam,  # optional\n    batch_size=0,  # optional \n    base_learning_rate=1.,  # optional \n    convergence_threshold=0.0001,  # optional \n    max_steps=200,  # optional\n    lr_scheduler=ConstantLR,  # optional\n)\n</code></pre> <p>The function <code>train()</code> estimates the model until convergence specified by the gradient norm between two complete passes of the entire training dataset. In order to limit repeated calculation, we store the \\(\\beta\\) of the previous epoch and roughly calculate the gradient using: \\(\\nabla_\\beta = \\beta_t - \\beta_{t-1}\\). The estimation is terminated either when the <code>max_steps</code> is reached or when the gradient norm \\(||\\nabla_\\beta||_{_2}\\) is less than the <code>convergence_threshold</code> value.</p> <p>The <code>train()</code> function takes the following required arguments:</p> <ul> <li><code>model</code>: The model object. <code>MNL</code> in the example above</li> <li><code>ds</code>: The dataset object</li> </ul> <p>The other arguments are optional, and they can be set when calling the <code>train()</code> function. These optional arguments are the 'hyperparameters' of the model that modifies the training procedure. For a list of possible hyperparameter options, explanations, and default values, see configuration in the user guide.</p> <p>Note</p> <p>A <code>step</code> is one full pass of the training dataset. An <code>iteration</code> is one model update operation, usually it is every mini-batch (when <code>batch_size != 0</code>).</p> <p>Tip</p> <p>The hyperparameters can also be set with the <code>pycmtensor.config</code> module before the training function is called.</p> <p>For example, to set the training <code>batch_size</code> to <code>50</code> and <code>base_learning_rate</code> to <code>0.1</code>:</p> <pre><code>pycmtensor.config.batch_size = 50\npycmtensor.config.base_learning_rate = 0.1\n\ntrain (\n    model=...\n)\n</code></pre> <p>Output:</p> <pre><code>[INFO] Start (n=3986, Step=0, LL=-5525.77, Error=80.34%)\n[INFO] Train (Step=0, LL=-9008.61, Error=80.34%, gnorm=2.44949e+00, 0/2000)\n[INFO] Train (Step=16, LL=-3798.26, Error=39.12%, gnorm=4.46640e-01, 16/2000)\n[INFO] Train (Step=54, LL=-3487.97, Error=35.21%, gnorm=6.80979e-02, 54/2000)\n[INFO] Train (Step=87, LL=-3471.01, Error=35.01%, gnorm=9.93509e-03, 87/2000)\n[INFO] Train (Step=130, LL=-3470.29, Error=34.70%, gnorm=1.92617e-03, 130/2000)\n[INFO] Train (Step=168, LL=-3470.28, Error=34.70%, gnorm=3.22536e-04, 168/2000)\n[INFO] Train (Step=189, LL=-3470.28, Error=34.70%, gnorm=8.74120e-05, 189/2000)\n[INFO] Model converged (t=0.492)\n[INFO] Best results obtained at Step 185: LL=-3470.28, Error=34.70%, gnorm=2.16078e-04\n</code></pre>"},{"location":"getting_started/overview/#printing-statistical-test-results","title":"Printing statistical test results","text":"<pre><code>print(mymodel.results.beta_statistics())\nprint(mymodel.results.model_statistics())\nprint(mymodel.results.benchmark())\n</code></pre> <p>Ouput:</p> <pre><code>              value   std err     t-test p-value rob. std err rob. t-test rob. p-value\nasc_cycle -3.853007  0.117872 -32.688157     0.0     0.120295  -32.029671          0.0\nasc_drive -2.060414  0.099048 -20.802183     0.0     0.102918  -20.019995          0.0\nasc_pt    -1.305677  0.076151 -17.145988     0.0     0.079729  -16.376401          0.0\nasc_walk        0.0         -          -       -            -           -            -\nb_cost    -0.135635  0.012788 -10.606487     0.0      0.01269  -10.688684          0.0\nb_licence  1.420747  0.079905  17.780484     0.0     0.084526   16.808497          0.0\nb_time    -4.947477  0.183329 -26.986865     0.0     0.192431  -25.710378          0.0\n\n                                         value\nNumber of training samples used         3986.0\nNumber of validation samples used        997.0\nNull. log likelihood              -5525.769323\nFinal log likelihood              -3470.282749\nAccuracy                                65.30%\nLikelihood ratio test              4110.973149\nRho square                            0.371982\nRho square bar                        0.370715\nAkaike Information Criterion       6954.565498\nBayesian Information Criterion     6998.599302\nFinal gradient norm                2.16078e-04\n\n                            value\nSeed                        42069\nModel build time         00:00:09\nModel train time         00:00:00\niterations per sec  384.15 iter/s\n</code></pre>"},{"location":"getting_started/overview/#prediction-and-validation","title":"Prediction and validation","text":""},{"location":"getting_started/overview/#putting-it-all-together","title":"Putting it all together","text":"<pre><code>import pycmtensor\nimport pandas as pd\n\nfrom pycmtensor.dataset import Dataset\nfrom pycmtensor.expressions import Beta\n\n# read data\nlpmc = pd.read_csv(\"lpmc.dat\", sep='\\t')\nlpmc = lpmc[lpmc[\"travel_year\"]==2015] \n\n# load data into dataset\nds = Dataset(df=lpmc, choice=\"travel_mode\")\nds.split(frac=0.8) \n\n# Beta parameters\nasc_walk = Beta(\"asc_walk\", 0.0, None, None, 1)\nasc_cycle = Beta(\"asc_cycle\", 0.0, None, None, 0)\nasc_pt = Beta(\"asc_pt\", 0.0, None, None, 0)\nasc_drive = Beta(\"asc_drive\", 0.0, None, None, 0)\nb_cost = Beta(\"b_cost\", 0.0, None, None, 0)\nb_time = Beta(\"b_time\", 0.0, None, None, 0)\nb_licence = Beta(\"b_licence\", 0.0, None, None, 0)\n\n# utility equations\nU_walk  = asc_walk + b_time * ds[\"dur_walking\"]\nU_cycle = asc_cycle + b_time  * ds[\"dur_cycling\"]\nU_pt    = asc_pt + b_time * (ds[\"dur_pt_rail\"] + ds[\"dur_pt_bus\"] + \\\n          ds[\"dur_pt_int\"]) + b_cost * ds[\"cost_transit\"]\nU_drive = asc_drive + b_time * ds[\"dur_driving\"] + b_licence * ds[\"driving_license\"] + \\\n          b_cost * (ds[\"cost_driving_fuel\"] + ds[\"cost_driving_ccharge\"])\n\n# vectorize the utility function\nU = [U_walk, U_cycle, U_pt, U_drive]\n\nmymodel = pycmtensor.models.MNL(ds=ds, params=locals(), utility=U, av=None)\n\n\nfrom pycmtensor.models import train\nfrom pycmtensor.optimizers import Adam\nfrom pycmtensor.scheduler import ConstantLR\n\n# main training loop\ntrain(\n    model=mymodel, \n    ds=ds, \n    optimizer=Adam,  # optional\n    batch_size=0,  # optional \n    base_learning_rate=1.,  # optional \n    convergence_threshold=0.0001,  # optional \n    max_steps=200,  # optional\n    lr_scheduler=ConstantLR,  # optional\n)\n\n# print results\nprint(mymodel.results.beta_statistics())\nprint(mymodel.results.model_statistics())\nprint(mymodel.results.benchmark())\n</code></pre>"},{"location":"getting_started/troubleshooting/","title":"Troubleshooting","text":""},{"location":"user_guide/","title":"User guide","text":""},{"location":"user_guide/#data-structures","title":"Data structures","text":""},{"location":"user_guide/#calling-variables-and-selecting-data","title":"Calling variables and selecting data","text":""},{"location":"user_guide/#mathematical-operations","title":"Mathematical operations","text":""},{"location":"user_guide/#working-with-generic-and-alternative-specific-variables","title":"Working with generic and alternative specific variables","text":""},{"location":"user_guide/#model-estimation","title":"Model estimation","text":""},{"location":"user_guide/#configuration","title":"Configuration","text":""},{"location":"user_guide/#optimizers","title":"Optimizers","text":""},{"location":"user_guide/#fine-tuning-model","title":"Fine tuning model","text":""},{"location":"user_guide/#generate-results","title":"Generate results","text":""},{"location":"user_guide/configuration/","title":"PyCMTensor Configuration","text":""},{"location":"user_guide/configuration/#guide","title":"Guide","text":"<p>The <code>config</code> module contains <code>attributes</code> that is used for setting the model training hyperparameters, type of optimizer to use, random seed value, and other customizable values. These attributes are loaded when importing the <code>pycmtensor</code> module, but can be modified at any time before invoking the <code>train()</code> method.</p> <p>Display the list of configuration settings with the following in Python: <pre><code>import pycmtensor\nprint(pycmtensor.config)\n</code></pre></p> <p>Set or update a given configuration with the following: <pre><code>pycmtensor.config.add('seed', 100)\n</code></pre></p>"},{"location":"user_guide/configuration/#configuration-attributes","title":"Configuration attributes","text":""},{"location":"user_guide/configuration/#configseed","title":"<code>config.seed</code>","text":"<p>Seed value for random number generators. </p> <p>Default: <code>100</code></p>"},{"location":"user_guide/configuration/#configbatch_size","title":"<code>config.batch_size</code>","text":"<p>Number of samples processed on each iteration of the model update</p> <p>Default: <code>32</code></p>"},{"location":"user_guide/configuration/#configmax_epochs","title":"<code>config.max_epochs</code>","text":"<p>Maximum number of model update epochs</p> <p>Default: <code>500</code></p>"},{"location":"user_guide/configuration/#configpatience","title":"<code>config.patience</code>","text":"<p>Process this number of iterations at minimum</p> <p>Default: <code>2000</code></p>"},{"location":"user_guide/configuration/#configpatience_increase","title":"<code>config.patience_increase</code>","text":"<p>Increase patience by this factor if model does not converge</p> <p>Default: <code>2</code></p>"},{"location":"user_guide/configuration/#configvalidation_threshold","title":"<code>config.validation_threshold</code>","text":"<p>The factor of the validation error score to meet in order to register an improvement</p> <p>Default: <code>1.003</code></p>"},{"location":"user_guide/configuration/#configconvergence_threshold","title":"<code>config.convergence_threshold</code>","text":"<p>The gradient norm convergence threshold before model termination</p> <p>Default: <code>1e-4</code></p>"},{"location":"user_guide/configuration/#configbase_learning_rate","title":"<code>config.base_learning_rate</code>","text":"<p>The initial learning rate</p> <p>Default: <code>1.003</code></p>"},{"location":"user_guide/configuration/#configmax_learning_rate","title":"<code>config.max_learning_rate</code>","text":"<p>The maximum learning rate (additional option for various schedulers)</p> <p>Default: <code>0.1</code></p>"},{"location":"user_guide/configuration/#configmin_learning_rate","title":"<code>config.min_learning_rate</code>","text":"<p>The minimum learning rate (additional option for various schedulers)</p> <p>Default: <code>1e-5</code></p>"},{"location":"user_guide/configuration/#configoptimizer","title":"<code>config.optimizer</code>","text":"<p>Optimization algorithm to use for model estimation</p> <p>Default: <code>pycmtensor.optimizers.Adam</code></p> <p>Possible options are: </p> <ul> <li>1st order optimizers: <code>Adam</code>, <code>Nadam</code>, <code>Adam</code>, <code>Adamax</code>, <code>Adadelta</code>, <code>RMSProp</code>, <code>Momentum</code>, <code>NAG</code>, <code>AdaGrad</code>, <code>SGD</code></li> <li>2nd order optimizers: <code>BFGS</code></li> </ul> <p>Note</p> <p><code>config.optimizer</code> takes a <code>pycmtensor.optimizers.Optimizer</code> class object as a value. Refer to here for more information on optimizers.</p>"},{"location":"user_guide/configuration/#configbfgs_warmup","title":"<code>config.BFGS_warmup</code>","text":"<p>Discards this number of hessian matrix updates when running the <code>BFGS</code> algorithm</p> <p>Default: <code>10</code></p>"},{"location":"user_guide/configuration/#configlr_scheduler","title":"<code>config.lr_scheduler</code>","text":"<p>Learning rate scheduler to use for model estimation</p> <p>Default: <code>pycmtensor.scheduler.ConstantLR</code></p> <p>Possible options are: </p> <ul> <li><code>ConstantLR</code>, <code>StepLR</code>, <code>PolynomialLR</code>, <code>CyclicLR</code>, <code>TriangularCLR</code>, <code>ExpRangeCLR</code></li> </ul> <p>Note</p> <p><code>config.lr_scheduler</code> takes a <code>pycmtensor.optimizers.Scheduler</code> class object as a value. Refer to here for more information on learning rate scheduler.</p>"},{"location":"user_guide/configuration/#configlr_exprangeclr_gamma","title":"<code>config.lr_ExpRangeCLR_gamma</code>","text":"<p>Gamma parameter for <code>ExpRangeCLR</code></p> <p>Default: <code>0.5</code></p>"},{"location":"user_guide/configuration/#configlr_steplr_factor","title":"<code>config.lr_stepLR_factor</code>","text":"<p>Drop step multiplier factor for <code>stepLR</code></p> <p>Default: <code>0.5</code></p>"},{"location":"user_guide/configuration/#configlr_steplr_drop_every","title":"<code>config.lr_stepLR_drop_every</code>","text":"<p>Drop learning rate every n steps for <code>stepLR</code></p> <p>Default: <code>10</code></p>"},{"location":"user_guide/configuration/#configlr_clr_cycle_steps","title":"<code>config.lr_CLR_cycle_steps</code>","text":"<p>Steps per cycle for <code>CyclicLR</code></p> <p>Default: <code>16</code></p>"},{"location":"user_guide/configuration/#configlr_polynomiallr_power","title":"<code>config.lr_PolynomialLR_power</code>","text":"<p>Power factor for <code>PolynomialLR</code></p> <p>Default: <code>0.999</code></p>"},{"location":"user_guide/configuration/#aesara-config","title":"Aesara config","text":"<p>PyCMTensor uses the <code>aesara</code> library, which has its own set of configurations. We use the following by default:</p> <p><code>aesara.config.on_unused_input = \"ignore\"</code></p> <p><code>aesara.config.mode = \"Mode\"</code></p> <p><code>aesara.config.allow_gc = False</code></p> <p>Refer to https://aesara.readthedocs.io/en/latest/config.html for other options. </p>"}]}