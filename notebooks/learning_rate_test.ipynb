{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class LR:\n",
    "\tdef plot(self, max_steps):\n",
    "\t\tlrs = [self(i) for i in range(1, max_steps)]\n",
    "\t\tplt.style.use(\"ggplot\")\n",
    "\t\tplt.figure()\n",
    "\t\tplt.plot(range(1, max_steps), lrs)\n",
    "\t\tplt.xlabel(\"Step #\")\n",
    "\t\tplt.ylabel(\"Learning Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler(LR):\n",
    "    def __str__(self):\n",
    "        return f\"{self.name}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        msg = f\"{self.name}(\"\n",
    "        attrs = [d for d in dir(self) if not d.startswith('_')]\n",
    "        for a in attrs:\n",
    "            if a == \"name\":\n",
    "                continue\n",
    "            if isinstance(getattr(self, a), (int, str, float)):\n",
    "                msg += f\"{a}={getattr(self, a)}, \"\n",
    "\n",
    "        return msg[:-2]+\")\"\n",
    "\n",
    "\n",
    "class ConstantLR(Scheduler):\n",
    "    \"\"\"Base class for constant learning rate scheduler\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        \"\"\"Constructor for ConstantLR class object\n",
    "\n",
    "        Args:\n",
    "            lr (float, optional): learning rate value\n",
    "        \"\"\"\n",
    "        self.name = \"ConstantLR\"\n",
    "        self._base_lr = lr\n",
    "        self._history = OrderedDict()\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        \"\"\"Returns the learning rate value\"\"\"\n",
    "        return self._base_lr\n",
    "\n",
    "    @property\n",
    "    def history(self):\n",
    "        \"\"\"Returns the histroy of the learning rate\"\"\"\n",
    "        return self._history\n",
    "\n",
    "    def __call__(self, step):\n",
    "        \"\"\"Computes the learning rate for this step\"\"\"\n",
    "        self._record(step, self.lr)\n",
    "        return self.lr\n",
    "\n",
    "    def _record(self, step, lr):\n",
    "        \"\"\"Saves the history of the learning rate and returns the current rate\"\"\"\n",
    "        self._history[step] = lr\n",
    "        return lr\n",
    "\n",
    "\n",
    "class StepLR(ConstantLR):\n",
    "    \"\"\"Base class for step learning rate scheduler\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, factor=0.25, drop_every=10):\n",
    "        \"\"\"Constructor for StepLR class object\n",
    "\n",
    "        Args:\n",
    "            lr (float): initial learning rate value\n",
    "            factor (float, optional): percentage reduction to the learning rate\n",
    "            drop_every (int, optional): step down the learning rate after every n steps\n",
    "        \"\"\"\n",
    "        super().__init__(lr)\n",
    "        self.name = \"StepLR\"\n",
    "        self._factor = factor\n",
    "        self._drop_every = drop_every\n",
    "\n",
    "    @property\n",
    "    def factor(self):\n",
    "        return self._factor\n",
    "    \n",
    "    @property\n",
    "    def drop_every(self):\n",
    "        return self._drop_every\n",
    "\n",
    "    def __call__(self, step):\n",
    "        \"\"\"Computes the learning rate for this step\"\"\"\n",
    "        decay = self.factor ** np.floor(step / self._drop_every)\n",
    "        lr = float(self.lr * decay)\n",
    "        return self._record(step, lr)\n",
    "\n",
    "\n",
    "class PolynomialLR(ConstantLR):\n",
    "    \"\"\"Base class for polynomial decay learning rate scheduler\"\"\"\n",
    "\n",
    "    def __init__(self, max_steps, lr=0.01, power=1.0):\n",
    "        \"\"\"Constructor for PolynomialLR class object\n",
    "\n",
    "        Args:\n",
    "            lr (float): initial learning rate value\n",
    "            max_steps (int): the max number of training steps to take\n",
    "            power (float, optional): the exponential factor to decay\n",
    "        \"\"\"\n",
    "        super().__init__(lr)\n",
    "        self.name = \"PolynomialLR\"\n",
    "        self._max_steps = max_steps\n",
    "        self._power = power\n",
    "\n",
    "    @property\n",
    "    def power(self):\n",
    "        return self._power\n",
    "\n",
    "    @property\n",
    "    def max_steps(self):\n",
    "        return self._max_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        \"\"\"Computes the learning rate for this step\"\"\"\n",
    "        decay = (1 - (step / float(self.max_steps))) ** self.power\n",
    "        lr = float(self.lr * decay)\n",
    "        return self._record(step, lr)\n",
    "\n",
    "\n",
    "class CyclicLR(ConstantLR):\n",
    "    \"\"\"Base class for cyclical learning rate scheduler\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, max_lr=0.1, cycle_steps=16, scale_fn=None):\n",
    "        \"\"\"Constructor for ConstantLR class object\n",
    "\n",
    "        Args:\n",
    "            lr (float, optional): the base learning rate value\n",
    "            max_lr (float, optional): the maximum learning rate value\n",
    "            cycle_steps (int, optional): the number of steps to complete a cycle\n",
    "            scale_fn (func, optional): custom scaling policy defined by a single arg\n",
    "        \"\"\"\n",
    "        super().__init__(lr)\n",
    "        self.name = \"CyclicLR\"\n",
    "        self._max_lr = max_lr\n",
    "        self._cycle_steps = cycle_steps\n",
    "        self._scale_fn = scale_fn\n",
    "\n",
    "    @property\n",
    "    def max_lr(self):\n",
    "        \"\"\"Returns the maximum learning rate value\"\"\"\n",
    "        return self._max_lr\n",
    "\n",
    "    @property\n",
    "    def cycle_steps(self):\n",
    "        \"\"\"Returns the cycle steps value\"\"\"\n",
    "        return self._cycle_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        \"\"\"Computes the learning rate for this step\"\"\"\n",
    "        cycle = np.floor(1 + step / self.cycle_steps)\n",
    "        x = np.abs(step / (self.cycle_steps / 2) - 2 * cycle + 1)\n",
    "        height = (self.max_lr - self.lr) * self.scale_fn(cycle)\n",
    "        lr = self.lr + height * np.maximum(0, 1 - x)\n",
    "        return self._record(step, lr)\n",
    "\n",
    "    def scale_fn(self, k):\n",
    "        \"\"\"Custom scaling policy\"\"\"\n",
    "        if self._scale_fn is None:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return self._scale_fn(k)\n",
    "\n",
    "\n",
    "class Triangular2CLR(CyclicLR):\n",
    "    \"\"\"Class object for the Triangular2 Cyclic LR scheduler\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, max_lr=0.1, cycle_steps=16):\n",
    "        \"\"\"Constructor for Triangular2CLR class object\n",
    "\n",
    "        Args:\n",
    "            lr (float, optional): the base learning rate value\n",
    "            max_lr (float, optional): the maximum learning rate value\n",
    "            cycle_steps (int, optional): the number of steps to complete a cycle\n",
    "        \"\"\"\n",
    "        super().__init__(lr, max_lr, cycle_steps)\n",
    "        self.name = \"Triangular2CLR\"\n",
    "\n",
    "    def scale_fn(self, k):\n",
    "        return float(1.0 / (2.0 ** (k - 1.0)))\n",
    "\n",
    "\n",
    "class ExpRangeCLR(CyclicLR):\n",
    "    \"\"\"Class object for the exponential range Cyclic LR scheduler\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, max_lr=0.1, cycle_steps=16, gamma=0.5):\n",
    "        \"\"\"Constructor for Triangular2CLR class object\n",
    "\n",
    "        Args:\n",
    "            lr (float, optional): the base learning rate value\n",
    "            max_lr (float, optional): the maximum learning rate value\n",
    "            cycle_steps (int, optional): the number of steps to complete a cycle\n",
    "        \"\"\"\n",
    "        super().__init__(lr, max_lr, cycle_steps)\n",
    "        self.name = \"ExpRangeCLR\"\n",
    "        self._gamma = gamma\n",
    "\n",
    "    @property\n",
    "    def gamma(self):\n",
    "        return self._gamma\n",
    "\n",
    "    def scale_fn(self, k):\n",
    "        return self.gamma**k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ExpRangeCLR(cycle_steps=16, gamma=0.5, lr=0.01, max_lr=0.1)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler = ExpRangeCLR(gamma=0.5)\n",
    "\n",
    "str(scheduler)\n",
    "repr(scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pycmtensor-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8519cd4d47b15910736c5570af4c4e0aa29a6c9a8fe56130f373e7aa596bd9e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
